# @package _global_
# make sure to include the above commented global setting!

# pipeline info
pipeline: CUL_MAIN_EXPR
expr_name: clpu_derpp_scifar100_20tasks
train_tasks: 20
eval_after_tasks: 20
global_seed: 1

# paradigm settings
cl_paradigm: TIL
unlearning_requests:
  2: [1]
  4: [2]
  5: [5]
  7: [6]
  9: [7]
  10: [10]
  12: [11]
  14: [12]
  15: [15]
  17: [16]
  19: [18]
  20: [20]
unlearnable_age: 3


# components
defaults:
  - /cl_dataset: split_cifar100.yaml
  - /backbone: clresnet.yaml
  - /cl_algorithm: clpu_derpp.yaml
  - /cul_algorithm: clpu_derpp_unlearn.yaml
  - /optimizer: adam.yaml
  - /lr_scheduler: reduce_lr_on_plateau.yaml
  - /trainer: single_gpu.yaml
  - /metrics: cl_main_expr_default.yaml
  - /lightning_loggers: default.yaml
  - /callbacks: cul_default.yaml
  - /hydra: default.yaml
  - /misc: default.yaml

# outputs
output_dir: outputs/${expr_name}/${misc.timestamp}

# overrides
trainer: 
  max_epochs: 20
