window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "clarena", "modulename": "clarena", "kind": "module", "doc": "<h1 id=\"continual-learning-arena-clarena\">Continual Learning Arena (CLArena)</h1>\n\n<p><strong>CLArena (Continual Learning Arena)</strong> is an open-source Python package designed for <strong>Continual Learning (CL)</strong> research. This package provides an integrated environment with extensive APIs for conducting CL experiments, along with pre-implemented algorithms and datasets that you can start using immediately. This package also supports <strong>Continual Unlearning (CUL)</strong>, <strong>Multi-Task Learning (MTL)</strong> and <strong>Single-Task Learning (STL)</strong>.</p>\n\n<p>Please note this is the API reference providing detailed information about the available classes, functions, and modules in CLArena. Please refer to the main documentation for tutorials, examples, and guides on how to use CLArena:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena\"><strong>Main Documentation</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide\"><strong>A Beginners' Guide to Continual Learning</strong></a></li>\n</ul>\n\n<p>We provide various components in the submodules:</p>\n\n<ul>\n<li><code>clarena.pipelines</code>: Pre-defined experiment and evaluation pipelines for different paradigms.</li>\n<li><code>clarena.cl_datasets</code>: Continual learning datasets.</li>\n<li><code>clarena.mtl_datasets</code>: Multi-task learning datasets.</li>\n<li><code>clarena.stl_datasets</code>: Single-task learning datasets.</li>\n<li><code>clarena.backbones</code>: Neural network architectures used as backbones networks.</li>\n<li><code>clarena.heads</code>: Output heads.</li>\n<li><code>clarena.cl_algorithms</code>: Continual learning algorithms.</li>\n<li><code>clarena.cul_algorithms</code>: Continual unlearning algorithms.</li>\n<li><code>clarena.mtl_algorithms</code>: Multi-task learning algorithms.</li>\n<li><code>clarena.stl_algorithms</code>: Single-task learning algorithms.</li>\n<li><code>clarena.metrics</code>: Metrics for evaluation.</li>\n<li><code>clarena.callbacks</code>: Extra actions added to the pipelines.</li>\n<li><code>clarena.utils</code>: Utilities for the package.</li>\n</ul>\n"}, {"fullname": "clarena.backbones", "modulename": "clarena.backbones", "kind": "module", "doc": "<h1 id=\"backbone-networks\">Backbone Networks</h1>\n\n<p>This submodule provides the <strong>backbone neural network architectures</strong> for all paradigms in CLArena.</p>\n\n<p>Here are the base classes for backbone networks, which inherit from PyTorch <code>nn.Module</code>:</p>\n\n<ul>\n<li><code>Backbone</code>: the base class for all backbone networks. Multi-task and single-task learning can use this class directly.</li>\n<li><code>CLBackbone</code>: the base class for continual learning backbone networks, which incorporates mechanisms for managing continual learning tasks.\n<ul>\n<li><code>HATMaskBackbone</code>: the base class for backbones used in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> CL algorithm.</li>\n<li><code>WSNMaskBackbone</code>: The base class for backbones used in <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks)</a> CL algorithm.</li>\n</ul></li>\n</ul>\n\n<p>Please note that this is an API documentation. Please refer to the main documentation pages for more information about how to configure and implement backbone networks:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/backbone-network\"><strong>Configure Backbone Network</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/backbone-network\"><strong>Implement Custom Backbone Network</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.backbones.Backbone", "modulename": "clarena.backbones", "qualname": "Backbone", "kind": "class", "doc": "<p>The base class for backbone networks.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.backbones.Backbone.__init__", "modulename": "clarena.backbones", "qualname": "Backbone.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code> | <code>None</code>): The output dimension that connects to output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code>output_dim</code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.Backbone.output_dim", "modulename": "clarena.backbones", "qualname": "Backbone.output_dim", "kind": "variable", "doc": "<p>The output dimension of the backbone network.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.Backbone.weighted_layer_names", "modulename": "clarena.backbones", "qualname": "Backbone.weighted_layer_names", "kind": "variable", "doc": "<p>The list of the weighted layer names in order (from input to output). A weighted layer has weights connecting to other weighted layers. They are the main part of neural networks. <strong>It must be provided in subclasses.</strong></p>\n\n<p>The layer names must match the names of weighted layers defined in the backbone and include all of them. The names follow the <code>nn.Module</code> internal naming mechanism with <code>.</code> replaced with <code>/</code>. For example: </p>\n\n<ul>\n<li>If a layer is assigned to <code>self.conv1</code>, the name becomes <code>conv1</code>. </li>\n<li>If <code>nn.Sequential</code> is used, the name becomes the index of the layer in the sequence, such as <code>0</code>, <code>1</code>, etc. </li>\n<li>If a hierarchical structure is used, for example, a <code>nn.Module</code> is assigned to <code>self.block</code> which has <code>self.conv1</code>, the name becomes <code>block/conv1</code>. Note that it should have been <code>block.conv1</code> according to <code>nn.Module</code>'s rules, but we use '/' instead of '.' to avoid errors when using '.' as keys in a <code>ModuleDict</code>.</li>\n</ul>\n", "annotation": ": list[str]"}, {"fullname": "clarena.backbones.Backbone.get_layer_by_name", "modulename": "clarena.backbones", "qualname": "Backbone.get_layer_by_name", "kind": "function", "doc": "<p>Get the layer by its name.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code> | <code>None</code>): The layer name following the <code>nn.Module</code> internal naming mechanism with <code>.</code> replaced with <code>/</code>. If <code>None</code>, return <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>layer</strong> (<code>nn.Module</code> | <code>None</code>): The layer. If <code>layer_name</code> is <code>None</code>, return <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.Backbone.preceding_layer_name", "modulename": "clarena.backbones", "qualname": "Backbone.preceding_layer_name", "kind": "function", "doc": "<p>Get the name of the preceding layer of the given layer from the stored <code>weighted_layer_names</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code> | <code>None</code>): The layer name following the <code>nn.Module</code> internal naming mechanism with <code>.</code> replaced with <code>/</code>. If <code>None</code>, return <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>preceding_layer_name</strong> (<code>str</code>): The name of the preceding layer. If the given layer is the first layer, return <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.Backbone.next_layer_name", "modulename": "clarena.backbones", "qualname": "Backbone.next_layer_name", "kind": "function", "doc": "<p>Get the name of the next layer of the given layer from the stored <code>self.masked_layer_order</code>. If the given layer is the last layer of the BACKBONE, return <code>None</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): The name of the layer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>next_layer_name</strong> (<code>str</code>): The name of the next layer.</li>\n</ul>\n\n<p><strong>Raises:</strong></p>\n\n<ul>\n<li><strong>ValueError</strong>: If <code>layer_name</code> is not in the weighted layer order.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.Backbone.forward", "modulename": "clarena.backbones", "qualname": "Backbone.forward", "kind": "function", "doc": "<p>The forward pass. <strong>It must be implemented by subclasses.</strong></p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for certain algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.CLBackbone", "modulename": "clarena.backbones", "qualname": "CLBackbone", "kind": "class", "doc": "<p>The base class of continual learning backbone networks.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.CLBackbone.__init__", "modulename": "clarena.backbones", "qualname": "CLBackbone.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code> | <code>None</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code>output_dim</code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.CLBackbone.task_id", "modulename": "clarena.backbones", "qualname": "CLBackbone.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to the number of tasks in the CL dataset.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.CLBackbone.processed_task_ids", "modulename": "clarena.backbones", "qualname": "CLBackbone.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.backbones.CLBackbone.setup_task_id", "modulename": "clarena.backbones", "qualname": "CLBackbone.setup_task_id", "kind": "function", "doc": "<p>Set up task <code>task_id</code>. This must be done before the <code>forward()</code> method is called.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.CLBackbone.forward", "modulename": "clarena.backbones", "qualname": "CLBackbone.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. In some backbones, the forward pass might be different for different tasks. <strong>It must be implemented by subclasses.</strong></p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code> | <code>None</code>): The task ID where the data are from. If the stage is 'train' or 'validation', it is usually the current task <code>self.task_id</code>. If the stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided; thus this argument can be used. In CIL, they are not provided, so it is just a placeholder for API consistency and is not used. Best practice is not to provide this argument and leave it as the default value.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone", "kind": "class", "doc": "<p>The backbone network for HAT-based algorithms with learnable hard attention masks.</p>\n\n<p>HAT-based algorithms include:</p>\n\n<ul>\n<li><a href=\"http://proceedings.mlr.press/v80/serra18a\"><strong>HAT (Hard Attention to the Task, 2018)</strong></a> is an architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</li>\n<li><a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\"><strong>AdaHAT (Adaptive Hard Attention to the Task, 2024)</strong></a> is an architecture-based continual learning approach that improves HAT by introducing adaptive soft gradient clipping based on parameter importance and network sparsity.</li>\n<li><strong>FG-AdaHAT</strong> is an architecture-based continual learning approach that improves HAT by introducing fine-grained neuron-wise importance measures guiding the adaptive adjustment mechanism in AdaHAT.</li>\n</ul>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.HATMaskBackbone.__init__", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code>output_dim</code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>\n<li><strong>gate</strong> (<code>str</code>): The type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n<li><code>tanh</code>: the hyperbolic tangent function.</li>\n</ul></li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.HATMaskBackbone.gate", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.gate", "kind": "variable", "doc": "<p>The type of gate function.</p>\n", "annotation": ": str"}, {"fullname": "clarena.backbones.HATMaskBackbone.gate_fn", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.gate_fn", "kind": "variable", "doc": "<p>The gate function mapping the real value task embeddings into attention masks.</p>\n", "annotation": ": Callable"}, {"fullname": "clarena.backbones.HATMaskBackbone.task_embedding_t", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.task_embedding_t", "kind": "variable", "doc": "<p>The task embedding for the current task <code>self.task_id</code>. Keys are layer names and values are the task embedding <code>nn.Embedding</code> for the layer. Each task embedding has size (1, number of units).</p>\n\n<p>We use <code>ModuleDict</code> rather than <code>dict</code> to ensure <code>LightningModule</code> properly registers these model parameters for purposes such as automatic device transfer and model summaries.</p>\n\n<p>We use <code>nn.Embedding</code> rather than <code>nn.Parameter</code> to store the task embedding for each layer, which is a type of <code>nn.Module</code> and can be accepted by <code>nn.ModuleDict</code>. (<code>nn.Parameter</code> cannot be accepted by <code>nn.ModuleDict</code>.)</p>\n\n<p><strong>This must be defined to cover each weighted layer (as listed in <code>weighted_layer_names</code>) in the backbone network.</strong> Otherwise, the uncovered parts will keep being updated for all tasks and become the source of catastrophic forgetting.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.backbones.HATMaskBackbone.masks", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.masks", "kind": "variable", "doc": "<p>The binary attention mask of each previous task gated from the task embedding. Keys are task IDs and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has size (number of units, ).</p>\n", "annotation": ": dict[int, dict[str, torch.Tensor]]"}, {"fullname": "clarena.backbones.HATMaskBackbone.initialize_task_embedding", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.initialize_task_embedding", "kind": "function", "doc": "<p>Initialize the task embedding for the current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mode</strong> (<code>str</code>): The initialization mode for task embeddings; one of:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit task embeddings from the last task.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.sanity_check", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.get_mask", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.get_mask", "kind": "function", "doc": "<p>Get the hard attention mask used in the <code>forward()</code> method for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): The stage when applying the conversion; one of:\n<ol>\n<li>'train': training stage. Get the mask from the current task embedding through the gate function, scaled by an annealed scalar. See Sec. 2.4 in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li>'validation': validation stage. Get the mask from the current task embedding through the gate function, scaled by <code>s_max</code>, where large scaling makes masks nearly binary. (Note that in this stage, the binary mask hasn't been stored yet, as training is not over.)</li>\n<li>'test': testing stage. Apply the test mask directly from the stored masks using <code>test_task_id</code>.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): The maximum scaling factor in the gate function. Doesn't apply to the testing stage. See Sec. 2.4 in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): The current batch index. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): The total number of batches. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): The hard attention (with values 0 or 1) mask. Keys (<code>str</code>) are the layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has size (number of units, ).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.te_to_binary_mask", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.te_to_binary_mask", "kind": "function", "doc": "<p>Convert the current task embedding into a binary mask.</p>\n\n<p>This method is used before the testing stage to convert the task embedding into a binary mask for each layer. The binary mask is used to select parameters for the current task.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mask_t</strong> (<code>dict[str, Tensor]</code>): The binary mask for the current task. Keys (<code>str</code>) are the layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has size (number of units, ).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.store_mask", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.store_mask", "kind": "function", "doc": "<p>Store the mask for the current task <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.get_layer_measure_parameter_wise", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.get_layer_measure_parameter_wise", "kind": "function", "doc": "<p>Get the parameter-wise measure on the parameters right before the given layer.</p>\n\n<p>It is calculated from the given neuron-wise measure. It aggregates two feature-sized vectors (corresponding to the given layer and the preceding layer) into a weight-wise matrix (corresponding to the weights in between) and a bias-wise vector (corresponding to the bias of the given layer), using the given aggregation method. For example, given two feature-sized measures $m_{l,i}$ and $m_{l-1,j}$ and 'min' aggregation, the parameter-wise measure is $\\min \\left(a_{l,i}, a_{l-1,j}\\right)$, a matrix with respect to $i, j$.</p>\n\n<p>Note that if the given layer is the first layer with no preceding layer, we will get the parameter-wise measure directly by broadcasting from the neuron-wise measure of the given layer.</p>\n\n<p>This method is used to calculate parameter-wise measures in various HAT-based algorithms:</p>\n\n<ul>\n<li><strong>HAT</strong>: the parameter-wise measure is the binary mask for previous tasks from the neuron-wise cumulative mask of previous tasks <code>cumulative_mask_for_previous_tasks</code>, which is $\\text{Agg} \\left(a_{l,i}^{<t}, a_{l-1,j}^{<t}\\right)$ in Eq. (2) in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>AdaHAT</strong>: the parameter-wise measure is the parameter importance for previous tasks from the neuron-wise summative mask of previous tasks <code>summative_mask_for_previous_tasks</code>, which is $\\text{Agg} \\left(m_{l,i}^{<t,\\text{sum}}, m_{l-1,j}^{<t,\\text{sum}}\\right)$ in Eq. (9) in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li><strong>FG-AdaHAT</strong>: the parameter-wise measure is the parameter importance for previous tasks from the neuron-wise importance of previous tasks <code>summative_importance_for_previous_tasks</code>, which is $\\text{Agg} \\left(I_{l,i}^{<t}, I_{l-1,j}^{<t}\\right)$ in Eq. (2) in the FG-AdaHAT paper.</li>\n</ul>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>neuron_wise_measure</strong> (<code>dict[str, Tensor]</code>): The neuron-wise measure. Keys are layer names and values are the neuron-wise measure tensor. The tensor has size (number of units, ).</li>\n<li><strong>layer_name</strong> (<code>str</code>): The name of the given layer.</li>\n<li><strong>aggregation_mode</strong> (<code>str</code>): The aggregation mode mapping two feature-wise measures into a weight-wise matrix; one of:\n<ul>\n<li>'min': takes the minimum of the two connected unit measures.</li>\n<li>'max': takes the maximum of the two connected unit measures.</li>\n<li>'mean': takes the mean of the two connected unit measures.</li>\n</ul></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>weight_measure</strong> (<code>Tensor</code>): The weight measure matrix, the same size as the corresponding weights.</li>\n<li><strong>bias_measure</strong> (<code>Tensor</code>): The bias measure vector, the same size as the corresponding bias.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">neuron_wise_measure</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">aggregation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.HATMaskBackbone.forward", "modulename": "clarena.backbones", "qualname": "HATMaskBackbone.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>self.task_id</code>. Task-specific masks for <code>self.task_id</code> are applied to the units in each layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): The maximum scaling factor in the gate function. See Sec. 2.4 in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): The current batch index. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): The total number of batches. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): The mask for the current task. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for continual learning algorithms that need hidden features. Although the HAT algorithm does not need this, it is still provided for API consistency for other HAT-based algorithms that inherit this <code>forward()</code> method of the <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.WSNMaskBackbone", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone", "kind": "class", "doc": "<p>The backbone network for the WSN algorithm with learnable parameter masks.</p>\n\n<p><a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks, 2022)</a> is an architecture-based continual learning algorithm. It trains learnable parameter-wise scores and selects the most scored $c\\%$ of the network parameters to be used for each task.</p>\n", "bases": "clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.backbones.WSNMaskBackbone.__init__", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads. The <code>input_dim</code> of output heads is expected to be the same as this <code>output_dim</code>. In some cases, this class is used as a block in the backbone network that doesn't have an output dimension. In this case, it can be <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.WSNMaskBackbone.gate_fn", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.gate_fn", "kind": "variable", "doc": "<p>The gate function mapping the real-value parameter score into binary parameter masks. It is a custom autograd function that applies percentile parameter masking by score.</p>\n", "annotation": ": torch.autograd.function.Function"}, {"fullname": "clarena.backbones.WSNMaskBackbone.weight_score_t", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.weight_score_t", "kind": "variable", "doc": "<p>The weight score for the current task <code>self.task_id</code>. Keys are the layer names and values are the task embedding <code>nn.Embedding</code> for the layer. Each task embedding has the same size (output features, input features) as the weight.</p>\n\n<p>We use <code>ModuleDict</code> rather than <code>dict</code> to ensure <code>LightningModule</code> properly registers these model parameters for purposes such as automatic device transfer and model summaries.</p>\n\n<p>We use <code>nn.Embedding</code> rather than <code>nn.Parameter</code> to store the task embedding for each layer, which is a type of <code>nn.Module</code> and can be accepted by <code>nn.ModuleDict</code>. (<code>nn.Parameter</code> cannot be accepted by <code>nn.ModuleDict</code>.)</p>\n\n<p><strong>This must be defined to cover each weighted layer (as listed in <code>weighted_layer_names</code>) in the backbone network.</strong> Otherwise, the uncovered parts will keep being updated for all tasks and become the source of catastrophic forgetting.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.backbones.WSNMaskBackbone.bias_score_t", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.bias_score_t", "kind": "variable", "doc": "<p>The bias score for the current task <code>self.task_id</code>. Keys are the layer names and values are the task embedding <code>nn.Embedding</code> for the layer. Each task embedding has the same size (1, output features) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</p>\n\n<p>We use <code>ModuleDict</code> rather than <code>dict</code> to ensure <code>LightningModule</code> properly registers these model parameters for purposes such as automatic device transfer and model summaries.</p>\n\n<p>We use <code>nn.Embedding</code> rather than <code>nn.Parameter</code> to store the task embedding for each layer, which is a type of <code>nn.Module</code> and can be accepted by <code>nn.ModuleDict</code>. (<code>nn.Parameter</code> cannot be accepted by <code>nn.ModuleDict</code>.)</p>\n\n<p><strong>This must be defined to cover each weighted layer (as listed in <code>weighted_layer_names</code>) in the backbone network.</strong> Otherwise, the uncovered parts will keep being updated for all tasks and become the source of catastrophic forgetting.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.backbones.WSNMaskBackbone.sanity_check", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.WSNMaskBackbone.initialize_parameter_score", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.initialize_parameter_score", "kind": "function", "doc": "<p>Initialize the parameter score for the current task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mode</strong> (<code>str</code>): The initialization mode for parameter scores; one of:\n<ol>\n<li>'default': the default initialization mode in the original WSN code.</li>\n<li>'N01': standard normal distribution $N(0, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.WSNMaskBackbone.get_mask", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.get_mask", "kind": "function", "doc": "<p>Get the binary parameter mask used in the <code>forward()</code> method for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): The stage when applying the conversion; one of:\n<ol>\n<li>'train': training stage. Get the mask from the parameter score of the current task through the gate function that masks the top $c\\%$ largest scored parameters. See Sec. 3.1 \"Winning Subnetworks\" in the <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN paper</a>.</li>\n<li>'validation': validation stage. Same as 'train'. (Note that in this stage, the binary mask hasn't been stored yet, as training is not over.)</li>\n<li>'test': testing stage. Apply the test mask directly from the argument <code>test_mask</code>.</li>\n</ol></li>\n<li><strong>test_mask</strong> (<code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> | <code>None</code>): The binary weight and bias masks used for testing. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): The binary mask on weights. Key (<code>str</code>) are the layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has the same size (output features, input features) as the weight.</li>\n<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): The binary mask on biases. Keys (<code>str</code>) are the layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has the same size (output features, ) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">mask_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.WSNMaskBackbone.forward", "modulename": "clarena.backbones", "qualname": "WSNMaskBackbone.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>self.task_id</code>. Task-specific mask for <code>self.task_id</code> are applied to the units in each layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>mask_percentage</strong> (<code>float</code>): The percentage of parameters to be masked. The value should be between 0 and 1.</li>\n<li><strong>test_mask</strong> (<code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> | <code>None</code>): The binary weight and bias mask used for test. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): The weight mask for the current task. Key (<code>str</code>) are layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has same (output features, input features) as the weight.</li>\n<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): The bias mask for the current task. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has same (output features, ) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">mask_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp", "modulename": "clarena.backbones.mlp", "kind": "module", "doc": "<p>The submodule in <code>backbones</code> for the MLP backbone network. It includes multiple versions of MLP, including the basic MLP, the continual learning MLP, the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT</a> masked MLP, and the <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN</a> masked MLP.</p>\n"}, {"fullname": "clarena.backbones.mlp.MLP", "modulename": "clarena.backbones.mlp", "qualname": "MLP", "kind": "class", "doc": "<p>Multi-layer perceptron (MLP), a.k.a. fully connected network.</p>\n\n<p>MLP is a dense network architecture with several fully connected layers, each followed by an activation function. The last layer connects to the output heads.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.mlp.MLP.__init__", "modulename": "clarena.backbones.mlp", "qualname": "MLP.__init__", "kind": "function", "doc": "<p>Construct and initialize the MLP backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): The input dimension. Any data need to be flattened before entering the MLP. Note that it is not required in convolutional networks.</li>\n<li><strong>hidden_dims</strong> (<code>list[int]</code>): List of hidden layer dimensions. It can be an empty list, which means a single-layer MLP, and it can be as many layers as you want. Note that it doesn't include the last dimension, which we take as the output dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to output heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code> | <code>None</code>): Activation function of each layer (if not <code>None</code>). If <code>None</code>, this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): Whether to use batch normalization after the fully connected layers. Default <code>False</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): Whether to use bias in the linear layer. Default <code>True</code>.</li>\n<li><strong>dropout</strong> (<code>float</code> | <code>None</code>): The probability for the dropout layer. If <code>None</code>, this layer won't be used. Default <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.mlp.MLP.input_dim", "modulename": "clarena.backbones.mlp", "qualname": "MLP.input_dim", "kind": "variable", "doc": "<p>The input dimension of the MLP backbone network.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.mlp.MLP.hidden_dims", "modulename": "clarena.backbones.mlp", "qualname": "MLP.hidden_dims", "kind": "variable", "doc": "<p>The hidden dimensions of the MLP backbone network.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.backbones.mlp.MLP.output_dim", "modulename": "clarena.backbones.mlp", "qualname": "MLP.output_dim", "kind": "variable", "doc": "<p>The output dimension of the MLP backbone network.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.mlp.MLP.num_fc_layers", "modulename": "clarena.backbones.mlp", "qualname": "MLP.num_fc_layers", "kind": "variable", "doc": "<p>The number of fully-connected layers in the MLP backbone network, which helps form the loops in constructing layers and forward pass.</p>\n", "annotation": ": int"}, {"fullname": "clarena.backbones.mlp.MLP.batch_normalization", "modulename": "clarena.backbones.mlp", "qualname": "MLP.batch_normalization", "kind": "variable", "doc": "<p>Whether to use batch normalization after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.mlp.MLP.activation", "modulename": "clarena.backbones.mlp", "qualname": "MLP.activation", "kind": "variable", "doc": "<p>Whether to use activation function after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.mlp.MLP.dropout", "modulename": "clarena.backbones.mlp", "qualname": "MLP.dropout", "kind": "variable", "doc": "<p>Whether to use dropout after the fully-connected layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.mlp.MLP.fc", "modulename": "clarena.backbones.mlp", "qualname": "MLP.fc", "kind": "variable", "doc": "<p>The list of fully connected (<code>nn.Linear</code>) layers.</p>\n", "annotation": ": torch.nn.modules.container.ModuleList"}, {"fullname": "clarena.backbones.mlp.MLP.forward", "modulename": "clarena.backbones.mlp", "qualname": "MLP.forward", "kind": "function", "doc": "<p>The forward pass for data. It is the same for all tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for certain algorithms that need to use hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.CLMLP", "modulename": "clarena.backbones.mlp", "qualname": "CLMLP", "kind": "class", "doc": "<p>Multi-layer perceptron (MLP), a.k.a. fully connected network. Used as a continual learning backbone.</p>\n\n<p>MLP is a dense network architecture with several fully connected layers, each followed by an activation function. The last layer connects to the CL output heads.</p>\n", "bases": "clarena.backbones.base.CLBackbone, MLP"}, {"fullname": "clarena.backbones.mlp.CLMLP.__init__", "modulename": "clarena.backbones.mlp", "qualname": "CLMLP.__init__", "kind": "function", "doc": "<p>Construct and initialize the CLMLP backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension. Any data need to be flattened before going in MLP. Note that it is not required in convolutional networks.</li>\n<li><strong>hidden_dims</strong> (<code>list[int]</code>): list of hidden layer dimensions. It can be empty list which means single-layer MLP, and it can be as many layers as you want. Note that it doesn't include the last dimension which we take as output dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension which connects to CL output heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code> | <code>None</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the fully-connected layers. Default <code>False</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the linear layer. Default <code>True</code>.</li>\n<li><strong>dropout</strong> (<code>float</code> | <code>None</code>): the probability for the dropout layer, if <code>None</code> this layer won't be used. Default <code>None</code>.        - <strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.mlp.CLMLP.forward", "modulename": "clarena.backbones.mlp", "qualname": "CLMLP.forward", "kind": "function", "doc": "<p>The forward pass for data. It is the same for all tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name; value (<code>Tensor</code>) is the hidden feature tensor. This is used for continual learning algorithms that need hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP", "kind": "class", "doc": "<p>HAT-masked multi-layer perceptron (MLP).</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</p>\n\n<p>MLP is a dense network architecture with several fully connected layers, each followed by an activation function. The last layer connects to the CL output heads.</p>\n\n<p>The mask is applied to units (neurons) in each fully connected layer. The mask is generated from the neuron-wise task embedding and the gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, MLP"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.__init__", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.__init__", "kind": "function", "doc": "<p>Construct and initialize the HAT-masked MLP backbone network with task embeddings. Note that batch normalization is incompatible with the HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): The input dimension. Any data need to be flattened before entering the MLP.</li>\n<li><strong>hidden_dims</strong> (<code>list[int]</code>): List of hidden layer dimensions. It can be an empty list, which means a single-layer MLP, and it can be as many layers as you want. Note that it doesn't include the last dimension, which we take as the output dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads.</li>\n<li><strong>gate</strong> (<code>str</code>): The type of gate function turning real-valued task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code> | <code>None</code>): Activation function of each layer (if not <code>None</code>). If <code>None</code>, this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>str</code> | <code>None</code>): How to use batch normalization after the fully connected layers; one of:\n<ul>\n<li><code>None</code>: no batch normalization layers.</li>\n<li><code>shared</code>: use a single batch normalization layer for all tasks. Note that this can cause catastrophic forgetting.</li>\n<li><code>independent</code>: use independent batch normalization layers for each task.</li>\n</ul></li>\n<li><strong>bias</strong> (<code>bool</code>): Whether to use bias in the linear layer. Default <code>True</code>.</li>\n<li><strong>dropout</strong> (<code>float</code> | <code>None</code>): The probability for the dropout layer. If <code>None</code>, this layer won't be used. Default <code>None</code>.        - <strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.batch_normalization", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.batch_normalization", "kind": "variable", "doc": "<p>The way to use batch normalization after the fully-connected layers. This overrides the <code>batch_normalization</code> argument in <code>MLP</code> class.</p>\n", "annotation": ": str | None"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.setup_task_id", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.setup_task_id", "kind": "function", "doc": "<p>Set up task <code>task_id</code>. This must be done before the <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.get_bn", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.get_bn", "kind": "function", "doc": "<p>Get the batch normalization layer used in the <code>forward()</code> method for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>fc_bn</strong> (<code>nn.Module</code> | <code>None</code>): The batch normalization module.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.initialize_independent_bn", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.initialize_independent_bn", "kind": "function", "doc": "<p>Initialize the independent batch normalization layer for the current task. This is called when a new task is created. Applies only when <code>batch_normalization</code> is 'independent'.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.store_bn", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.store_bn", "kind": "function", "doc": "<p>Store the batch normalization layer for the current task <code>self.task_id</code>. Applies only when <code>batch_normalization</code> is 'independent'.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.HATMaskMLP.forward", "modulename": "clarena.backbones.mlp", "qualname": "HATMaskMLP.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>self.task_id</code>. Task-specific masks for <code>self.task_id</code> are applied to units (neurons) in each fully connected layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): The maximum scaling factor in the gate function. Doesn't apply to the testing stage. See Sec. 2.4 in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): The current batch index. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): The total number of batches. Applies only to the training stage. For other stages, it is <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>int</code> | <code>None</code>): The test task ID. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): The mask for the current task. Keys (<code>str</code>) are the layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for continual learning algorithms that need hidden features. Although the HAT algorithm does not need this, it is still provided for API consistency for other HAT-based algorithms that inherit this <code>forward()</code> method of the <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.mlp.WSNMaskMLP", "modulename": "clarena.backbones.mlp", "qualname": "WSNMaskMLP", "kind": "class", "doc": "<p><a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks, 2022)</a> masked multi-layer perceptron (MLP).</p>\n\n<p><a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks, 2022)</a> is an architecture-based continual learning algorithm. It trains learnable parameter-wise importance and selects the most important $c\\%$ of the network parameters to be used for each task.</p>\n\n<p>MLP is a dense network architecture with several fully connected layers, each followed by an activation function. The last layer connects to the CL output heads.</p>\n\n<p>The mask is applied to the weights and biases in each fully connected layer. The mask is generated from the parameter-wise score and the gate function.</p>\n", "bases": "MLP, clarena.backbones.base.WSNMaskBackbone"}, {"fullname": "clarena.backbones.mlp.WSNMaskMLP.__init__", "modulename": "clarena.backbones.mlp", "qualname": "WSNMaskMLP.__init__", "kind": "function", "doc": "<p>Construct and initialize the WSN-masked MLP backbone network with task embeddings.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): The input dimension. Any data need to be flattened before entering the MLP.</li>\n<li><strong>hidden_dims</strong> (<code>list[int]</code>): List of hidden layer dimensions. It can be an empty list, which means a single-layer MLP, and it can be as many layers as you want. Note that it doesn't include the last dimension, which we take as the output dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): The output dimension that connects to CL output heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code> | <code>None</code>): Activation function of each layer (if not <code>None</code>). If <code>None</code>, this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): Whether to use bias in the linear layer. Default <code>True</code>.</li>\n<li><strong>dropout</strong> (<code>float</code> | <code>None</code>): The probability for the dropout layer. If <code>None</code>, this layer won't be used. Default <code>None</code>.        - <strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_dims</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.backbones.mlp.WSNMaskMLP.forward", "modulename": "clarena.backbones.mlp", "qualname": "WSNMaskMLP.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>self.task_id</code>. Task-specific masks for <code>self.task_id</code> are applied to units (neurons) in each fully connected layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): The stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>mask_percentage</strong> (<code>float</code>): The percentage of parameters to be masked. The value should be between 0 and 1.</li>\n<li><strong>test_mask</strong> (<code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> | <code>None</code>): The binary weight and bias masks used for testing. Applies only to the testing stage. For other stages, it is <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): The output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): The weight mask for the current task. Keys (<code>str</code>) are the layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has the same size (output features, input features) as the weight.</li>\n<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): The bias mask for the current task. Keys (<code>str</code>) are the layer names and values (<code>Tensor</code>) are the mask tensors. The mask tensor has the same size (output features, ) as the bias. If the layer doesn't have a bias, it is <code>None</code>.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): The hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for continual learning algorithms that need hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">mask_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">test_mask</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet", "modulename": "clarena.backbones.resnet", "kind": "module", "doc": "<p>The submodule in <code>backbones</code> for ResNet backbone network. It includes multiple versions of ResNet, including the basic ResNet, the continual learning ResNet, and the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT</a> masked ResNet.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall", "kind": "class", "doc": "<p>The smaller building block for ResNet-18/34.</p>\n\n<p>It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.__init__", "kind": "function", "doc": "<p>Construct and initialize the smaller building block.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.batch_normalization", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.batch_normalization", "kind": "variable", "doc": "<p>Whether to use batch normalization after convolutional layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.activation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.activation", "kind": "variable", "doc": "<p>Whether to use activation function after convolutional layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.full_1st_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.full_1st_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 1st weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.full_2nd_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.full_2nd_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 2nd weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.conv1", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.conv1", "kind": "variable", "doc": "<p>The 1st weight convolutional layer of the smaller building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.conv2", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.conv2", "kind": "variable", "doc": "<p>The 2nd weight convolutional layer of the smaller building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.identity_downsample", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.identity_downsample", "kind": "variable", "doc": "<p>The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn't match the output's. This case only happens when the number of input channels doesn't equal to the number of preceding output channels or a layer with stride &gt; 1 exists.</p>\n", "annotation": ": torch.nn.modules.module.Module"}, {"fullname": "clarena.backbones.resnet.ResNetBlockSmall.forward", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockSmall.forward", "kind": "function", "doc": "<p>The forward pass for data.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input feature maps.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge", "kind": "class", "doc": "<p>The larger building block for ResNet-50/101/152. It is referred to \"bottleneck\" building block in the paper.</p>\n\n<p>It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.__init__", "kind": "function", "doc": "<p>Construct and initialize the larger building block.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.batch_normalization", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.batch_normalization", "kind": "variable", "doc": "<p>Whether to use batch normalization after convolutional layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.activation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.activation", "kind": "variable", "doc": "<p>Whether to use activation function after convolutional layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.full_1st_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.full_1st_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 1st weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.full_2nd_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.full_2nd_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 2nd weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.full_3rd_layer_name", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.full_3rd_layer_name", "kind": "variable", "doc": "<p>Format and store full name of the 3rd weighted convolutional layer.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.conv1", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.conv1", "kind": "variable", "doc": "<p>The 1st weight convolutional layer of the larger building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.conv2", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.conv2", "kind": "variable", "doc": "<p>The 2nd weight convolutional layer of the larger building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.conv3", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.conv3", "kind": "variable", "doc": "<p>The 3rd weight convolutional layer of the larger building block.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.identity_downsample", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.identity_downsample", "kind": "variable", "doc": "<p>The convolutional layer for downsampling identity in the shortcut connection if the dimension of identity from input doesn't match the output's. This case only happens when the number of input channels doesn't equal to the number of preceding output channels or a layer with stride &gt; 1 exists.</p>\n", "annotation": ": torch.nn.modules.module.Module"}, {"fullname": "clarena.backbones.resnet.ResNetBlockLarge.forward", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBlockLarge.forward", "kind": "function", "doc": "<p>The forward pass for data.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input feature maps.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.ResNetBase", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase", "kind": "class", "doc": "<p>The base class of <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">residual network (ResNet)</a>.</p>\n\n<p>ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (<code>ResNetBlockSmall</code>) or large (<code>ResNetBlockLarge</code>). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it's called residual (find \"shortcut connections\" in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</p>\n", "bases": "clarena.backbones.base.Backbone"}, {"fullname": "clarena.backbones.resnet.ResNetBase.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>building_block_type</strong> (<code>ResNetBlockSmall</code> | <code>ResNetBlockLarge</code>): the type of building block used in the ResNet.</li>\n<li><strong>building_block_nums</strong> (<code>tuple[int, int, int, int]</code>): the number of building blocks in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_preceding_output_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_input_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_type</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">ResNetBlockSmall</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">ResNetBlockLarge</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_nums</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_input_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNetBase.batch_normalization", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.batch_normalization", "kind": "variable", "doc": "<p>Whether to use batch normalization after convolutional layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBase.activation", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.activation", "kind": "variable", "doc": "<p>Whether to use activation function after convolutional layers.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv1", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv1", "kind": "variable", "doc": "<p>The 1st weight convolutional layer of the entire ResNet. It  is always with fixed kernel size 7x7, stride 2, and padding 3.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.maxpool", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.maxpool", "kind": "variable", "doc": "<p>The max pooling layer which is laid in between 1st and 2nd convolutional layers with kernel size 3x3, stride 2.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv2x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv2x", "kind": "variable", "doc": "<p>The 2nd convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv3x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv3x", "kind": "variable", "doc": "<p>The 3rd convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv4x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv4x", "kind": "variable", "doc": "<p>The 4th convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.conv5x", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.conv5x", "kind": "variable", "doc": "<p>The 5th convolutional layer of the ResNet, which contains multiple blocks.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.avepool", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.avepool", "kind": "variable", "doc": "<p>The average pooling layer which is laid after the convolutional layers and before feature maps are flattened.</p>\n"}, {"fullname": "clarena.backbones.resnet.ResNetBase.forward", "modulename": "clarena.backbones.resnet", "qualname": "ResNetBase.forward", "kind": "function", "doc": "<p>The forward pass for data. It is the same for all tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed into heads. This is the main target of backpropagation.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.ResNet18", "modulename": "clarena.backbones.resnet", "qualname": "ResNet18", "kind": "class", "doc": "<p>ResNet-18 backbone network.</p>\n\n<p>This is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet18.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet18.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-18 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>pretrained_weights</strong> (<code>str</code>): the name of pretrained weights to be loaded. See <a href=\"https://pytorch.org/vision/main/models.html\">TorchVision docs</a>. If <code>None</code>, no pretrained weights are loaded. Default <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained_weights</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet34", "modulename": "clarena.backbones.resnet", "qualname": "ResNet34", "kind": "class", "doc": "<p>ResNet-34 backbone network.</p>\n\n<p>This is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet34.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet34.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-34 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet50", "modulename": "clarena.backbones.resnet", "qualname": "ResNet50", "kind": "class", "doc": "<p>ResNet-50 backbone network.</p>\n\n<p>This is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet50.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet50.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-50 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet101", "modulename": "clarena.backbones.resnet", "qualname": "ResNet101", "kind": "class", "doc": "<p>ResNet-101 backbone network.</p>\n\n<p>This is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet101.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet101.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-101 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.ResNet152", "modulename": "clarena.backbones.resnet", "qualname": "ResNet152", "kind": "class", "doc": "<p>ResNet-152 backbone network.</p>\n\n<p>This is the largest architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "ResNetBase"}, {"fullname": "clarena.backbones.resnet.ResNet152.__init__", "modulename": "clarena.backbones.resnet", "qualname": "ResNet152.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-152 backbone network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.CLResNet18", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet18", "kind": "class", "doc": "<p>The ResNet-18 backbone network for continual learning.</p>\n\n<p>This is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "clarena.backbones.base.CLBackbone, ResNet18"}, {"fullname": "clarena.backbones.resnet.CLResNet18.__init__", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet18.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-18 backbone network for continual learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>pretrained_weights</strong> (<code>str</code>): the name of pretrained weights to be loaded. See <a href=\"https://pytorch.org/vision/main/models.html\">TorchVision docs</a>. If <code>None</code>, no pretrained weights are loaded. Default <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained_weights</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.CLResNet34", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet34", "kind": "class", "doc": "<p>The ResNet-34 backbone network for continual learning.</p>\n\n<p>This is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "clarena.backbones.base.CLBackbone, ResNet34"}, {"fullname": "clarena.backbones.resnet.CLResNet34.__init__", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet34.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-34 backbone network for continual learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>pretrained_weights</strong> (<code>str</code>): the name of pretrained weights to be loaded. See <a href=\"https://pytorch.org/vision/main/models.html\">TorchVision docs</a>. If <code>None</code>, no pretrained weights are loaded. Default <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained_weights</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.CLResNet50", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet50", "kind": "class", "doc": "<p>The ResNet-50 backbone network for continual learning.</p>\n\n<p>This is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "clarena.backbones.base.CLBackbone, ResNet50"}, {"fullname": "clarena.backbones.resnet.CLResNet50.__init__", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet50.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-50 backbone network for continual learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>pretrained_weights</strong> (<code>str</code>): the name of pretrained weights to be loaded. See <a href=\"https://pytorch.org/vision/main/models.html\">TorchVision docs</a>. If <code>None</code>, no pretrained weights are loaded. Default <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained_weights</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.CLResNet101", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet101", "kind": "class", "doc": "<p>The ResNet-101 backbone network for continual learning.</p>\n\n<p>This is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "clarena.backbones.base.CLBackbone, ResNet101"}, {"fullname": "clarena.backbones.resnet.CLResNet101.__init__", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet101.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-101 backbone network for continual learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>pretrained_weights</strong> (<code>str</code>): the name of pretrained weights to be loaded. See <a href=\"https://pytorch.org/vision/main/models.html\">TorchVision docs</a>. If <code>None</code>, no pretrained weights are loaded. Default <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained_weights</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.CLResNet152", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet152", "kind": "class", "doc": "<p>The ResNet-152 backbone network for continual learning.</p>\n\n<p>This is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n", "bases": "clarena.backbones.base.CLBackbone, ResNet152"}, {"fullname": "clarena.backbones.resnet.CLResNet152.__init__", "modulename": "clarena.backbones.resnet", "qualname": "CLResNet152.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-152 backbone network for continual learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>batch_normalization</strong> (<code>bool</code>): whether to use batch normalization after the weight convolutional layers. Default <code>True</code>, same as what the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a> does.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>pretrained_weights</strong> (<code>str</code>): the name of pretrained weights to be loaded. See <a href=\"https://pytorch.org/vision/main/models.html\">TorchVision docs</a>. If <code>None</code>, no pretrained weights are loaded. Default <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained_weights</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall", "kind": "class", "doc": "<p>The smaller building block for HAT masked ResNet-18/34.</p>\n\n<p>It consists of 2 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (left) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the neuron-wise task embedding and gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, ResNetBlockSmall"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall.__init__", "kind": "function", "doc": "<p>Construct and initialize the smaller building block with task embedding.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (last) convolutional layer where the 1st convolutional layer remain stride of 1.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n<li><strong>batch_normalization</strong> (<code>str</code> | <code>None</code>): the way to use batch normalization after the fully-connected layers; one of:\n<ul>\n<li><code>None</code>: no batch normalization layers.</li>\n<li><code>shared</code>: use a single batch normalization layer for all tasks. Note that this can cause catastrophic forgetting.</li>\n<li><code>independent</code>: use independent batch normalization layers for each task.</li>\n</ul></li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">batch_normalization</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall.setup_task_id", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall.setup_task_id", "kind": "function", "doc": "<p>Setup about task <code>task_id</code>. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockSmall.forward", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockSmall.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>self.task_id</code>. Task-specific mask for <code>self.task_id</code> are applied to the units which are channels in each weighted convolutional layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the test task ID. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockLarge", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockLarge", "kind": "class", "doc": "<p>The larger building block for ResNet-50/101/152. It is referred to \"bottleneck\" building block in the ResNet paper.</p>\n\n<p>It consists of 3 weight convolutional layers, each followed by an activation function. See Table 1 or Figure 5 (right) in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the neuron-wise task embedding and gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, ResNetBlockLarge"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockLarge.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockLarge.__init__", "kind": "function", "doc": "<p>Construct and initialize the larger building block with task embedding.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outer_layer_name</strong> (<code>str</code>): pass the name of the multi-building-block layer that contains this building block to construct the full name of each weighted convolutional layer.</li>\n<li><strong>block_idx</strong> (<code>int</code>): the index of the building blocks in the multi-building-block layer to construct the full name of each weighted convolutional layer.</li>\n<li><strong>preceding_output_channels</strong> (<code>int</code>): the number of channels of preceding output of this particular building block.</li>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block.</li>\n<li><strong>overall_stride</strong> (<code>int</code>): the overall stride of this building block. This stride is performed at 2nd (middle) convolutional layer where 1st and 3rd convolutional layers remain stride of 1.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">outer_layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">block_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">overall_stride</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBlockLarge.forward", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBlockLarge.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>self.task_id</code>. Task-specific mask for <code>self.task_id</code> are applied to the units which are channels in each weighted convolutional layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the test task ID. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature maps.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase", "kind": "class", "doc": "<p>The base class of HAT masked <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">residual network (ResNet)</a>.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet is a convolutional network architecture, which has 1st convolutional parameter layer and a maxpooling layer, connecting to 4 convolutional layers which contains multiple convolutional parameter layer. Each layer of the 4 are constructed from basic building blocks which are either small (<code>ResNetBlockSmall</code>) or large (<code>ResNetBlockLarge</code>). Each building block contains several convolutional parameter layers. The building blocks are connected by a skip connection which is a direct connection from the input of the block to the output of the block, and this is why it's called residual (find \"shortcut connections\" in the paper for more details). After the 5th convolutional layer, there are average pooling layer and a fully connected layer which connects to the CL output heads.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the neuron-wise task embedding and gate function.</p>\n", "bases": "clarena.backbones.base.HATMaskBackbone, ResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase.__init__", "kind": "function", "doc": "<p>Construct and initialize the HAT masked ResNet backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input. Image data are kept channels when going in ResNet. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>building_block_type</strong> (<code>HATMaskResNetBlockSmall</code> | <code>HATMaskResNetBlockLarge</code>): the type of building block used in the ResNet.</li>\n<li><strong>building_block_nums</strong> (<code>tuple[int, int, int, int]</code>): the number of building blocks in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_preceding_output_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of preceding output of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>building_block_input_channels</strong> (<code>tuple[int, int, int, int]</code>): the number of channels of input of each building block in the 2-5 convolutional layer correspondingly.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>, because batch normalization are doing the similar thing with bias.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_type</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">HATMaskResNetBlockSmall</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">resnet</span><span class=\"o\">.</span><span class=\"n\">HATMaskResNetBlockLarge</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_nums</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_preceding_output_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">building_block_input_channels</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase.update_multiple_blocks_task_embedding", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase.update_multiple_blocks_task_embedding", "kind": "function", "doc": "<p>Collect the task embeddings in the multiple building blocks (2-5 convolutional layers) and sync to the weighted layer names list in the outer network.</p>\n\n<p>This should only be called explicitly after the <code>__init__()</code> method, just because task embedding as <code>nn.Module</code> instance was wiped out at the beginning of it.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNetBase.forward", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNetBase.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>self.task_id</code>. Task-specific mask for <code>self.task_id</code> are applied to the units which are channels in each weighted convolutional layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code> | <code>None</code>): the maximum scaling factor in the gate function. Doesn't apply to testing stage. See chapter 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>test_task_id</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the test task ID. Applies only to testing stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output_feature</strong> (<code>Tensor</code>): the output feature tensor to be passed to the heads.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Keys (<code>str</code>) are the weighted layer names and values (<code>Tensor</code>) are the hidden feature tensors. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet18", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet18", "kind": "class", "doc": "<p>HAT masked ResNet-18 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-18 is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 18 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the neuron-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet18.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet18.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-18 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n<li><strong>pretrained_weights</strong> (<code>str</code>): the name of pretrained weights to be loaded. See <a href=\"https://pytorch.org/vision/main/models.html\">TorchVision docs</a>. If <code>None</code>, no pretrained weights are loaded. Default <code>None</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pretrained_weights</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet34", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet34", "kind": "class", "doc": "<p>HAT masked ResNet-34 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-34 is a smaller architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 34 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the neuron-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet34.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet34.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-34 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet50", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet50", "kind": "class", "doc": "<p>HAT masked ResNet-50 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-50 is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 50 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the neuron-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet50.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet50.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-50 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet101", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet101", "kind": "class", "doc": "<p>HAT masked ResNet-101 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-101 is a larger architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 101 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the neuron-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet101.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet101.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-101 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet152", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet152", "kind": "class", "doc": "<p>HAT masked ResNet-152 backbone network.</p>\n\n<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task, 2018)</a> is an architecture-based continual learning approach that uses learnable hard attention masks to select the task-specific parameters.</p>\n\n<p>ResNet-152 is the largest architecture proposed in the <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\">original ResNet paper</a>. It consists of 152 weight convolutional layers in total. See Table 1 in the paper for details.</p>\n\n<p>Mask is applied to the units which are output channels in each weighted convolutional layer. The mask is generated from the neuron-wise task embedding and gate function.</p>\n", "bases": "HATMaskResNetBase"}, {"fullname": "clarena.backbones.resnet.HATMaskResNet152.__init__", "modulename": "clarena.backbones.resnet", "qualname": "HATMaskResNet152.__init__", "kind": "function", "doc": "<p>Construct and initialize the ResNet-152 backbone network with task embedding. Note that batch normalization is incompatible with HAT mechanism.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_channels</strong> (<code>int</code>): the number of channels of input of this building block. Note that convolutional networks require number of input channels instead of dimension.</li>\n<li><strong>output_dim</strong> (<code>int</code>): the output dimension after flattening at last which connects to CL output heads. Although this is not determined by us but the architecture built before the flattening layer, we still need to provide this to construct the heads.</li>\n<li><strong>gate</strong> (<code>str</code>): the type of gate function turning the real value task embeddings into attention masks; one of:\n<ul>\n<li><code>sigmoid</code>: the sigmoid function.</li>\n</ul></li>\n<li><strong>activation_layer</strong> (<code>nn.Module</code>): activation function of each layer (if not <code>None</code>), if <code>None</code> this layer won't be used. Default <code>nn.ReLU</code>.</li>\n<li><strong>bias</strong> (<code>bool</code>): whether to use bias in the convolutional layer. Default <code>False</code>.</li>\n<li><strong>kwargs</strong>: Reserved for multiple inheritance.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">output_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\tactivation_layer: torch.nn.modules.module.Module | None = &lt;class &#x27;torch.nn.modules.activation.ReLU&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "clarena.callbacks", "modulename": "clarena.callbacks", "kind": "module", "doc": "<h1 id=\"callbacks\">Callbacks</h1>\n\n<p>This submodule provides <strong>callbacks</strong> (other than metric callbacks) that can be used in CLArena.</p>\n\n<p>The callbacks inherit from <code>lightning.Callback</code>.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about how to configure and implement callbacks:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/callbacks\"><strong>Configure Callbacks</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/callback\"><strong>Implement Custom Callback</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.callbacks.cl_rich_progress_bar", "modulename": "clarena.callbacks.cl_rich_progress_bar", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for <code>CLRichProgressBar</code>.</p>\n"}, {"fullname": "clarena.callbacks.cl_rich_progress_bar.CLRichProgressBar", "modulename": "clarena.callbacks.cl_rich_progress_bar", "qualname": "CLRichProgressBar", "kind": "class", "doc": "<p>Customized <code>RichProgressBar</code> for continual learning.</p>\n", "bases": "lightning.pytorch.callbacks.progress.rich_progress.RichProgressBar"}, {"fullname": "clarena.callbacks.cl_rich_progress_bar.CLRichProgressBar.get_metrics", "modulename": "clarena.callbacks.cl_rich_progress_bar", "qualname": "CLRichProgressBar.get_metrics", "kind": "function", "doc": "<p>Filter out the version number from the metrics displayed in the progress bar.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"o\">*</span><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger", "modulename": "clarena.callbacks.pylogger", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for Pylogger callbacks.</p>\n"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger", "kind": "class", "doc": "<p>Provides additional logging messages for during continual learning progress.</p>\n\n<p>Put logging messages here if you don't want to mess up the <code>CLAlgorithm</code> (<code>LightningModule</code>) with a huge amount of logging codes.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger.on_fit_start", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger.on_fit_start", "kind": "function", "doc": "<p>Log messages for the start of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger.on_train_end", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger.on_train_end", "kind": "function", "doc": "<p>Log messages for the end of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger.on_test_start", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger.on_test_start", "kind": "function", "doc": "<p>Log messages for the start of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CLPylogger.on_test_end", "modulename": "clarena.callbacks.pylogger", "qualname": "CLPylogger.on_test_end", "kind": "function", "doc": "<p>Log messages for the end of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger", "kind": "class", "doc": "<p>Provides additional logging messages for during continual learning progress.</p>\n\n<p>Put logging messages here if you don't want to mess up the <code>CLAlgorithm</code> (<code>LightningModule</code>) with a huge amount of logging codes.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger.on_fit_start", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger.on_fit_start", "kind": "function", "doc": "<p>Log messages for the start of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger.on_train_end", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger.on_train_end", "kind": "function", "doc": "<p>Log messages for the end of training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger.on_test_start", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger.on_test_start", "kind": "function", "doc": "<p>Log messages for the start of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.CULPylogger.on_test_end", "modulename": "clarena.callbacks.pylogger", "qualname": "CULPylogger.on_test_end", "kind": "function", "doc": "<p>Log messages for the end of testing task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger", "kind": "class", "doc": "<p>Pylogger Callback provides additional logging for during multi-task learning progress.</p>\n\n<p>Put logging messages here if you don't want to mess up the <code>MTLAlgorithm</code> (<code>LightningModule</code>) with a huge amount of logging codes.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger.on_fit_start", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger.on_fit_start", "kind": "function", "doc": "<p>Log messages for the start of training.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger.on_train_end", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger.on_train_end", "kind": "function", "doc": "<p>Log messages for the end of training.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger.on_test_start", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger.on_test_start", "kind": "function", "doc": "<p>Log messages for the start of testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.MTLPylogger.on_test_end", "modulename": "clarena.callbacks.pylogger", "qualname": "MTLPylogger.on_test_end", "kind": "function", "doc": "<p>Log messages for the end of testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger", "kind": "class", "doc": "<p>Pylogger Callback provides additional logging for during single-task learning progress.</p>\n\n<p>Put logging messages here if you don't want to mess up the <code>STLAlgorithm</code> (<code>LightningModule</code>) with a huge amount of logging codes.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger.on_fit_start", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger.on_fit_start", "kind": "function", "doc": "<p>Log messages for the start of training.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger.on_train_end", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger.on_train_end", "kind": "function", "doc": "<p>Log messages for the end of training.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger.on_test_start", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger.on_test_start", "kind": "function", "doc": "<p>Log messages for the start of testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.pylogger.STLPylogger.on_test_end", "modulename": "clarena.callbacks.pylogger", "qualname": "STLPylogger.on_test_end", "kind": "function", "doc": "<p>Log messages for the end of testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.save_first_batch_images", "modulename": "clarena.callbacks.save_first_batch_images", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for callback of saving first batch images.</p>\n"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages", "kind": "class", "doc": "<p>Saves images and labels of the first batch of training data into files. In continual learning / unlearning, applies to all tasks.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.__init__", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.__init__", "kind": "function", "doc": "<p>Initialize the Save First Batch Images Callback.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): the directory to save images and labels as files. Better inside the output directory.</li>\n<li><strong>img_prefix</strong> (<code>str</code>): the prefix for image files.</li>\n<li><strong>labels_filename</strong> (<code>str</code>): the filename for the labels file as texts.</li>\n<li><strong>task_ids_filename</strong> (<code>str</code> | <code>None</code>): the filename for the task IDs file as texts. Only used in MTL algorithms. If <code>None</code>, no task IDs file is saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">img_prefix</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sample&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">labels_filename</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;labels.txt&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">task_ids_filename</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;tasks.txt&#39;</span></span>)</span>"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.save_dir", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.save_dir", "kind": "variable", "doc": "<p>Store the directory to save images and labels as files.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.img_prefix", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.img_prefix", "kind": "variable", "doc": "<p>Store the prefix for image files.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.labels_filename", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.labels_filename", "kind": "variable", "doc": "<p>Store the filename for the labels file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.task_ids_filename", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.task_ids_filename", "kind": "variable", "doc": "<p>Store the filename for the task IDs file. Only used in MTL algorithms. If <code>None</code>, no task IDs file is saved.</p>\n", "annotation": ": str | None"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.called", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.called", "kind": "variable", "doc": "<p>Flag to avoid calling the callback multiple times.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.on_train_batch_end", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.on_train_batch_end", "kind": "function", "doc": "<p>Save images and labels into files in the first batch of training data at the beginning of the training of the task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.save_first_batch_images.SaveFirstBatchImages.on_validation_batch_end", "modulename": "clarena.callbacks.save_first_batch_images", "qualname": "SaveFirstBatchImages.on_validation_batch_end", "kind": "function", "doc": "<p>Called when the validation batch ends.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "clarena.callbacks.save_models", "modulename": "clarena.callbacks.save_models", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for callback of saving models.</p>\n"}, {"fullname": "clarena.callbacks.save_models.SaveModels", "modulename": "clarena.callbacks.save_models", "qualname": "SaveModels", "kind": "class", "doc": "<p>Saves the model at the end of training. In continual learning / unlearning, applies to all tasks.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.callbacks.save_models.SaveModels.__init__", "modulename": "clarena.callbacks.save_models", "qualname": "SaveModels.__init__", "kind": "function", "doc": "<p>Initialize the SaveModel callback.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_path</strong> (<code>str</code>): the path to save the model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">save_after_each_task</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "clarena.callbacks.save_models.SaveModels.save_dir", "modulename": "clarena.callbacks.save_models", "qualname": "SaveModels.save_dir", "kind": "variable", "doc": "<p>Store the path to save the model.</p>\n"}, {"fullname": "clarena.callbacks.save_models.SaveModels.save_after_each_task", "modulename": "clarena.callbacks.save_models", "qualname": "SaveModels.save_after_each_task", "kind": "variable", "doc": "<p>Whether to save the model after each task in continual learning / unlearning.</p>\n"}, {"fullname": "clarena.callbacks.save_models.SaveModels.on_test_end", "modulename": "clarena.callbacks.save_models", "qualname": "SaveModels.on_test_end", "kind": "function", "doc": "<p>Save the model at the end of each training task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms", "modulename": "clarena.cl_algorithms", "kind": "module", "doc": "<h1 id=\"continual-learning-algorithms\">Continual Learning Algorithms</h1>\n\n<p>This submodule provides the <strong>continual learning algorithms</strong> in CLArena.</p>\n\n<p>Here are the base classes for CL algorithms, which inherit from PyTorch Lightning <code>LightningModule</code>:</p>\n\n<ul>\n<li><code>CLAlgorithm</code>: the base class for all continual learning algorithms.\n<ul>\n<li><code>UnlearnableCLAlgorithm</code>: the base class for unlearnable continual learning algorithms.</li>\n</ul></li>\n</ul>\n\n<p>Please note that this is an API documentation. Please refer to the main documentation pages for more information about and how to configure and implement CL algorithms:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/cl-algorithm\"><strong>Configure CL Algorithm</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/cl-algorithm\"><strong>Implement Custom CL Algorithm</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-methodology\"><strong>A Beginners' Guide to Continual Learning (Methodology Overview)</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm", "kind": "class", "doc": "<p>The base class of continual learning algorithms.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.__init__", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.backbone", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.backbone", "kind": "variable", "doc": "<p>The backbone network.</p>\n", "annotation": ": clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.heads", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.heads", "kind": "variable", "doc": "<p>The output heads.</p>\n", "annotation": ": clarena.heads.heads_til.HeadsTIL | clarena.heads.heads_cil.HeadsCIL"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.optimizer_t", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.optimizer_t", "kind": "variable", "doc": "<p>Optimizer (partially initialized) for the current task <code>self.task_id</code>. Will be equipped with parameters in <code>configure_optimizers()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.lr_scheduler_t", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.lr_scheduler_t", "kind": "variable", "doc": "<p>Learning rate scheduler for the optimizer of the current task <code>self.task_id</code>. If <code>None</code>, no scheduler is used.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler | None"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.criterion", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.criterion", "kind": "variable", "doc": "<p>Loss function between the output logits and the target labels. Default is cross-entropy loss.</p>\n"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.if_forward_func_return_logits_only", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.if_forward_func_return_logits_only", "kind": "variable", "doc": "<p>Whether the <code>forward()</code> method returns logits only. If <code>False</code>, it returns a dictionary containing logits and other information. Default is <code>False</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.task_id", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.processed_task_ids", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.sanity_check", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.setup_task_id", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.setup_task_id", "kind": "function", "doc": "<p>Set up which task the CL experiment is on. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n<li><strong>num_classes</strong> (<code>int</code>): the number of classes in the task.</li>\n<li><strong>optimizer</strong> (<code>Optimizer</code>): the optimizer object (partially initialized) for the task.</li>\n<li><strong>lr_scheduler</strong> (<code>LRScheduler</code> | <code>None</code>): the learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">LRScheduler</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.get_test_task_id_from_dataloader_idx", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.get_test_task_id_from_dataloader_idx", "kind": "function", "doc": "<p>Get the test task ID from the dataloader index.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the dataloader index.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_task_id</strong> (<code>int</code>): the test task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.set_forward_func_return_logits_only", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.set_forward_func_return_logits_only", "kind": "function", "doc": "<p>Set whether the <code>forward()</code> method returns logits only. This is useful for some CL algorithms that require the forward function to return logits only, such as FG-AdaHAT.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>forward_func_return_logits_only</strong> (<code>bool</code>): whether the <code>forward()</code> method returns logits only. If <code>False</code>, it returns a dictionary containing logits and other information.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">forward_func_return_logits_only</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.preceding_layer", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.preceding_layer", "kind": "function", "doc": "<p>Get the preceding layer of the given layer (including backbone and output heads). If the given layer is the first layer, return <code>None</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of the layer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>preceding_layer</strong> (<code>nn.Module</code> | <code>None</code>): the preceding layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.next_layer", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.next_layer", "kind": "function", "doc": "<p>Get the next layer of the given layer (including backbone and output heads). If the given layer is the last layer, return <code>None</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of the layer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>preceding_layer</strong> (<code>nn.Module</code> | <code>None</code>): the next layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.forward", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>. This definition provides a template that many CL algorithm including the vanilla Finetuning algorithm use. It works both for TIL and CIL.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID where the data are from. If stage is 'train' or <code>validation</code>, it is usually from the current task <code>self.task_id</code>. If stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. In CIL, they are not provided, so it is just a placeholder for API consistence but never used, and best practices are not to provide this argument and leave it as the default value.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.CLAlgorithm.configure_optimizers", "modulename": "clarena.cl_algorithms", "qualname": "CLAlgorithm.configure_optimizers", "kind": "function", "doc": "<p>Configure optimizer hooks by Lightning. See <a href=\"https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers\">Lightning docs</a> for more details.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm", "kind": "class", "doc": "<p>The base class of unlearnable continual learning algorithms.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.__init__", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.unlearning_task_ids", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.unlearning_task_ids", "kind": "variable", "doc": "<p>The list of task IDs that are requested to be unlearned after training <code>self.task_id</code>.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.unlearned_task_ids", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.unlearned_task_ids", "kind": "variable", "doc": "<p>The list of task IDs that have been unlearned in the experiment.</p>\n", "annotation": ": set[int]"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.sanity_check", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.UnlearnableCLAlgorithm.aggregated_backbone_output", "modulename": "clarena.cl_algorithms", "qualname": "UnlearnableCLAlgorithm.aggregated_backbone_output", "kind": "function", "doc": "<p>Get the aggregated backbone output for the input data. All parts of backbones should be aggregated together.</p>\n\n<p>This output feature is used for measuring unlearning metrics, such as Distribution Distance (DD). An aggregated output involving every part of the backbone is needed to ensure the fairness of the metric.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>output</strong> (<code>Tensor</code>): the aggregated backbone output tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat", "modulename": "clarena.cl_algorithms.adahat", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT (Adaptive Hard Attention to the Task)</a> algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT", "kind": "class", "doc": "<p><a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT (Adaptive Hard Attention to the Task)</a> algorithm.</p>\n\n<p>An architecture-based continual learning approach that improves <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> by introducing adaptive soft gradient clipping based on parameter importance and network sparsity.</p>\n\n<p>We implement AdaHAT as a subclass of HAT, as it shares the same <code>forward()</code>, <code>compensate_task_embedding_gradients()</code>, <code>training_step()</code>, <code>on_train_end()</code>, <code>validation_step()</code>, and <code>test_step()</code> methods as the <code>HAT</code> class.</p>\n", "bases": "clarena.cl_algorithms.hat.HAT"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.__init__", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.__init__", "kind": "function", "doc": "<p>Initialize the AdaHAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with the HAT mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. AdaHAT supports only TIL (Task-Incremental Learning).</li>\n<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:\n<ol>\n<li>'adahat': set gradients of parameters linking to masked units to a soft adjustment rate in the original AdaHAT approach (allows slight updates on previous-task parameters). See Eqs. (8) and (9) in Sec. 3.1 of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'adahat_no_sum': as above but without parameter-importance (i.e., no summative mask). See Sec. 4.3 (ablation study) in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'adahat_no_reg': as above but without network sparsity (i.e., no mask sparsity regularization term). See Sec. 4.3 (ablation study) in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n</ol></li>\n<li><strong>adjustment_intensity</strong> (<code>float</code>): hyperparameter, controls the overall intensity of gradient adjustment (the $\\alpha$ in Eq. (9) of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>).</li>\n<li><strong>s_max</strong> (<code>float</code>): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>clamp_threshold</strong> (<code>float</code>): the threshold for task embedding gradient compensation. See Sec. 2.5 \"Embedding Gradient Compensation\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>mask_sparsity_reg_factor</strong> (<code>float</code>): hyperparameter, the regularization factor for mask sparsity.</li>\n<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularization, must be one of:\n<ol>\n<li>'original' (default): the original mask sparsity regularization in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li>'cross': the cross version of mask sparsity regularization.</li>\n</ol></li>\n<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialization mode for task embeddings, must be one of:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit the task embedding from the last task.</li>\n</ol></li>\n<li><strong>epsilon</strong> (<code>float</code>): the value added to network sparsity to avoid division by zero (appearing in Eq. (9) of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>).</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">HATMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_intensity</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">clamp_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">task_embedding_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;N01&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.adjustment_intensity", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.adjustment_intensity", "kind": "variable", "doc": "<p>The adjustment intensity in Eq. (9) of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.epsilon", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.epsilon", "kind": "variable", "doc": "<p>The small value to avoid division by zero (appearing in Eq. (9) of the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>).</p>\n", "annotation": ": float | None"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.summative_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.summative_mask_for_previous_tasks", "kind": "variable", "doc": "<p>The summative binary attention mask $\\mathrm{M}^{<t,\\text{sum}}$ of previous tasks $1,\\cdots, t-1$, gated from the task embedding. It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.automatic_optimization", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.sanity_check", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.on_train_start", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.on_train_start", "kind": "function", "doc": "<p>Additionally initialize the summative mask at the beginning of the first task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.clip_grad_by_adjustment", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.clip_grad_by_adjustment", "kind": "function", "doc": "<p>Clip the gradients by the adjustment rate. See Eq. (8) in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p>Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</p>\n\n<p>Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code> | <code>None</code>): the network sparsity (i.e., the mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. Applies only to mode <code>adahat</code> and <code>adahat_no_sum</code>, not <code>adahat_no_reg</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate_weight</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for weights. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>adjustment_rate_bias</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for biases. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">network_sparsity</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.adahat.AdaHAT.on_train_end", "modulename": "clarena.cl_algorithms.adahat", "qualname": "AdaHAT.on_train_end", "kind": "function", "doc": "<p>Additionally update the summative mask after training the task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbp", "modulename": "clarena.cl_algorithms.cbp", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP (Continual Backpropagation)</a> algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.cbp.CBP", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP", "kind": "class", "doc": "<p><a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP (Continual Backpropagation)</a> algorithm.</p>\n\n<p>A continual learning approach that reinitializes a small number of units during training, using an utility measures to determine which units to reinitialize. It aims to address loss of plasticity problem for learning new tasks, yet not very well solve the catastrophic forgetting problem in continual learning.</p>\n\n<p>We implement CBP as a subclass of Finetuning algorithm, as CBP has the same <code>forward()</code>, <code>training_step()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.__init__", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.__init__", "kind": "function", "doc": "<p>Initialize the Finetuning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>replacement_rate</strong> (<code>float</code>): the replacement rate of units. It is the precentage of units to be reinitialized during training.</li>\n<li><strong>maturity_threshold</strong> (<code>int</code>): the maturity threshold of units. It is the number of training steps before a unit can be reinitialized.</li>\n<li><strong>utility_decay_rate</strong> (<code>float</code>): the utility decay rate of units. It is the rate at which the utility of a unit decays over time.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">replacement_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">maturity_threshold</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">utility_decay_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.replacement_rate", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.replacement_rate", "kind": "variable", "doc": "<p>The replacement rate of units.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.maturity_threshold", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.maturity_threshold", "kind": "variable", "doc": "<p>The maturity threshold of units.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.utility_decay_rate", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.utility_decay_rate", "kind": "variable", "doc": "<p>The utility decay rate of units.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.contribution_utility", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.contribution_utility", "kind": "variable", "doc": "<p>The contribution utility of units. See equation (1) in the <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">continual backpropagation paper</a>. Keys are layer names and values are the utility tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units, ).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.num_replacements", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.num_replacements", "kind": "variable", "doc": "<p>The number of replacements of units in each layer. Keys are layer names and values are the number of replacements for the layer.</p>\n", "annotation": ": dict[str, int]"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.age", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.age", "kind": "variable", "doc": "<p>The age of units. Keys are layer names and values are the age tensor for the layer. The age tensor is the same size as the feature tensor with size (1, number of units).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.on_train_start", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.on_train_start", "kind": "function", "doc": "<p>Initialize the utility, number of replacements and age for each layer as zeros.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.cbp.CBP.on_train_batch_end", "modulename": "clarena.cl_algorithms.cbp", "qualname": "CBP.on_train_batch_end", "kind": "function", "doc": "<p>Update the contribution utility and age of units after each training step, and conduct reinitialization of units based on utility measures. This is the core of the CBP algorithm.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc", "modulename": "clarena.cl_algorithms.ewc", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC (Elastic Weight Consolidation) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC", "kind": "class", "doc": "<p><a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC (Elastic Weight Consolidation)</a> algorithm.</p>\n\n<p>A regularization-based approach that calculates the fisher information as parameter importance for the previous tasks and penalizes the current task loss with the importance of the parameters.</p>\n\n<p>We implement EWC as a subclass of Finetuning algorithm, as EWC has the same <code>forward()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.__init__", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.__init__", "kind": "function", "doc": "<p>Initialize the HAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>parameter_change_reg_factor</strong> (<code>float</code>): the parameter change regularization factor. It controls the strength of preventing forgetting.</li>\n<li><strong>when_calculate_fisher_information</strong> (<code>str</code>): when to calculate the fisher information. It should be one of the following:\n<ol>\n<li>'train_end': calculate the fisher information at the end of training of the task.</li>\n<li>'train': accumulate the fisher information in the training step of the task.</li>\n</ol></li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">parameter_change_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">when_calculate_fisher_information</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_importance", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_importance", "kind": "variable", "doc": "<p>The parameter importance of each previous task. Keys are task IDs and values are the corresponding importance. Each importance entity is a dict where keys are parameter names (named by <code>named_parameters()</code> of the <code>nn.Module</code>) and values are the importance tensor for the layer. It has the same shape as the parameters of the layer.</p>\n", "annotation": ": dict[str, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.previous_task_backbones", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.previous_task_backbones", "kind": "variable", "doc": "<p>The backbone models of the previous tasks. Keys are task IDs and values are the corresponding models. Each model is a <code>nn.Module</code> backbone after the corresponding previous task was trained.</p>\n\n<p>Some would argue that since we could store the model of the previous tasks, why don't we test the task directly with the stored model, instead of doing the less easier EWC thing? The thing is, EWC only uses the model of the previous tasks to train current and future tasks, which aggregate them into a single model. Once the training of the task is done, the storage for those parameters can be released. However, this make the future tasks not able to use EWC anymore, which is a disadvantage for EWC.</p>\n", "annotation": ": dict[str, torch.nn.modules.module.Module]"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_change_reg_factor", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_change_reg_factor", "kind": "variable", "doc": "<p>The parameter change regularization factor.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.parameter_change_reg", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.parameter_change_reg", "kind": "variable", "doc": "<p>Initialize and store the parameter change regularizer.</p>\n"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.when_calculate_fisher_information", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.when_calculate_fisher_information", "kind": "variable", "doc": "<p>When to calculate the fisher information.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.num_data", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.num_data", "kind": "variable", "doc": "<p>The number of data used to calculate the fisher information. It is used to average the fisher information over the data.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.automatic_optimization", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.sanity_check", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.on_train_start", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.on_train_start", "kind": "function", "doc": "<p>Initialize the parameter importance and num of data counter.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.training_step", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.on_train_end", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.on_train_end", "kind": "function", "doc": "<p>Calculate the fisher information as parameter importance and store the backbone model after the training of a task.</p>\n\n<p>The calculated importance and model are stored in <code>self.parameter_importance[self.task_id]</code> and <code>self.previous_task_backbones[self.task_id]</code> respectively for constructing the regularization loss in the future tasks.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.ewc.EWC.accumulate_fisher_information_on_train_end", "modulename": "clarena.cl_algorithms.ewc", "qualname": "EWC.accumulate_fisher_information_on_train_end", "kind": "function", "doc": "<p>Accumulate the fisher information as the parameter importance for the learned task <code>self.task_id</code> at the end of its training. This is only called after the training of a task, which is the last previous task $t-1$. The accumulate importance is stored in <code>self.parameter_importance[self.task_id]</code> for constructing the regularization loss in the future tasks.</p>\n\n<p>According to <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">the EWC paper</a>, the importance tensor is a Laplace approximation to Fisher information matrix by taking the digonal, i.e. $F_i$, where $i$ is the index of a parameter. The calculation is not following that theory but the derived formula below:</p>\n\n<p>$$\\omega_i = F_i  =\\frac{1}{N_{t-1}} \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}^{(t-1)}_{\\text{train}}} \\left[\\frac{\\partial l(f^{(t-1)}\\left(\\mathbf{x}, \\theta), y\\right)}{\\partial \\theta_i}\\right]^2$$</p>\n\n<p>For a parameter $i$, its fisher information is the magnitude (square here) of gradient of the loss of model just trained over the training data just used. The $l$ is the classification loss. It shows the sensitivity of the loss to the parameter. The larger it is, the more it changed the performance (which is the loss) of the model, which indicates the importance of the parameter.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>fisher_information_t</strong> (<code>dict[str, Tensor]</code>): the fisher information for the learned task. Keys are parameter names (named by <code>named_parameters()</code> of the <code>nn.Module</code>) and values are the importance tensor for the layer. It has the same shape as the parameters of the layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat", "modulename": "clarena.cl_algorithms.fgadahat", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for FG-AdaHAT algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT", "kind": "class", "doc": "<p>FG-AdaHAT (Fine-Grained Adaptive Hard Attention to the Task) algorithm.</p>\n\n<p>An architecture-based continual learning approach that improves <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT (Adaptive Hard Attention to the Task)</a> by introducing fine-grained neuron-wise importance measures guiding the adaptive adjustment mechanism in AdaHAT.</p>\n\n<p>We implement FG-AdaHAT as a subclass of AdaHAT, as it reuses AdaHAT's summative mask and other components.</p>\n", "bases": "clarena.cl_algorithms.adahat.AdaHAT"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.__init__", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.__init__", "kind": "function", "doc": "<p>Initialize the FG-AdaHAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with the HAT mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. FG-AdaHAT supports only TIL (Task-Incremental Learning).</li>\n<li><strong>adjustment_intensity</strong> (<code>float</code>): hyperparameter, controls the overall intensity of gradient adjustment (the $\\alpha$ in the paper).</li>\n<li><strong>importance_type</strong> (<code>str</code>): the type of neuron-wise importance, must be one of:\n<ol>\n<li>'input_weight_abs_sum': sum of absolute input weights;</li>\n<li>'output_weight_abs_sum': sum of absolute output weights;</li>\n<li>'input_weight_gradient_abs_sum': sum of absolute gradients of the input weights (Input Gradients (IG) in the paper);</li>\n<li>'output_weight_gradient_abs_sum': sum of absolute gradients of the output weights (Output Gradients (OG) in the paper);</li>\n<li>'activation_abs': absolute activation;</li>\n<li>'input_weight_abs_sum_x_activation_abs': sum of absolute input weights multiplied by absolute activation (Input Contribution Utility (ICU) in the paper);</li>\n<li>'output_weight_abs_sum_x_activation_abs': sum of absolute output weights multiplied by absolute activation (Contribution Utility (CU) in the paper);</li>\n<li>'gradient_x_activation_abs': absolute gradient (the saliency) multiplied by activation;</li>\n<li>'input_weight_gradient_square_sum': sum of squared gradients of the input weights;</li>\n<li>'output_weight_gradient_square_sum': sum of squared gradients of the output weights;</li>\n<li>'input_weight_gradient_square_sum_x_activation_abs': sum of squared gradients of the input weights multiplied by absolute activation (Activation Fisher Information (AFI) in the paper);</li>\n<li>'output_weight_gradient_square_sum_x_activation_abs': sum of squared gradients of the output weights multiplied by absolute activation;</li>\n<li>'conductance_abs': absolute layer conductance;</li>\n<li>'internal_influence_abs': absolute internal influence (Internal Influence (II) in the paper);</li>\n<li>'gradcam_abs': absolute Grad-CAM;</li>\n<li>'deeplift_abs': absolute DeepLIFT (DeepLIFT (DL) in the paper);</li>\n<li>'deepliftshap_abs': absolute DeepLIFT-SHAP;</li>\n<li>'gradientshap_abs': absolute Gradient-SHAP (Gradient SHAP (GS) in the paper);</li>\n<li>'integrated_gradients_abs': absolute Integrated Gradients;</li>\n<li>'feature_ablation_abs': absolute Feature Ablation (Feature Ablation (FA) in the paper);</li>\n<li>'lrp_abs': absolute Layer-wise Relevance Propagation (LRP);</li>\n<li>'cbp_adaptation': the adaptation function in <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">Continual Backpropagation (CBP)</a>;</li>\n<li>'cbp_adaptive_contribution': the adaptive contribution function in <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">Continual Backpropagation (CBP)</a>;</li>\n</ol></li>\n<li><strong>importance_summing_strategy</strong> (<code>str</code>): the strategy to sum neuron-wise importance for previous tasks, must be one of:\n<ol>\n<li>'add_latest': add the latest neuron-wise importance to the summative importance;</li>\n<li>'add_all': add all previous neuron-wise importance (including the latest) to the summative importance;</li>\n<li>'add_average': add the average of all previous neuron-wise importance (including the latest) to the summative importance;</li>\n<li>'linear_decrease': weigh the previous neuron-wise importance by a linear factor that decreases with the task ID;</li>\n<li>'quadratic_decrease': weigh the previous neuron-wise importance that decreases quadratically with the task ID;</li>\n<li>'cubic_decrease': weigh the previous neuron-wise importance that decreases cubically with the task ID;</li>\n<li>'exponential_decrease': weigh the previous neuron-wise importance by an exponential factor that decreases with the task ID;</li>\n<li>'log_decrease': weigh the previous neuron-wise importance by a logarithmic factor that decreases with the task ID;</li>\n<li>'factorial_decrease': weigh the previous neuron-wise importance that decreases factorially with the task ID;</li>\n</ol></li>\n<li><strong>importance_scheduler_type</strong> (<code>str</code>): the scheduler for importance, i.e., the factor $c^t$ multiplied to parameter importance. Must be one of:\n<ol>\n<li>'linear_sparsity_reg': $c^t = (t+b_L) \\cdot [R(M^t, M^{<t}) + b_R]$, where $R(M^t, M^{<t})$ is the mask sparsity regularization betwwen the current task and previous tasks, $b_L$ is the base linear factor (see argument <code>base_linear</code>), and $b_R$ is the base mask sparsity regularization factor (see argument <code>base_mask_sparsity_reg</code>);</li>\n<li>'sparsity_reg': $c^t = [R(M^t, M^{<t}) + b_R]$;</li>\n<li>'summative_mask_sparsity_reg': $c^t_{l,ij} = \\left(\\min \\left(m^{<t, \\text{sum}}_{l,i}, m^{<t, \\text{sum}}_{l-1,j}\\right)+b_L\\right) \\cdot [R(M^t, M^{<t}) + b_R]$.</li>\n</ol></li>\n<li><strong>neuron_to_weight_importance_aggregation_mode</strong> (<code>str</code>): aggregation mode from neuron-wise to weight-wise importance ($\\text{Agg}(\\cdot)$ in the paper), must be one of:\n<ol>\n<li>'min': take the minimum of neuron-wise importance for each weight;</li>\n<li>'max': take the maximum of neuron-wise importance for each weight;</li>\n<li>'mean': take the mean of neuron-wise importance for each weight.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>clamp_threshold</strong> (<code>float</code>): the threshold for task embedding gradient compensation. See Sec. 2.5 \"Embedding Gradient Compensation\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>mask_sparsity_reg_factor</strong> (<code>float</code>): hyperparameter, the regularization factor for mask sparsity.</li>\n<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularization, must be one of:\n<ol>\n<li>'original' (default): the original mask sparsity regularization in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li>'cross': the cross version of mask sparsity regularization.</li>\n</ol></li>\n<li><strong>base_importance</strong> (<code>float</code>): base value added to importance ($b_I$ in the paper). Default: 0.01.</li>\n<li><strong>base_mask_sparsity_reg</strong> (<code>float</code>): base value added to mask sparsity regularization factor in the importance scheduler ($b_R$ in the paper). Default: 0.1.</li>\n<li><strong>base_linear</strong> (<code>float</code>): base value added to the linear factor in the importance scheduler ($b_L$ in the paper). Default: 10.</li>\n<li><strong>filter_by_cumulative_mask</strong> (<code>bool</code>): whether to multiply the cumulative mask to the importance when calculating adjustment rate. Default: False.</li>\n<li><strong>filter_unmasked_importance</strong> (<code>bool</code>): whether to filter unmasked importance values (set to 0) at the end of task training. Default: False.</li>\n<li><strong>step_multiply_training_mask</strong> (<code>bool</code>): whether to multiply the training mask to the importance at each training step. Default: True.</li>\n<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialization mode for task embeddings, must be one of:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit the task embedding from the last task.</li>\n</ol></li>\n<li><strong>importance_summing_strategy_linear_step</strong> (<code>float</code> | <code>None</code>): linear step for the importance summing strategy (used when <code>importance_summing_strategy</code> is 'linear_decrease'). Must be &gt; 0.</li>\n<li><strong>importance_summing_strategy_exponential_rate</strong> (<code>float</code> | <code>None</code>): exponential rate for the importance summing strategy (used when <code>importance_summing_strategy</code> is 'exponential_decrease'). Must be &gt; 1.</li>\n<li><strong>importance_summing_strategy_log_base</strong> (<code>float</code> | <code>None</code>): base for the logarithm in the importance summing strategy (used when <code>importance_summing_strategy</code> is 'log_decrease'). Must be &gt; 1.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">HATMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_intensity</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">importance_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">importance_summing_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">importance_scheduler_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">neuron_to_weight_importance_aggregation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">clamp_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">base_importance</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>,</span><span class=\"param\">\t<span class=\"n\">base_mask_sparsity_reg</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>,</span><span class=\"param\">\t<span class=\"n\">base_linear</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">filter_by_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">filter_unmasked_importance</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">step_multiply_training_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">task_embedding_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;N01&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">importance_summing_strategy_linear_step</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">importance_summing_strategy_exponential_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">importance_summing_strategy_log_base</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.importance_type", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.importance_type", "kind": "variable", "doc": "<p>The type of the neuron-wise importance added to AdaHAT importance.</p>\n", "annotation": ": str | None"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.importance_scheduler_type", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.importance_scheduler_type", "kind": "variable", "doc": "<p>The type of the importance scheduler.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.neuron_to_weight_importance_aggregation_mode", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.neuron_to_weight_importance_aggregation_mode", "kind": "variable", "doc": "<p>The mode of aggregation from neuron-wise to weight-wise importance.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.filter_by_cumulative_mask", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.filter_by_cumulative_mask", "kind": "variable", "doc": "<p>The flag to filter importance by the cumulative mask when calculating the adjustment rate.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.filter_unmasked_importance", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.filter_unmasked_importance", "kind": "variable", "doc": "<p>The flag to filter unmasked importance values (set them to 0) at the end of task training.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.step_multiply_training_mask", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.step_multiply_training_mask", "kind": "variable", "doc": "<p>The flag to multiply the training mask to the importance at each training step.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.importance_summing_strategy", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.importance_summing_strategy", "kind": "variable", "doc": "<p>The strategy to sum the neuron-wise importance for previous tasks.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.base_importance", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.base_importance", "kind": "variable", "doc": "<p>The base value added to the importance to avoid zero.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.base_mask_sparsity_reg", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.base_mask_sparsity_reg", "kind": "variable", "doc": "<p>The base value added to the mask sparsity regularization to avoid zero.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.base_linear", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.base_linear", "kind": "variable", "doc": "<p>The base value added to the linear layer to avoid zero.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.importances", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.importances", "kind": "variable", "doc": "<p>The min-max scaled ($[0, 1]$) neuron-wise importance of units. It is $I^{\\tau}_{l}$ in the paper. Keys are task IDs and values are the corresponding importance tensors. Each importance tensor is a dict where keys are layer names and values are the importance tensor for the layer. The utility tensor is the same size as the feature tensor with size (number of units, ).</p>\n", "annotation": ": dict[int, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.summative_importance_for_previous_tasks", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.summative_importance_for_previous_tasks", "kind": "variable", "doc": "<p>The summative neuron-wise importance values of units for previous tasks before the current task <code>self.task_id</code>. See $I^{<t}_{l}$ in the paper. Keys are layer names and values are the summative importance tensor for the layer. The summative importance tensor has the same size as the feature tensor with size (number of units, ).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.num_steps_t", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.num_steps_t", "kind": "variable", "doc": "<p>The number of training steps for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.automatic_optimization", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.sanity_check", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.on_train_start", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.on_train_start", "kind": "function", "doc": "<p>Initialize neuron importance accumulation variable for each layer as zeros, in addition to AdaHAT's summative mask initialization.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.clip_grad_by_adjustment", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.clip_grad_by_adjustment", "kind": "function", "doc": "<p>Clip the gradients by the adjustment rate. See Eq. (1) in the paper.</p>\n\n<p>Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system. This applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</p>\n\n<p>Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters. See Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code>): the network sparsity (i.e., mask sparsity loss of each layer) for the current task. Keys are layer names and values are the network sparsity values. It is used to calculate the adjustment rate for gradients. In FG-AdaHAT, it is used to construct the importance scheduler.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate_weight</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for weights. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>adjustment_rate_bias</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for biases. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">network_sparsity</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.on_train_batch_end", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.on_train_batch_end", "kind": "function", "doc": "<p>Calculate the step-wise importance, update the accumulated importance and number of steps counter after each training step.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): outputs of the training step (returns of <code>training_step()</code> in <code>CLAlgorithm</code>).</li>\n<li><strong>batch</strong> (<code>Any</code>): training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): index of the current batch (for mask figure file name).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.on_train_end", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.on_train_end", "kind": "function", "doc": "<p>Additionally calculate neuron-wise importance for previous tasks at the end of training each task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_abs_sum", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_abs_sum", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input or output weights.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n<li><strong>reciprocal</strong> (<code>bool</code>): whether to take reciprocal.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">reciprocal</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_gradient_abs_sum", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of gradients of the layer input or output weights.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_activation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_activation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute value of activation of the layer. This is our own implementation of <a href=\"https://captum.ai/api/layer.html#layer-activation\">Layer Activation</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_abs_sum_x_activation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer input / output weights multiplied by absolute values of activation. The input weights version is equal to the contribution utility in <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_gradient_x_activation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of the gradient of layer activation multiplied by the activation. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-gradient-x-activation\">Layer Gradient X Activation</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares. The weight gradient square is equal to fisher information in <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_weight_gradient_square_sum_x_activation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of layer weight gradient squares multiplied by absolute values of activation. The weight gradient square is equal to fisher information in <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n<li><strong>if_output_weight</strong> (<code>bool</code>): whether to use the output weights or input weights.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">if_output_weight</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_conductance_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_conductance_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://openreview.net/forum?id=SylKoo0cKm\">conductance</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-conductance\">Layer Conductance</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which integral is computed in this method. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerConductance.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.- <strong>mask</strong> (<code>Tensor</code>): the mask tensor of the layer. It has the same size as the feature tensor with size (number of units, ).</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_internal_influence_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_internal_influence_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://openreview.net/forum?id=SJPpHzW0-\">internal influence</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#internal-influence\">Internal Influence</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which integral is computed in this method. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.InternalInfluence.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_gradcam_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_gradcam_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://openreview.net/forum?id=SJPpHzW0-\">Grad-CAM</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#gradcam\">Layer Grad-CAM</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_deeplift_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_deeplift_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf\">DeepLift</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-deeplift\">Layer DeepLift</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): baselines define reference samples that are compared with the inputs. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerDeepLift.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_deepliftshap_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_deepliftshap_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf\">DeepLift SHAP</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-deepliftshap\">Layer DeepLiftShap</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): baselines define reference samples that are compared with the inputs. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerDeepLiftShap.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_gradientshap_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_gradientshap_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of gradient SHAP. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-gradientshap\">Layer GradientShap</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which expectation is computed. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerGradientShap.attribute\">Captum documentation</a> for more details. If <code>None</code>, the baselines are set to zero.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_integrated_gradients_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_integrated_gradients_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf\">integrated gradients</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-integrated-gradients\">Layer Integrated Gradients</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): starting point from which integral is computed. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerIntegratedGradients.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_feature_ablation_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_feature_ablation_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53\">feature ablation</a> attribution. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-feature-ablation\">Layer Feature Ablation</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>layer_baselines</strong> (<code>None</code> | <code>int</code> | <code>float</code> | <code>Tensor</code> | <code>tuple[int | float | Tensor, ...]</code>): reference values which replace each layer input / output value when ablated. Please refer to the <a href=\"https://captum.ai/api/layer.html#captum.attr.LayerFeatureAblation.attribute\">Captum documentation</a> for more details.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n<li><strong>if_captum</strong> (<code>bool</code>): whether to use Captum or not. If <code>True</code>, we use Captum to calculate the feature ablation. If <code>False</code>, we use our implementation. Default is <code>False</code>, because our implementation is much faster.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">layer_baselines</span><span class=\"p\">:</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">if_captum</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_lrp_abs", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_lrp_abs", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the absolute values of <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140\">LRP</a>. We implement this using <a href=\"https://captum.ai/api/layer.html#layer-lrp\">Layer LRP</a> in Captum.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>input</strong> (<code>Tensor</code> | <code>tuple[Tensor, ...]</code>): the input batch of the training step.</li>\n<li><strong>target</strong> (<code>Tensor</code> | <code>None</code>): the target batch of the training step.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is an argument of the forward function during training.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the number of batches in the training step. This is an argument of the forward function during training.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span> <span class=\"o\">|</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fgadahat.FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution", "modulename": "clarena.cl_algorithms.fgadahat", "qualname": "FGAdaHAT.get_importance_step_layer_cbp_adaptive_contribution", "kind": "function", "doc": "<p>Get the raw neuron-wise importance (before scaling) of a layer of a training step. See $I^{\\tau}_l(\\mathbf{x},y)$ (before Eqs. (5) and (6)) in the paper. This method uses the sum of absolute values of layer output weights multiplied by absolute values of activation, then divided by the reciprocal of sum of absolute values of layer input weights. It is equal to the adaptive contribution utility in <a href=\"https://www.nature.com/articles/s41586-024-07711-7\">CBP</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>layer_name</strong> (<code>str</code>): the name of layer to get neuron-wise importance.</li>\n<li><strong>activation</strong> (<code>Tensor</code>): the activation tensor of the layer. It has the same size of (number of units, ).</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>importance_step_layer</strong> (<code>Tensor</code>): the neuron-wise importance of the layer of the training step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">activation</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning", "modulename": "clarena.cl_algorithms.finetuning", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Finetuning algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning", "kind": "class", "doc": "<p>Finetuning algorithm.</p>\n\n<p>The most naive way for task-incremental learning. It simply initializes the backbone from the last task when training new task.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.__init__", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.__init__", "kind": "function", "doc": "<p>Initialize the Finetuning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.training_step", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.validation_step", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.validation_step", "kind": "function", "doc": "<p>Validation step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Key (<code>str</code>) are the metrics names, value (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.finetuning.Finetuning.test_step", "modulename": "clarena.cl_algorithms.finetuning", "qualname": "Finetuning.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Key (<code>str</code>) are the metrics name, value (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.fix", "modulename": "clarena.cl_algorithms.fix", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Fix algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.fix.Fix", "modulename": "clarena.cl_algorithms.fix", "qualname": "Fix", "kind": "class", "doc": "<p>Fix algorithm.</p>\n\n<p>Another naive way for task-incremental learning aside from Finetuning. It simply fixes the backbone forever after training first task. It serves as kind of toy algorithm when discussing stability-plasticity dilemma in continual learning.</p>\n\n<p>We implement <code>Fix</code> as a subclass of <code>Finetuning</code>, as it shares <code>forward()</code>, <code>validation_step()</code>, and <code>test_step()</code> with <code>Finetuning</code>.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.fix.Fix.__init__", "modulename": "clarena.cl_algorithms.fix", "qualname": "Fix.__init__", "kind": "function", "doc": "<p>Initialize the Fix algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.fix.Fix.training_step", "modulename": "clarena.cl_algorithms.fix", "qualname": "Fix.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat", "modulename": "clarena.cl_algorithms.hat", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.hat.HAT", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT", "kind": "class", "doc": "<p><a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> algorithm.</p>\n\n<p>An architecture-based continual learning approach that uses learnable hard attention masks to select task-specific parameters.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.hat.HAT.__init__", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.__init__", "kind": "function", "doc": "<p>Initialize the HAT algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>HATMaskBackbone</code>): must be a backbone network with the HAT mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. HAT only supports TIL (Task-Incremental Learning).</li>\n<li><strong>adjustment_mode</strong> (<code>str</code>): the strategy of adjustment (i.e., the mode of gradient clipping), must be one of:\n<ol>\n<li>'hat': set gradients of parameters linking to masked units to zero. This is how HAT fixes the part of the network for previous tasks completely. See Eq. (2) in Sec. 2.3 \"Network Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li>'hat_random': set gradients of parameters linking to masked units to random 0\u20131 values. See \"Baselines\" in Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'hat_const_alpha': set gradients of parameters linking to masked units to a constant value <code>alpha</code>. See \"Baselines\" in Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n<li>'hat_const_1': set gradients of parameters linking to masked units to a constant value of 1 (i.e., no gradient constraint). See \"Baselines\" in Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</li>\n</ol></li>\n<li><strong>s_max</strong> (<code>float</code>): hyperparameter, the maximum scaling factor in the gate function. See Sec. 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>clamp_threshold</strong> (<code>float</code>): the threshold for task embedding gradient compensation. See Sec. 2.5 \"Embedding Gradient Compensation\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n<li><strong>mask_sparsity_reg_factor</strong> (<code>float</code>): hyperparameter, the regularization factor for mask sparsity.</li>\n<li><strong>mask_sparsity_reg_mode</strong> (<code>str</code>): the mode of mask sparsity regularization, must be one of:\n<ol>\n<li>'original' (default): the original mask sparsity regularization in the HAT paper.</li>\n<li>'cross': the cross version of mask sparsity regularization.</li>\n</ol></li>\n<li><strong>task_embedding_init_mode</strong> (<code>str</code>): the initialization mode for task embeddings, must be one of:\n<ol>\n<li>'N01' (default): standard normal distribution $N(0, 1)$.</li>\n<li>'U-11': uniform distribution $U(-1, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n<li>'U-10': uniform distribution $U(-1, 0)$.</li>\n<li>'last': inherit the task embedding from the last task.</li>\n</ol></li>\n<li><strong>alpha</strong> (<code>float</code> | <code>None</code>): the <code>alpha</code> in the 'HAT-const-alpha' mode. Applies only when <code>adjustment_mode</code> is 'hat_const_alpha'.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">HATMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">s_max</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">clamp_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">mask_sparsity_reg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">task_embedding_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;N01&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.hat.HAT.adjustment_mode", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.adjustment_mode", "kind": "variable", "doc": "<p>The adjustment mode for gradient clipping.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.hat.HAT.s_max", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.s_max", "kind": "variable", "doc": "<p>The hyperparameter s_max.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.hat.HAT.clamp_threshold", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.clamp_threshold", "kind": "variable", "doc": "<p>The clamp threshold for task embedding gradient compensation.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.hat.HAT.mask_sparsity_reg_factor", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.mask_sparsity_reg_factor", "kind": "variable", "doc": "<p>The mask sparsity regularization factor.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.hat.HAT.mask_sparsity_reg_mode", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.mask_sparsity_reg_mode", "kind": "variable", "doc": "<p>The mask sparsity regularization mode.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.hat.HAT.mark_sparsity_reg", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.mark_sparsity_reg", "kind": "variable", "doc": "<p>The mask sparsity regularizer.</p>\n", "annotation": ": clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg"}, {"fullname": "clarena.cl_algorithms.hat.HAT.task_embedding_init_mode", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.task_embedding_init_mode", "kind": "variable", "doc": "<p>The task embedding initialization mode.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.hat.HAT.alpha", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.alpha", "kind": "variable", "doc": "<p>The hyperparameter alpha for <code>hat_const_alpha</code>.</p>\n", "annotation": ": float | None"}, {"fullname": "clarena.cl_algorithms.hat.HAT.cumulative_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.cumulative_mask_for_previous_tasks", "kind": "variable", "doc": "<p>The cumulative binary attention mask $\\mathrm{M}^{<t}$ of previous tasks $1,\\cdots, t-1$, gated from the task embedding ($t$ is <code>self.task_id</code>). It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has size (number of units, ).</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.hat.HAT.automatic_optimization", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.hat.HAT.sanity_check", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.on_train_start", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.on_train_start", "kind": "function", "doc": "<p>Initialize the task embedding before training the next task and initialize the cumulative mask at the beginning of the first task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.clip_grad_by_adjustment", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.clip_grad_by_adjustment", "kind": "function", "doc": "<p>Clip the gradients by the adjustment rate. See Eq. (2) in Sec. 2.3 \"Network Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n\n<p>Note that because the task embedding fully covers every layer in the backbone network, no parameters are left out of this system.\nThis applies not only to parameters between layers with task embeddings, but also to those before the first layer. We design it separately in the code.</p>\n\n<p>Network capacity is measured alongside this method. Network capacity is defined as the average adjustment rate over all parameters.\nSee Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate_weight</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for weights. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>adjustment_rate_bias</strong> (<code>dict[str, Tensor]</code>): the adjustment rate for biases. Keys (<code>str</code>) are layer name and values (<code>Tensor</code>) are the adjustment rate tensors.</li>\n<li><strong>capacity</strong> (<code>Tensor</code>): the calculated network capacity.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">],</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.compensate_task_embedding_gradients", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.compensate_task_embedding_gradients", "kind": "function", "doc": "<p>Compensate the gradients of task embeddings during training. See Sec. 2.5 \"Embedding Gradient Compensation\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch_idx</strong> (<code>int</code>): the current training batch index.</li>\n<li><strong>num_batches</strong> (<code>int</code>): the total number of training batches.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.forward", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code>| <code>None</code>): the task ID where the data are from. If the stage is 'train' or 'validation', it should be the current task <code>self.task_id</code>. If stage is 'test', it could be from any seen task. In TIL, the task IDs of test data are provided thus this argument can be used. HAT algorithm works only for TIL.</li>\n<li><strong>batch_idx</strong> (<code>int</code> | <code>None</code>): the current batch index. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n<li><strong>num_batches</strong> (<code>int</code> | <code>None</code>): the total number of batches. Applies only to training stage. For other stages, it is default <code>None</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has size (number of units, ).</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes. Although HAT algorithm does not need this, it is still provided for API consistence for other HAT-based algorithms inherited this <code>forward()</code> method of <code>HAT</code> class.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_batches</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.training_step", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the batch. Used for calculating annealed scalar in HAT. See Sec. 2.4 \"Hard Attention Training\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary containing loss and other metrics from this training step. Keys (<code>str</code>) are metric names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' (total loss) in the case of automatic optimization, according to PyTorch Lightning. For HAT, it includes 'mask' and 'capacity' for logging.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>, </span><span class=\"param\"><span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.on_train_end", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.on_train_end", "kind": "function", "doc": "<p>The mask and update the cumulative mask after training the task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.validation_step", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.validation_step", "kind": "function", "doc": "<p>Validation step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.hat.HAT.test_step", "modulename": "clarena.cl_algorithms.hat", "qualname": "HAT.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.independent", "modulename": "clarena.cl_algorithms.independent", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Independent learning algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.independent.Independent", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent", "kind": "class", "doc": "<p>Independent learning algorithm.</p>\n\n<p>Another naive way for task-incremental learning aside from Finetuning. It assigns a new independent model for each task. This is a simple way to avoid catastrophic forgetting at the extreme cost of memory. It achieves the theoretical upper bound of performance in continual learning.</p>\n\n<p>We implement Independent as a subclass of Finetuning algorithm, as Independent has the same <code>forward()</code>, <code>training_step()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.independent.Independent.__init__", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.__init__", "kind": "function", "doc": "<p>Initialize the Independent algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.independent.Independent.original_backbone", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.original_backbone", "kind": "variable", "doc": "<p>The original backbone network state dict is stored as the source of creating new independent backbone.</p>\n", "annotation": ": dict"}, {"fullname": "clarena.cl_algorithms.independent.Independent.backbones", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.backbones", "kind": "variable", "doc": "<p>The list of independent backbones for each task. Keys are task IDs and values are the corresponding backbones.</p>\n", "annotation": ": dict[int, clarena.backbones.base.CLBackbone]"}, {"fullname": "clarena.cl_algorithms.independent.Independent.backbone_valid_task_ids", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.backbone_valid_task_ids", "kind": "variable", "doc": "<p>The list of task IDs that have valid backbones.</p>\n", "annotation": ": set[int]"}, {"fullname": "clarena.cl_algorithms.independent.Independent.on_train_start", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.on_train_start", "kind": "function", "doc": "<p>Initialize an independent backbone for <code>self.task_id</code>, duplicated from the original backbone.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.independent.Independent.on_train_end", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.on_train_end", "kind": "function", "doc": "<p>The trained independent backbone for <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.independent.Independent.test_step", "modulename": "clarena.cl_algorithms.independent", "qualname": "Independent.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.lwf", "modulename": "clarena.cl_algorithms.lwf", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF (Learning without Forgetting) algorithm</a>.</p>\n"}, {"fullname": "clarena.cl_algorithms.lwf.LwF", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF", "kind": "class", "doc": "<p><a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF (Learning without Forgetting)</a> algorithm.</p>\n\n<p>A regularization-based continual learning approach that constrains the feature output of the model to be similar to that of the previous tasks. From the perspective of knowledge distillation, it distills previous tasks models into the training process for new task in the regularization term. It is a simple yet effective method for continual learning.</p>\n\n<p>We implement LwF as a subclass of Finetuning algorithm, as LwF has the same <code>forward()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.__init__", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.__init__", "kind": "function", "doc": "<p>Initialize the LwF algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>distillation_reg_factor</strong> (<code>float</code>): hyperparameter, the distillation regularization factor. It controls the strength of preventing forgetting.</li>\n<li><strong>distillation_reg_temperature</strong> (<code>float</code>): hyperparameter, the temperature in the distillation regularization. It controls the softness of the labels that the student model (here is the current model) learns from the teacher models (here are the previous models), thereby controlling the strength of the distillation. It controls the strength of preventing forgetting.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">distillation_reg_factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">distillation_reg_temperature</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.previous_task_backbones", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.previous_task_backbones", "kind": "variable", "doc": "<p>The backbone models of the previous tasks. Keys are task IDs (int) and values are the corresponding models. Each model is a <code>CLBackbone</code> after the corresponding previous task was trained.</p>\n\n<p>Some would argue that since we could store the model of the previous tasks, why don't we test the task directly with the stored model, instead of doing the less easier LwF thing? The thing is, LwF only uses the model of the previous tasks to train current and future tasks, which aggregate them into a single model. Once the training of the task is done, the storage for those parameters can be released. However, this make the future tasks not able to use LwF anymore, which is a disadvantage for LwF.</p>\n", "annotation": ": dict[int, clarena.backbones.base.CLBackbone]"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.distillation_reg_factor", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.distillation_reg_factor", "kind": "variable", "doc": "<p>The distillation regularization factor.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.distillation_reg_temperature", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.distillation_reg_temperature", "kind": "variable", "doc": "<p>The distillation regularization temperature.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.distillation_reg", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.distillation_reg", "kind": "variable", "doc": "<p>Initialize and store the distillation regularizer.</p>\n"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.sanity_check", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.training_step", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.lwf.LwF.on_train_end", "modulename": "clarena.cl_algorithms.lwf", "qualname": "LwF.on_train_end", "kind": "function", "doc": "<p>The backbone model after the training of a task.</p>\n\n<p>The model is stored in <code>self.previous_task_backbones</code> for constructing the regularization loss in the future tasks.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.random", "modulename": "clarena.cl_algorithms.random", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for Random algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.random.Random", "modulename": "clarena.cl_algorithms.random", "qualname": "Random", "kind": "class", "doc": "<p>Random stratified model.</p>\n\n<p>Pass the training step and simply use the randomly initialized model to predict the test data. This serves as a reference model to compute forgetting rate. See chapter 4 in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task) paper</a>.</p>\n\n<p>We implement Random as a subclass of Finetuning algorithm, as Random has the same <code>forward()</code>, <code>validation_step()</code> and <code>test_step()</code> method as <code>Finetuning</code> class.</p>\n", "bases": "clarena.cl_algorithms.finetuning.Finetuning"}, {"fullname": "clarena.cl_algorithms.random.Random.__init__", "modulename": "clarena.cl_algorithms.random", "qualname": "Random.__init__", "kind": "function", "doc": "<p>Initialize the Random algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>CLBackbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code> | <code>HeadsCIL</code>): output heads.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.random.Random.automatic_optimization", "modulename": "clarena.cl_algorithms.random", "qualname": "Random.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.random.Random.training_step", "modulename": "clarena.cl_algorithms.random", "qualname": "Random.training_step", "kind": "function", "doc": "<p>Pass the training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers", "modulename": "clarena.cl_algorithms.regularizers", "kind": "module", "doc": "<h1 id=\"continual-learning-regularizers\">Continual Learning Regularizers</h1>\n\n<p>This submodule provides the <strong>regularizers</strong> which are added to the loss function of corresponding continual learning algorithms. It can promote forgetting preventing which is the major mechanism in regularization-based approaches, or for other purposes.</p>\n\n<p>The regularizers inherit from <code>nn.Module</code>.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the regularizers:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/cl-algorithm#sec-regularizers\"><strong>Implement custom regularizers in CL algorithms</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-regularization-based-approaches\"><strong>A Beginners' Guide to Continual Learning (Regularization-based Approaches)</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation", "modulename": "clarena.cl_algorithms.regularizers.distillation", "kind": "module", "doc": "<p>The submodule in <code>regularizers</code> for distillation regularization.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg", "kind": "class", "doc": "<p>Distillation regularizer. This is the core of <a href=\"https://research.google/pubs/distilling-the-knowledge-in-a-neural-network/\">knowledge distillation</a> used as a regularizer in continual learning.</p>\n\n<p>$$R(\\theta^{\\text{student}}) = \\text{factor} * \\frac1N \\sum_{(\\mathbf{x}, y)\\in \\mathcal{D}} \\text{distance}\\left(f(\\mathbf{x};\\theta^{\\text{student}}),f(\\mathbf{x};\\theta^{\\text{teacher}})\\right)$$</p>\n\n<p>It promotes the target (student) model output logits $f(\\mathbf{x};\\theta^{\\text{student}})$ not changing too much from the reference (teacher) model output logits $f(\\mathbf{x};\\theta^{\\text{teacher}})$. The loss is averaged over the dataset $\\mathcal{D}$.</p>\n\n<p>It is used in:</p>\n\n<ul>\n<li><a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF (Learning without Forgetting) algorithm</a>: as a distillation regularizer for the output logits by current task model to be closer to output logits by previous tasks models. It uses a modified cross entropy as the distance. See equation (2) (3) in the <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF paper</a>.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.__init__", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>factor</strong> (<code>float</code>): the regularization factor.</li>\n<li><strong>temperature</strong> (<code>float</code>): the temperature of the distillation, should be a positive float.</li>\n<li><strong>distance</strong> (<code>str</code>): the type of distance function used in the distillation; one of:\n<ol>\n<li>\"lwf_cross_entropy\": the modified cross entropy loss from LwF. See equation (3) in the <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">LwF paper</a>.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">temperature</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">distance</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.factor", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.factor", "kind": "variable", "doc": "<p>The regularization factor for distillation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.temperature", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.temperature", "kind": "variable", "doc": "<p>The temperature of the distillation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.distance", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.distance", "kind": "variable", "doc": "<p>The type of distance function used in the distillation.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.distillation.DistillationReg.forward", "modulename": "clarena.cl_algorithms.regularizers.distillation", "qualname": "DistillationReg.forward", "kind": "function", "doc": "<p>Calculate the regularization loss.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>student_logits</strong> (<code>Tensor</code>): the output logits of target (student) model to learn the knowledge from distillation. In LwF, it's the model of current training task.</li>\n<li><strong>teacher_logits</strong> (<code>Tensor</code>): the output logits of reference (teacher) model that knowledge is distilled. In LwF, it's the model of one of the previous tasks.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the distillation regularization value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">student_logits</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">teacher_logits</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "kind": "module", "doc": "<p>The submodule in <code>regularizers</code> for <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> mask sparsity regularization.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg", "kind": "class", "doc": "<p>Mask sparsity regularizer of <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a>.</p>\n\n<p>$$\nR\\left(\\textsf{M}^t,\\textsf{M}^{<t}\\right)=\\text{factor} * \\frac{\\sum_{l=1}^{L-1}\\sum_{i=1}^{N_l}m_{l,i}^t\\left(1-m_{l,i}^{<t}\\right)}{\\sum_{l=1}^{L-1}\\sum_{i=1}^{N_l}\\left(1-m_{l,i}^{<t}\\right)}\n$$</p>\n\n<p>It promotes the low capacity usage that is reflected by occupation of masks in the parameter space.</p>\n\n<p>See chapter 2.6 \"Promoting Low Capacity Usage\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.__init__", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>factor</strong> (<code>float</code>): the regularization factor.</li>\n<li><strong>mode</strong> (<code>str</code>): the mode of mask sparsity regularization; one of:\n<ol>\n<li>'original' (default): the original mask sparsity regularization in HAT paper.</li>\n<li>'cross': the cross version mask sparsity regularization.</li>\n</ol></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span>, </span><span class=\"param\"><span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;original&#39;</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.factor", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.factor", "kind": "variable", "doc": "<p>The regularization factor for mask sparsity.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.mode", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.mode", "kind": "variable", "doc": "<p>The mode of mask sparsity regularization.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.forward", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.forward", "kind": "function", "doc": "<p>Calculate the mask sparsity regularization loss.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task.</li>\n<li><strong>previous_cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask for the previous tasks.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the mask sparsity regularization value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">previous_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.original_reg", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.original_reg", "kind": "function", "doc": "<p>Calculate the original mask sparsity regularization loss in HAT paper.</p>\n\n<p>See chapter 2.6 \"Promoting Low Capacity Usage\" in the <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT paper</a>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. The $\\mathrm{A}^t$ in the paper.</li>\n<li><strong>previous_cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask for the previous tasks. The $\\mathrm{A}^{<t}$ in the paper.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the original mask sparsity regularization loss.</li>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code>): the network sparsity for each layer. Keys are layer names and values are the network sparsity value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">previous_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers.hat_mask_sparsity.HATMaskSparsityReg.cross_reg", "modulename": "clarena.cl_algorithms.regularizers.hat_mask_sparsity", "qualname": "HATMaskSparsityReg.cross_reg", "kind": "function", "doc": "<p>Calculate the cross mask sparsity regularization loss. This is an attempting improvement by me to the original regularization, which not only considers the sparsity in available units but also the density in the units occupied by previous tasks.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the mask for the current task. The $\\mathrm{A}^t$ in the paper.</li>\n<li><strong>previous_cumulative_mask</strong> (<code>dict[str, Tensor]</code>): the cumulative mask for the previous tasks. The $\\mathrm{A}^{<t}$ in the paper.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (<code>Tensor</code>): the cross mask sparsity regularization loss.</li>\n<li><strong>network_sparsity</strong> (<code>dict[str, Tensor]</code>): the network sparsity for each layer. Keys are layer names and values are the network sparsity value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">previous_cumulative_mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "kind": "module", "doc": "<p>The submodule in <code>regularizers</code> for parameter change regularization.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change.ParameterChangeReg", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "qualname": "ParameterChangeReg", "kind": "class", "doc": "<p>Parameter change regularizer.</p>\n\n<p>$$R(\\theta) = \\text{factor} * \\sum_i w_i \\|\\theta_i - \\theta^\\star_i\\|^2$$</p>\n\n<p>It promotes the target set of parameters $\\theta = {\\theta_i}_i$ not changing too much from another set of parameters $\\theta^\\star = {\\theta^\\star_i}_i$. The parameter distance here is $L^2$ distance. The regularization can be parameter-wise weighted, i.e. $w_i$ in the formula.</p>\n\n<p>It is used in:</p>\n\n<ul>\n<li><a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">L2 Regularization algorithm</a>: as a L2 regularizer for the current task parameters to prevent them from changing too much from the previous task parameters.</li>\n<li><a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC (Elastic Weight Consolidation) algorithm</a>: as a weighted L2 regularizer for the current task parameters to prevent them from changing too much from the previous task parameters. The regularization weights are parameter importance measure calculated from fisher information. See equation 3 in the <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC paper</a>.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change.ParameterChangeReg.__init__", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "qualname": "ParameterChangeReg.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>factor</strong> (<code>float</code>): the regularization factor. Note that it is $\\frac{\\lambda}{2}$ rather than $\\lambda$ in the <a href=\"https://www.pnas.org/doi/10.1073/pnas.1611835114\">EWC paper</a>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">factor</span><span class=\"p\">:</span> <span class=\"nb\">float</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change.ParameterChangeReg.factor", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "qualname": "ParameterChangeReg.factor", "kind": "variable", "doc": "<p>The regularization factor for parameter change.</p>\n"}, {"fullname": "clarena.cl_algorithms.regularizers.parameter_change.ParameterChangeReg.forward", "modulename": "clarena.cl_algorithms.regularizers.parameter_change", "qualname": "ParameterChangeReg.forward", "kind": "function", "doc": "<p>Calculate the regularization loss.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>target_model</strong> (nn.Module): the model of the target parameters. In EWC, it's the model of current training task.</li>\n<li><strong>ref_model</strong> (nn.Module): the reference model that you want target model parameters to prevent changing from. The reference model must have the same structure as the target model. In EWC, it's the model of one of the previous tasks.</li>\n<li><strong>weights</strong> (dict[str, Tensor]): the regularization weight for each parameter. Keys are parameter names and values are the weight tensors. The weight tensors must match the shape of model parameters. In EWC, it's the importance measure of each parameter, calculated from fisher information thing.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>reg</strong> (Tensor): the parameter change regularization value.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">target_model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">ref_model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">weights</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn", "modulename": "clarena.cl_algorithms.wsn", "kind": "module", "doc": "<p>The submodule in <code>cl_algorithms</code> for <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks)</a> algorithm.</p>\n"}, {"fullname": "clarena.cl_algorithms.wsn.WSN", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN", "kind": "class", "doc": "<p><a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN (Winning Subnetworks)</a> algorithm.</p>\n\n<p>An architecture-based continual learning approach that trains learnable parameter-wise scores and selects the most scored c% of network parameters per task.</p>\n", "bases": "clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.__init__", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.__init__", "kind": "function", "doc": "<p>Initialize the WSN algorithm with the network.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>WSNMaskBackbone</code>): must be a backbone network with the WSN mask mechanism.</li>\n<li><strong>heads</strong> (<code>HeadsTIL</code>): output heads. WSN only supports TIL (Task-Incremental Learning).</li>\n<li><strong>mask_percentage</strong> (<code>float</code>): the percentage $c\\%$ of parameters to be used for each task. See Sec. 3 and Eq. (4) in the <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN paper</a>.</li>\n<li><strong>parameter_score_init_mode</strong> (<code>str</code>): the initialization mode for parameter scores, must be one of:\n<ol>\n<li>'default': the default initialization in the original WSN code.</li>\n<li>'N01': standard normal distribution $N(0, 1)$.</li>\n<li>'U01': uniform distribution $U(0, 1)$.</li>\n</ol></li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">WSNMaskBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span>,</span><span class=\"param\">\t<span class=\"n\">mask_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">parameter_score_init_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;default&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.mask_percentage", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.mask_percentage", "kind": "variable", "doc": "<p>The percentage of parameters to be used for each task.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.parameter_score_init_mode", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.parameter_score_init_mode", "kind": "variable", "doc": "<p>The parameter score initialization mode.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.weight_masks", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.weight_masks", "kind": "variable", "doc": "<p>The binary weight mask of each previous task percentile-gated from the weight score. Keys are task IDs and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has the same size (output features, input features) as weight.</p>\n", "annotation": ": dict[int, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.bias_masks", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.bias_masks", "kind": "variable", "doc": "<p>The binary bias mask of each previous task percentile-gated from the bias score. Keys are task IDs and values are the corresponding mask. Each mask is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has the same size (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</p>\n", "annotation": ": dict[int, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.cumulative_weight_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.cumulative_weight_mask_for_previous_tasks", "kind": "variable", "doc": "<p>The cumulative binary weight mask $\\mathbf{M}_{t-1}$ of previous tasks $1, \\cdots, t-1$, percentile-gated from the weight score. It is a dict where keys are layer names and values are the binary mask tensors for the layers. The mask tensor has the same size (output features, input features) as weight.</p>\n", "annotation": ": dict[str, torch.Tensor]"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.cumulative_bias_mask_for_previous_tasks", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.cumulative_bias_mask_for_previous_tasks", "kind": "variable", "doc": "<p>The cumulative binary bias mask $\\mathbf{M}_{t-1}$ of previous tasks $1, \\cdots, t-1$, percentile-gated from the bias score. It is a dict where keys are layer names and values are the binary mask tensor for the layer. The mask tensor has the same size (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</p>\n", "annotation": ": dict[str, dict[str, torch.Tensor]]"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.automatic_optimization", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.automatic_optimization", "kind": "variable", "doc": "<p>If set to <code>False</code> you are responsible for calling <code>.backward()</code>, <code>.step()</code>, <code>.zero_grad()</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.sanity_check", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.on_train_start", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.on_train_start", "kind": "function", "doc": "<p>Initialize the parameter scores before training the next task and initialize the cumulative masks at the beginning of the first task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.clip_grad_by_mask", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.clip_grad_by_mask", "kind": "function", "doc": "<p>Clip the gradients by the cumulative masks. The gradients are multiplied by (1 - cumulative_previous_mask) to keep previously masked parameters fixed. See Eq. (4) in the <a href=\"https://proceedings.mlr.press/v162/kang22b/kang22b.pdf\">WSN paper</a>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.forward", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): the input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass, should be one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n<li><strong>task_id</strong> (<code>int</code> | <code>None</code>): the task ID where the data are from. If the stage is 'train' or 'validation', it should be the current task <code>self.task_id</code>. If the stage is 'test', it could be from any seen task (TIL uses the provided task IDs for testing).</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>weight_mask</strong> (<code>dict[str, Tensor]</code>): the weight mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has same (output features, input features) as weight.</li>\n<li><strong>bias_mask</strong> (<code>dict[str, Tensor]</code>): the bias mask for the current task. Key (<code>str</code>) is layer name, value (<code>Tensor</code>) is the mask tensor. The mask tensor has same (output features, ) as bias. If the layer doesn't have bias, it is <code>None</code>.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.training_step", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.training_step", "kind": "function", "doc": "<p>Training step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary containing loss and other metrics from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs. For WSN, it includes 'weight_mask' and 'bias_mask' for logging.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.on_train_end", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.on_train_end", "kind": "function", "doc": "<p>Store the weight and bias masks and update the cumulative masks after training the task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.validation_step", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.validation_step", "kind": "function", "doc": "<p>Validation step for current task <code>self.task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this validation step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_algorithms.wsn.WSN.test_step", "modulename": "clarena.cl_algorithms.wsn", "qualname": "WSN.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets", "modulename": "clarena.cl_datasets", "kind": "module", "doc": "<h1 id=\"continual-learning-datasets\">Continual Learning Datasets</h1>\n\n<p>This submodule provides the <strong>continual learning datasets</strong> that can be used in CLArena.</p>\n\n<p>Here are the base classes for continual learning datasets, which inherit from Lightning <code>LightningDataModule</code>:</p>\n\n<ul>\n<li><code>CLDataset</code>: The base class for all continual learning datasets.\n<ul>\n<li><code>CLPermutedDataset</code>: The base class for permuted continual learning datasets. A child class of <code>CLDataset</code>.</li>\n<li><code>CLSplitDataset</code>: The base class for split continual learning datasets. A child class of <code>CLDataset</code>.</li>\n<li><code>CLCombinedDataset</code>: The base class for combined continual learning datasets. A child class of <code>CLDataset</code>.</li>\n</ul></li>\n</ul>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about how to configure and implement continual learning datasets:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/CL-dataset\"><strong>Configure CL Dataset</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/cl_dataset\"><strong>Implement Custom CL Dataset</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-CL-dataset\"><strong>A Beginners' Guide to Continual Learning (CL Dataset)</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.cl_datasets.CLDataset", "modulename": "clarena.cl_datasets", "qualname": "CLDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets.</p>\n", "bases": "lightning.pytorch.core.datamodule.LightningDataModule"}, {"fullname": "clarena.cl_datasets.CLDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>dict[int, str]</code>): the root directory where the original data files for constructing the CL dataset physically live.\nIf it is a dict, the keys are task IDs and the values are the root directories for each task. If it is a string, it is the same root directory for all tasks.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLDataset.root", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.root", "kind": "variable", "doc": "<p>The dict of root directories of the original data files for each task.</p>\n", "annotation": ": dict[int, str]"}, {"fullname": "clarena.cl_datasets.CLDataset.num_tasks", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.num_tasks", "kind": "variable", "doc": "<p>The maximum number of tasks supported by the dataset.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.cl_paradigm", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.cl_paradigm", "kind": "variable", "doc": "<p>The continual learning paradigm.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.CLDataset.batch_size", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.batch_size", "kind": "variable", "doc": "<p>The dict of batch sizes for each task.</p>\n", "annotation": ": dict[int, int]"}, {"fullname": "clarena.cl_datasets.CLDataset.num_workers", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.num_workers", "kind": "variable", "doc": "<p>The dict of numbers of workers for each task.</p>\n", "annotation": ": dict[int, int]"}, {"fullname": "clarena.cl_datasets.CLDataset.custom_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.custom_transforms", "kind": "variable", "doc": "<p>The dict of custom transforms for each task.</p>\n", "annotation": ": dict[int, typing.Union[typing.Callable, torchvision.transforms.transforms.Compose, NoneType]]"}, {"fullname": "clarena.cl_datasets.CLDataset.repeat_channels", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.repeat_channels", "kind": "variable", "doc": "<p>The dict of number of channels to repeat for each task.</p>\n", "annotation": ": dict[int, int | None]"}, {"fullname": "clarena.cl_datasets.CLDataset.to_tensor", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.to_tensor", "kind": "variable", "doc": "<p>The dict of to_tensor flag for each task.</p>\n", "annotation": ": dict[int, bool]"}, {"fullname": "clarena.cl_datasets.CLDataset.resize", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.resize", "kind": "variable", "doc": "<p>The dict of sizes to resize to for each task.</p>\n", "annotation": ": dict[int, tuple[int, int] | None]"}, {"fullname": "clarena.cl_datasets.CLDataset.root_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.root_t", "kind": "variable", "doc": "<p>The root directory of the original data files for the current task <code>self.task_id</code>.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.CLDataset.batch_size_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.batch_size_t", "kind": "variable", "doc": "<p>The batch size for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.num_workers_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.num_workers_t", "kind": "variable", "doc": "<p>The number of workers for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.custom_transforms_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.custom_transforms_t", "kind": "variable", "doc": "<p>The custom transforms for the current task <code>self.task_id</code>.</p>\n", "annotation": ": Union[Callable, torchvision.transforms.transforms.Compose, NoneType]"}, {"fullname": "clarena.cl_datasets.CLDataset.repeat_channels_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.repeat_channels_t", "kind": "variable", "doc": "<p>The number of channels to repeat for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int | None"}, {"fullname": "clarena.cl_datasets.CLDataset.to_tensor_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.to_tensor_t", "kind": "variable", "doc": "<p>The to_tensor flag for the current task <code>self.task_id</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cl_datasets.CLDataset.resize_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.resize_t", "kind": "variable", "doc": "<p>The size to resize for the current task <code>self.task_id</code>.</p>\n", "annotation": ": tuple[int, int] | None"}, {"fullname": "clarena.cl_datasets.CLDataset.mean_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.mean_t", "kind": "variable", "doc": "<p>The mean values for normalization for the current task <code>self.task_id</code>.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.CLDataset.std_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.std_t", "kind": "variable", "doc": "<p>The standard deviation values for normalization for the current task <code>self.task_id</code>.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.CLDataset.dataset_train_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.dataset_train_t", "kind": "variable", "doc": "<p>The training dataset object. Can be a PyTorch Dataset object or any other dataset object.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.cl_datasets.CLDataset.dataset_val_t", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.dataset_val_t", "kind": "variable", "doc": "<p>The validation dataset object. Can be a PyTorch Dataset object or any other dataset object.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.cl_datasets.CLDataset.dataset_test", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.dataset_test", "kind": "variable", "doc": "<p>The dictionary to store test dataset object of each task. Keys are task IDs and values are the dataset objects. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": dict[int, typing.Any]"}, {"fullname": "clarena.cl_datasets.CLDataset.task_id", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to the number of tasks in the CL dataset.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLDataset.processed_task_ids", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.cl_datasets.CLDataset.sanity_check", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.get_cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.get_cl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map of the task. Keys are the original class labels and values are the integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of each task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of each task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.prepare_data", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.prepare_data", "kind": "function", "doc": "<p>Use this to download and prepare data. It must be implemented by subclasses, as required by <code>LightningDataModule</code>. This method is called at the beginning of each task.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.setup", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.setup", "kind": "function", "doc": "<p>Set up the dataset for different stages. This method is called at the beginning of each task.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage of the experiment; one of:\n<ul>\n<li>'fit': training and validation datasets of the current task <code>self.task_id</code> are assigned to <code>self.dataset_train_t</code> and <code>self.dataset_val_t</code>.</li>\n<li>'test': a dict of test datasets of all seen tasks should be assigned to <code>self.dataset_test</code>.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.setup_tasks_eval", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.setup_tasks_eval", "kind": "function", "doc": "<p>Set up tasks for continual learning main evaluation.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to evaluate.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.set_cl_paradigm", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.set_cl_paradigm", "kind": "function", "doc": "<p>Set <code>cl_paradigm</code> to <code>self.cl_paradigm</code>. It is used to define the CL class map.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_paradigm</strong> (<code>str</code>): the continual learning paradigm, either 'TIL' (Task-Incremental Learning) or 'CIL' (Class-Incremental Learning).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_paradigm</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.train_and_val_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms for training and validation datasets, incorporating the custom transforms with basic transforms like normalization and <code>ToTensor()</code>. It can be used in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed train/val transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.test_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.test_transforms", "kind": "function", "doc": "<p>Transforms for the test dataset. Only basic transforms like normalization and <code>ToTensor()</code> are included. It is used in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed test transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.target_transform", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.target_transform", "kind": "function", "doc": "<p>Target transform to map the original class labels to CL class labels according to <code>self.cl_paradigm</code>. It can be used in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>target_transform</strong> (<code>Callable</code>): the target transform function.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ClassMapping</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.train_and_val_dataset", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation datasets of the current task <code>self.task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Any, Any]</code>): the train and validation datasets of the current task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.test_dataset", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of the current task <code>self.task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Any</code>): the test dataset of the current task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.train_dataloader", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.train_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the train stage of the current task <code>self.task_id</code>. It is automatically called before training the task.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_dataloader</strong> (<code>DataLoader</code>): the train DataLoader of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.val_dataloader", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.val_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the validation stage of the current task <code>self.task_id</code>. It is automatically called before the task's validation.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>val_dataloader</strong> (<code>DataLoader</code>): the validation DataLoader of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLDataset.test_dataloader", "modulename": "clarena.cl_datasets", "qualname": "CLDataset.test_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the test stage of the current task <code>self.task_id</code>. It is automatically called before testing the task.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataloader</strong> (<code>dict[int, DataLoader]</code>): the test DataLoader dict of <code>self.task_id</code> and all tasks before (as the test is conducted on all seen tasks). Keys are task IDs and values are the DataLoaders.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets constructed as permutations of an original dataset.</p>\n", "bases": "clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original dataset live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.original_dataset_python_class", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class. <strong>It must be provided in subclasses.</strong></p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.original_dataset_constants", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.original_dataset_constants", "kind": "variable", "doc": "<p>The original dataset constants class.</p>\n", "annotation": ": type[clarena.stl_datasets.raw.constants.DatasetConstants]"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permutation_mode", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permutation_mode", "kind": "variable", "doc": "<p>The mode of permutation.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permutation_seeds", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permutation_seeds", "kind": "variable", "doc": "<p>The dict of permutation seeds for each task.</p>\n", "annotation": ": dict[int, int]"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permutation_seed_t", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permutation_seed_t", "kind": "variable", "doc": "<p>The permutation seed for the current task <code>self.task_id</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.permute_transform_t", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.permute_transform_t", "kind": "variable", "doc": "<p>The permutation transform for the current task <code>self.task_id</code>.</p>\n", "annotation": ": clarena.utils.transforms.Permute"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.sanity_check", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.get_cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.get_cl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map of the task. Keys are the original class labels and values are the integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of a task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.train_and_val_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms for training and validation datasets, incorporating the custom transforms with basic transforms like normalization and <code>ToTensor()</code>. In permuted CL datasets, a permute transform also applies.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed train/val transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLPermutedDataset.test_transforms", "modulename": "clarena.cl_datasets", "qualname": "CLPermutedDataset.test_transforms", "kind": "function", "doc": "<p>Transforms for the test dataset. Only basic transforms like normalization and <code>ToTensor()</code> are included. In permuted CL datasets, a permute transform also applies.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed test transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets constructed as splits of an original dataset.</p>\n", "bases": "clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original dataset live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.original_dataset_python_class", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class. <strong>It must be provided in subclasses.</strong></p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.original_dataset_constants", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.original_dataset_constants", "kind": "variable", "doc": "<p>The original dataset constants class.</p>\n", "annotation": ": type[clarena.stl_datasets.raw.constants.DatasetConstants]"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.class_split", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.class_split", "kind": "variable", "doc": "<p>The dict of class splits for each task.</p>\n", "annotation": ": dict[int, list[int]]"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.sanity_check", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.get_cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.get_cl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map of the task. Keys are the original class labels and values are the integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of a task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLSplitDataset.get_subset_of_classes", "modulename": "clarena.cl_datasets", "qualname": "CLSplitDataset.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset for the current task <code>self.task_id</code>. It is used when constructing the split. <strong>It must be implemented by subclasses.</strong></p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve the subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset", "kind": "class", "doc": "<p>The base class of continual learning datasets constructed as combinations of several single-task datasets (one dataset per task).</p>\n", "bases": "clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.__init__", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>datasets</strong> (<code>dict[int, str]</code>): the dict of dataset class paths for each task. The keys are task IDs and the values are the dataset class paths (as strings) to use for each task.</li>\n<li><strong>root</strong> (<code>str</code> | <code>dict[int, str]</code>): the root directory where the original data files for constructing the CL dataset physically live.\nIf it is a dict, the keys are task IDs and the values are the root directories for each task. If it is a string, it is the same root directory for all tasks.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">datasets</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.original_dataset_python_classes", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.original_dataset_python_classes", "kind": "variable", "doc": "<p>The dict of dataset classes for each task.</p>\n", "annotation": ": dict[int, torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.original_dataset_python_class_t", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.original_dataset_python_class_t", "kind": "variable", "doc": "<p>The dataset class for the current task <code>self.task_id</code>.</p>\n", "annotation": ": torch.utils.data.dataset.Dataset"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.original_dataset_constants_t", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.original_dataset_constants_t", "kind": "variable", "doc": "<p>The original dataset constants class for the current task <code>self.task_id</code>.</p>\n", "annotation": ": type[clarena.stl_datasets.raw.constants.DatasetConstants]"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.get_cl_class_map", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.get_cl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit continual learning settings <code>self.cl_paradigm</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the CL class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the CL class map of the task. Keys are the original class labels and values are the integer class label for continual learning.\n<ul>\n<li>If <code>self.cl_paradigm</code> is 'TIL', the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n<li>If <code>self.cl_paradigm</code> is 'CIL', the mapped class labels of a task should be continuous integers from the number of classes of previous tasks to the number of classes of the current task.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.CLCombinedDataset.setup_task_id", "modulename": "clarena.cl_datasets", "qualname": "CLCombinedDataset.setup_task_id", "kind": "function", "doc": "<p>Set up which task's dataset the CL experiment is on. This must be done before <code>setup()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.combined", "modulename": "clarena.cl_datasets.combined", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for combined datasets.</p>\n"}, {"fullname": "clarena.cl_datasets.combined.Combined", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined", "kind": "class", "doc": "<p>Combined CL dataset from available datasets.</p>\n", "bases": "clarena.cl_datasets.base.CLCombinedDataset"}, {"fullname": "clarena.cl_datasets.combined.Combined.__init__", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.__init__", "kind": "function", "doc": "<p>Initialize the Combined Torchvision dataset object providing the root where data files live.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>datasets</strong> (<code>list[str]</code>): the list of dataset class paths for each task. Each element in the list must be a string referring to a valid PyTorch Dataset class. It needs to be one in <code>self.AVAILABLE_DATASETS</code>.</li>\n<li><strong>root</strong> (<code>list[str]</code>): the list of root directory where the original data files for constructing the CL dataset physically live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some of the training data into validation data (only if validation set is not provided in the dataset).</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some of the entire data into test data (only if test set is not provided in the dataset).</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>list[int]</code>): The batch size in train, val, test dataloader. If <code>list[str]</code>, it should be a list of integers, each integer is the batch size for each task.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>list[int]</code>): the number of workers for dataloaders. If <code>list[str]</code>, it should be a list of integers, each integer is the num of workers for each task.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or list of them): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize, permute and so on are not included. If it is a list, each item is the custom transforms for each task.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | list of them): the number of channels to repeat for each task. Default is None, which means no repeat. If not None, it should be an integer. If it is a list, each item is the number of channels to repeat for each task.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>list[bool]</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers. If it is a list, each item is the size to resize for each task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">datasets</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.combined.Combined.AVAILABLE_DATASETS", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.AVAILABLE_DATASETS", "kind": "variable", "doc": "<p>The list of available datasets.</p>\n", "annotation": ": list[torchvision.datasets.vision.VisionDataset]", "default_value": "[&lt;class &#x27;clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits&#x27;&gt;, &lt;class &#x27;torchvision.datasets.cifar.CIFAR10&#x27;&gt;, &lt;class &#x27;torchvision.datasets.cifar.CIFAR100&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.cub2002011.CUB2002011&#x27;&gt;, &lt;class &#x27;torchvision.datasets.caltech.Caltech101&#x27;&gt;, &lt;class &#x27;torchvision.datasets.caltech.Caltech256&#x27;&gt;, &lt;class &#x27;torchvision.datasets.celeba.CelebA&#x27;&gt;, &lt;class &#x27;torchvision.datasets.country211.Country211&#x27;&gt;, &lt;class &#x27;torchvision.datasets.dtd.DTD&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTBalanced&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTByClass&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTByMerge&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTDigits&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTLetters&#x27;&gt;, &lt;class &#x27;torchvision.datasets.eurosat.EuroSAT&#x27;&gt;, &lt;class &#x27;torchvision.datasets.fer2013.FER2013&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftFamily&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftManufacturer&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftVariant&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub10&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub100&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub20&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub50&#x27;&gt;, &lt;class &#x27;torchvision.datasets.mnist.FashionMNIST&#x27;&gt;, &lt;class &#x27;torchvision.datasets.flowers102.Flowers102&#x27;&gt;, &lt;class &#x27;torchvision.datasets.food101.Food101&#x27;&gt;, &lt;class &#x27;torchvision.datasets.gtsrb.GTSRB&#x27;&gt;, &lt;class &#x27;torchvision.datasets.mnist.KMNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_128&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_256&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_32&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_64&#x27;&gt;, &lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.notmnist.NotMNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet2&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet37&#x27;&gt;, &lt;class &#x27;torchvision.datasets.pcam.PCAM&#x27;&gt;, &lt;class &#x27;torchvision.datasets.rendered_sst2.RenderedSST2&#x27;&gt;, &lt;class &#x27;torchvision.datasets.semeion.SEMEION&#x27;&gt;, &lt;class &#x27;torchvision.datasets.sun397.SUN397&#x27;&gt;, &lt;class &#x27;torchvision.datasets.svhn.SVHN&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST&#x27;&gt;, &lt;class &#x27;torchvision.datasets.stanford_cars.StanfordCars&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT&#x27;&gt;, &lt;class &#x27;tinyimagenet.TinyImageNet&#x27;&gt;, &lt;class &#x27;torchvision.datasets.usps.USPS&#x27;&gt;]"}, {"fullname": "clarena.cl_datasets.combined.Combined.test_percentage", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.combined.Combined.validation_percentage", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.combined.Combined.prepare_data", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.prepare_data", "kind": "function", "doc": "<p>Download the original datasets if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.combined.Combined.train_and_val_dataset", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.combined.Combined.test_dataset", "modulename": "clarena.cl_datasets.combined", "qualname": "Combined.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_SEMEION", "modulename": "clarena.cl_datasets.permuted_SEMEION", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted SEMEION dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_SEMEION.PermutedSEMEION", "modulename": "clarena.cl_datasets.permuted_SEMEION", "qualname": "PermutedSEMEION", "kind": "class", "doc": "<p>Permuted SEMEION dataset. The <a href=\"https://archive.ics.uci.edu/dataset/178/semeion+handwritten+digit\">SEMEION dataset</a> is a collection of handwritten digits. It consists of 1,593 handwritten digit images (10 classes), each 16x16 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_SEMEION.PermutedSEMEION.__init__", "modulename": "clarena.cl_datasets.permuted_SEMEION", "qualname": "PermutedSEMEION.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original SEMEION data 'SEMEION/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_SEMEION.PermutedSEMEION.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_SEMEION", "qualname": "PermutedSEMEION.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.semeion.SEMEION&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_SEMEION.PermutedSEMEION.test_percentage", "modulename": "clarena.cl_datasets.permuted_SEMEION", "qualname": "PermutedSEMEION.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_SEMEION.PermutedSEMEION.validation_percentage", "modulename": "clarena.cl_datasets.permuted_SEMEION", "qualname": "PermutedSEMEION.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_SEMEION.PermutedSEMEION.prepare_data", "modulename": "clarena.cl_datasets.permuted_SEMEION", "qualname": "PermutedSEMEION.prepare_data", "kind": "function", "doc": "<p>Download the original SEMEION dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_SEMEION.PermutedSEMEION.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_SEMEION", "qualname": "PermutedSEMEION.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_SEMEION.PermutedSEMEION.test_dataset", "modulename": "clarena.cl_datasets.permuted_SEMEION", "qualname": "PermutedSEMEION.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_ahdd", "modulename": "clarena.cl_datasets.permuted_ahdd", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Arabic Handwritten Digits dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits", "kind": "class", "doc": "<p>Permuted Arabic Handwritten Digits dataset. The <a href=\"https://www.kaggle.com/datasets/mloey1/ahdd1\">Arabic Handwritten Digits dataset</a> is a collection of handwritten Arabic digits (0-9). It consists of 60,000 training and 10,000 test images of handwritten Arabic digits (10 classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.__init__", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Arabic Handwritten Digits data 'ArabicHandwrittenDigits/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.validation_percentage", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.prepare_data", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.prepare_data", "kind": "function", "doc": "<p>Download the original Arabic Handwritten Digits dataset if haven't. Because the original dataset is published on Kaggle, we need to download it manually. This function will not download the original dataset automatically.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_ahdd.PermutedArabicHandwrittenDigits.test_dataset", "modulename": "clarena.cl_datasets.permuted_ahdd", "qualname": "PermutedArabicHandwrittenDigits.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech101", "modulename": "clarena.cl_datasets.permuted_caltech101", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Caltech 101 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101", "kind": "class", "doc": "<p>Permuted Caltech 101 dataset. The <a href=\"https://data.caltech.edu/records/mzrjq-6wc02\">Caltech 101 dataset</a> is a collection of pictures of objects. It consists of 9,146 images of 101 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.__init__", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Caltech data 'Caltech/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.caltech.Caltech101&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.test_percentage", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.validation_percentage", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.prepare_data", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.prepare_data", "kind": "function", "doc": "<p>Download the original Caltech 101 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech101.PermutedCaltech101.test_dataset", "modulename": "clarena.cl_datasets.permuted_caltech101", "qualname": "PermutedCaltech101.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech256", "modulename": "clarena.cl_datasets.permuted_caltech256", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Caltech 256 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256", "kind": "class", "doc": "<p>Permuted Caltech 256 dataset. The <a href=\"https://data.caltech.edu/records/nyy15-4j048\">Caltech 256 dataset</a> is a collection of pictures of objects. It consists of 30,607 images of 256 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.__init__", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Caltech data 'Caltech/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.caltech.Caltech256&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.test_percentage", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.validation_percentage", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.prepare_data", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.prepare_data", "kind": "function", "doc": "<p>Download the original Caltech 256 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_caltech256.PermutedCaltech256.test_dataset", "modulename": "clarena.cl_datasets.permuted_caltech256", "qualname": "PermutedCaltech256.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_celeba", "modulename": "clarena.cl_datasets.permuted_celeba", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted CelebA dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA", "kind": "class", "doc": "<p>Permuted CelebA dataset. The <a href=\"https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\">CelebFaces Attributes Dataset (CelebA)</a> is a large-scale celebrity faces dataset. It consists of 202,599 face images of 10,177 celebrity identities (classes), each 178x218 color image.</p>\n\n<p>Note that the original CelebA dataset is not a classification dataset but an attributes dataset. We only use the identity of each face as the class label for classification.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.__init__", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CelebA data 'CelebA/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.celeba.CelebA&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.prepare_data", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.prepare_data", "kind": "function", "doc": "<p>Download the original CelebA dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_celeba.PermutedCelebA.test_dataset", "modulename": "clarena.cl_datasets.permuted_celeba", "qualname": "PermutedCelebA.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar10", "modulename": "clarena.cl_datasets.permuted_cifar10", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted CIFAR-10 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10", "kind": "class", "doc": "<p>Permuted CIFAR-10 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 10 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.__init__", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-10 data 'cifar-10-python/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR10&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.validation_percentage", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.prepare_data", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar10.PermutedCIFAR10.test_dataset", "modulename": "clarena.cl_datasets.permuted_cifar10", "qualname": "PermutedCIFAR10.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar100", "modulename": "clarena.cl_datasets.permuted_cifar100", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted CIFAR-100 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100", "kind": "class", "doc": "<p>Permuted CIFAR-100 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-100 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 100 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.__init__", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-100 data 'cifar-100-python/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR100&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.validation_percentage", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.prepare_data", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cifar100.PermutedCIFAR100.test_dataset", "modulename": "clarena.cl_datasets.permuted_cifar100", "qualname": "PermutedCIFAR100.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_country211", "modulename": "clarena.cl_datasets.permuted_country211", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Country211 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211", "kind": "class", "doc": "<p>Permuted Country211 dataset. The <a href=\"https://github.com/openai/CLIP/blob/main/data/country211.md\">Country211 dataset</a> is a collection of geolocation pictures of different countries. It consists of 31,650 training, 10,550 validation, and 21,100 test images of 211 countries (classes), each 256x256 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.__init__", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Country211 data 'Country211/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.country211.Country211&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.prepare_data", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.prepare_data", "kind": "function", "doc": "<p>Download the original Country211 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_country211.PermutedCountry211.test_dataset", "modulename": "clarena.cl_datasets.permuted_country211", "qualname": "PermutedCountry211.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011", "modulename": "clarena.cl_datasets.permuted_cub2002011", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted CUB-200-2011 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011", "kind": "class", "doc": "<p>Permuted CUB-200-2011 dataset. The <a href=\"https://www.vision.caltech.edu/datasets/cub_200_2011/\">CUB (Caltech-UCSD Birds)-200-2011)</a> is a bird image dataset. It consists of 100,000 training, 10,000 validation, 10,000 test images of 200 bird species (classes), each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.__init__", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CUB-200-2011 data 'CUB_200_2011/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.cub2002011.CUB2002011&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.validation_percentage", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.prepare_data", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.prepare_data", "kind": "function", "doc": "<p>Download the original CUB-200-2011 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_cub2002011.PermutedCUB2002011.test_dataset", "modulename": "clarena.cl_datasets.permuted_cub2002011", "qualname": "PermutedCUB2002011.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_dtd", "modulename": "clarena.cl_datasets.permuted_dtd", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted DTD dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD", "kind": "class", "doc": "<p>Permuted DTD dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/dtd/\">DTD dataset</a> is a collection of describable texture pictures. It consists of 5,640 images of 47 kinds of textures (classes), each 300x300-640x640 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.__init__", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original DTD data 'DTD/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.dtd.DTD&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.prepare_data", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.prepare_data", "kind": "function", "doc": "<p>Download the original DTD dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_dtd.PermutedDTD.test_dataset", "modulename": "clarena.cl_datasets.permuted_dtd", "qualname": "PermutedDTD.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_emnist", "modulename": "clarena.cl_datasets.permuted_emnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted EMNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST", "kind": "class", "doc": "<p>Permuted EMNIST dataset. The <a href=\"https://www.nist.gov/itl/products-and-services/emnist-dataset/\">EMNIST dataset</a> is a collection of handwritten letters and digits (including A-Z, a-z, 0-9). It consists of 814,255 images in 62 classes, each 28x28 grayscale image.</p>\n\n<p>EMNIST has 6 different splits: <code>byclass</code>, <code>bymerge</code>, <code>balanced</code>, <code>letters</code>, <code>digits</code> and <code>mnist</code>, each containing a different subset of the original collection. We support all of them in Permuted EMNIST.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original EMNIST data 'EMNIST/' live.</li>\n<li><strong>split</strong> (<code>str</code>): the original EMNIST dataset has 6 different splits: <code>byclass</code>, <code>bymerge</code>, <code>balanced</code>, <code>letters</code>, <code>digits</code> and <code>mnist</code>. This argument specifies which one to use.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.split", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.split", "kind": "variable", "doc": "<p>The split of the original EMNIST dataset. It can be <code>byclass</code>, <code>bymerge</code>, <code>balanced</code>, <code>letters</code>, <code>digits</code> or <code>mnist</code>.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original EMNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_emnist.PermutedEMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_emnist", "qualname": "PermutedEMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_eurosat", "modulename": "clarena.cl_datasets.permuted_eurosat", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted EuroSAT dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT", "kind": "class", "doc": "<p>Permuted EuroSAT dataset. The <a href=\"https://github.com/phelber/eurosat\">EuroSAT dataset</a> is a collection of satellite images of lands. It consists of 27,000 images of 10 classes, each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.__init__", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Caltech data 'EuroSAT/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.eurosat.EuroSAT&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.test_percentage", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.validation_percentage", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.prepare_data", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.prepare_data", "kind": "function", "doc": "<p>Download the original EuroSAT dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_eurosat.PermutedEuroSAT.test_dataset", "modulename": "clarena.cl_datasets.permuted_eurosat", "qualname": "PermutedEuroSAT.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_facescrub", "modulename": "clarena.cl_datasets.permuted_facescrub", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted FaceScrub dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub", "kind": "class", "doc": "<p>Permuted FaceScrub dataset. The <a href=\"https://vintage.winklerbros.net/facescrub.html\">original FaceScrub dataset</a> is a collection of human face images. It consists 106,863 images of 530 people (classes), each high resolution color image.</p>\n\n<p>To make it simple, <a href=\"https://github.com/nkundiushuti/facescrub_subset\">this version</a> uses subset of the official <a href=\"http://megaface.cs.washington.edu/participate/challenge.html\">Megaface FaceScrub challenge</a>, cropped and resized to 32x32. We have <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_10.zip\">FaceScrub-10</a>, <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_20.zip\">FaceScrub-20</a>, <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_50.zip\">FaceScrub-50</a>, <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_100.zip\">FaceScrub-100</a> datasets where the number of classes are 10, 20, 50 and 100 respectively.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.__init__", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original FaceScrub data 'FaceScrub/' live.</li>\n<li><strong>size</strong> (<code>str</code>): the size of the dataset; one of:\n<ol>\n<li>'10': 10 classes (10 people).</li>\n<li>'20': 20 classes (20 people).</li>\n<li>'50': 50 classes (50 people).</li>\n<li>'100': 100 classes (100 people).</li>\n</ol></li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">size</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.validation_percentage", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.prepare_data", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.prepare_data", "kind": "function", "doc": "<p>Download the original FaceScrub dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_facescrub.PermutedFaceScrub.test_dataset", "modulename": "clarena.cl_datasets.permuted_facescrub", "qualname": "PermutedFaceScrub.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Fashion-MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST", "kind": "class", "doc": "<p>Permuted Fashion-MNIST dataset. The <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST dataset</a> is a collection of fashion images. It consists of 60,000 training and 10,000 test images of 10 types of clothing (classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Fashion-MNIST data 'FashionMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.FashionMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Fashion-MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fashionmnist.PermutedFashionMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_fashionmnist", "qualname": "PermutedFashionMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fer2013", "modulename": "clarena.cl_datasets.permuted_fer2013", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted FER2013 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013", "kind": "class", "doc": "<p>Permuted FER2013 dataset. The <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0893608014002159\">FER2013 dataset</a> is a collection of facial expression images. It consists of 35,887 images of 7 facial expressions (classes), each 48x48 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.__init__", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original FER2013 data 'FER2013/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.fer2013.FER2013&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.validation_percentage", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.prepare_data", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.prepare_data", "kind": "function", "doc": "<p>Download the original FER2013 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fer2013.PermutedFER2013.test_dataset", "modulename": "clarena.cl_datasets.permuted_fer2013", "qualname": "PermutedFER2013.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted FGVC-Aircraft dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft", "kind": "class", "doc": "<p>Permuted FGVC-Aircraft dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/\">FGVC-Aircraft dataset</a> is a collection of aircraft images. It consists of 10,200 images, each color image.</p>\n\n<p>FGVC-Aircraft has 3 different class labels by variant, family and manufacturer, which has 102, 70, 41 classes respectively. We support all of them in Permuted FGVC-Aircraft.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.__init__", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original FGVCAircraft data 'FGVCAircraft/' live.</li>\n<li><strong>annotation_level</strong> (<code>str</code>): The annotation level, supports 'variant', 'family' and 'manufacturer'.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">annotation_level</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.annotation_level", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.annotation_level", "kind": "variable", "doc": "<p>The annotation level, supports 'variant', 'family' and 'manufacturer'.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.prepare_data", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.prepare_data", "kind": "function", "doc": "<p>Download the original FGVC-Aircraft dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_fgvc_aircraft.PermutedFGVCAircraft.test_dataset", "modulename": "clarena.cl_datasets.permuted_fgvc_aircraft", "qualname": "PermutedFGVCAircraft.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_flowers102", "modulename": "clarena.cl_datasets.permuted_flowers102", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Oxford 102 Flower dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102", "kind": "class", "doc": "<p>Permuted Oxford 102 Flower dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Oxford 102 Flower dataset</a> is a collection of flower pictures. It consists of 8,189 images of 102 kinds of flowers (classes), each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.__init__", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Oxford 102 Flower data 'Flower102/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.flowers102.Flowers102&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.prepare_data", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.prepare_data", "kind": "function", "doc": "<p>Download the original Oxford 102 Flower dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_flowers102.PermutedFlowers102.test_dataset", "modulename": "clarena.cl_datasets.permuted_flowers102", "qualname": "PermutedFlowers102.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_food101", "modulename": "clarena.cl_datasets.permuted_food101", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Food-101 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101", "kind": "class", "doc": "<p>Permuted Food-101 dataset. The <a href=\"https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/\">Food-101 dataset</a> is a collection of food images. It consists of 101,000 images of 101 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.__init__", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Food-101 data 'Food101/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.food101.Food101&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.validation_percentage", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.prepare_data", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.prepare_data", "kind": "function", "doc": "<p>Download the original Food-101 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_food101.PermutedFood101.test_dataset", "modulename": "clarena.cl_datasets.permuted_food101", "qualname": "PermutedFood101.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb", "modulename": "clarena.cl_datasets.permuted_gtsrb", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted GTSRB dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB", "kind": "class", "doc": "<p>Permuted GTSRB dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">GTSRB dataset</a> is a collection of traffic sign images. It consists of 51,839 images of 43 different traffic signs (classes), each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.__init__", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original GTSRB data 'GTSRB/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.gtsrb.GTSRB&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.validation_percentage", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.prepare_data", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.prepare_data", "kind": "function", "doc": "<p>Download the original GTSRB dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_gtsrb.PermutedGTSRB.test_dataset", "modulename": "clarena.cl_datasets.permuted_gtsrb", "qualname": "PermutedGTSRB.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_imagenette", "modulename": "clarena.cl_datasets.permuted_imagenette", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Imagenette dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette", "kind": "class", "doc": "<p>Permuted Imagenette dataset. The <a href=\"https://github.com/fastai/imagenette\">Imagenette dataset</a> is a subset of 10 easily classified classes from <a href=\"https://www.image-net.org\">Imagenet</a>. It provides full sizes (as Imagenet), and resized 320x320 and 160x160. We support all of them in Permuted Imagenette.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.__init__", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Imagenette data 'Imagenette/' live.</li>\n<li><strong>size</strong> (<code>str</code>): image size type. Supports \"full\" (default), \"320px\", and \"160px\".</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">size</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.imagenette.Imagenette&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.size", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.size", "kind": "variable", "doc": "<p>The size type of image.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.validation_percentage", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.prepare_data", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.prepare_data", "kind": "function", "doc": "<p>Download the original Imagenette dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_imagenette.PermutedImagenette.test_dataset", "modulename": "clarena.cl_datasets.permuted_imagenette", "qualname": "PermutedImagenette.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Kannada-MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST", "kind": "class", "doc": "<p>Permuted Kannada-MNIST dataset. The <a href=\"https://github.com/vinayprabhu/Kannada_MNIST\">Kannada-MNIST dataset</a> is a collection of handwritten Kannada digits (0-9). It consists of 60,000 training and 10,000 test images of handwritten Kannada digits (10 classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Kannada-MNIST data 'KannadaMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Kannada-MNIST dataset if haven't. Because the original dataset is published on Kaggle, we need to download it manually. This function will not download the original dataset automatically.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kannadamnist.PermutedKannadaMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_kannadamnist", "qualname": "PermutedKannadaMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kmnist", "modulename": "clarena.cl_datasets.permuted_kmnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Kuzushiji-MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST", "kind": "class", "doc": "<p>Permuted Kuzushiji-MNIST dataset. The <a href=\"https://github.com/rois-codh/kmnist\">Kuzushiji-MNIST dataset</a> is a collection of Japanese Kuzushiji character images. It consists of 60,000 training and 10,000 test images of Japanese Kuzushiji images (10 classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Kuzushiji-MNIST data 'KMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.KMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Kuzushiji-MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_kmnist.PermutedKMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_kmnist", "qualname": "PermutedKMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Linnaeus 5 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5", "kind": "class", "doc": "<p>Permuted Linnaeus 5 dataset. The <a href=\"https://chaladze.com/l5/\">Linnaeus 5 dataset</a> is a collection of flower images. It consists of 8,000 images of 5 flower species (classes). It provides 256x256, 128x128, 64x64, and 32x32 color images. We support all of them in Permuted Linnaeus 5.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.__init__", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Linnaeus 5 data 'Linnaeus5/' live.</li>\n<li><strong>resolution</strong> (<code>str</code>): Image resolution, one of [\"256\", \"128\", \"64\", \"32\"].</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">resolution</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.resolution", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.resolution", "kind": "variable", "doc": "<p>Store the resolution of the original dataset.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.validation_percentage", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.prepare_data", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.prepare_data", "kind": "function", "doc": "<p>Download the original Linnaeus 5 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_linnaeus5.PermutedLinnaeus5.test_dataset", "modulename": "clarena.cl_datasets.permuted_linnaeus5", "qualname": "PermutedLinnaeus5.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_mnist", "modulename": "clarena.cl_datasets.permuted_mnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST", "kind": "class", "doc": "<p>Permuted MNIST dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST dataset</a> is a collection of handwritten digits. It consists of 60,000 training and 10,000 test images of handwritten digit images (10 classes), each 28x28 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original MNIST data 'MNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_mnist.PermutedMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_mnist", "qualname": "PermutedMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_notmnist", "modulename": "clarena.cl_datasets.permuted_notmnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted NotMNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST", "kind": "class", "doc": "<p>Permuted NotMNIST dataset. The <a href=\"https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\">NotMNIST dataset</a> is a collection of letters (A-J). Permuted MNIST dataset. This version uses the smaller set, which consists of about 19,000 images of 10 classes, each 28x28 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original NotMNIST data 'NotMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.notmnist.NotMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original NotMNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_notmnist.PermutedNotMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_notmnist", "qualname": "PermutedNotMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Oxford-IIIT Pet dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet", "kind": "class", "doc": "<p>Permuted Oxford-IIIT Pet dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/pets/\">Oxford-IIIT Pet dataset</a> is a collection of cat and dog pictures. It consists of 7,349 images of 37 breeds (classes), each color image. It also provides a binary classification version with 2 classes (cat or dog). We support both versions in Permuted Oxford-IIIT Pet.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.__init__", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Oxford-IIIT Pet data 'OxfordIIITPet/' live.</li>\n<li><strong>target_type</strong> (<code>str</code>): the target type; one of:\n<ol>\n<li>'category': Label for one of the 37 pet categories.</li>\n<li>'binary-category': Binary label for cat or dog.</li>\n</ol></li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">target_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.target_type", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.target_type", "kind": "variable", "doc": "<p>The target type.</p>\n", "annotation": ": str"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.validation_percentage", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.prepare_data", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.prepare_data", "kind": "function", "doc": "<p>Download the original Oxford-IIIT Pet dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_oxford_iiit_pet.PermutedOxfordIIITPet.test_dataset", "modulename": "clarena.cl_datasets.permuted_oxford_iiit_pet", "qualname": "PermutedOxfordIIITPet.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_pcam", "modulename": "clarena.cl_datasets.permuted_pcam", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted PCAM dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM", "kind": "class", "doc": "<p>Permuted PCAM dataset. The <a href=\"https://github.com/basveeling/pcam\">PCAM dataset</a> is a collection of medical images of breast cancer. It consists of 327,680 images in 2 classes (benign and malignant), each 96x96 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.__init__", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original PCAM data 'PCAM/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.pcam.PCAM&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.prepare_data", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.prepare_data", "kind": "function", "doc": "<p>Download the original PCAM dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_pcam.PermutedPCAM.test_dataset", "modulename": "clarena.cl_datasets.permuted_pcam", "qualname": "PermutedPCAM.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Rendered SST2 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2", "kind": "class", "doc": "<p>Permuted Rendered SST2 dataset. The <a href=\"https://github.com/openai/CLIP/blob/main/data/rendered-sst2.md\">Rendered SST2 dataset</a> is a collection of optical character recognition images. It consists of 9,613 images in 2 classes (positive and negative sentiment), each 448x448 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.__init__", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Rendered SST2 data 'RenderedSST2/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.rendered_sst2.RenderedSST2&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.prepare_data", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.prepare_data", "kind": "function", "doc": "<p>Download the original Rendered SST2 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_renderedsst2.PermutedRenderedSST2.test_dataset", "modulename": "clarena.cl_datasets.permuted_renderedsst2", "qualname": "PermutedRenderedSST2.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Sign Language MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST", "kind": "class", "doc": "<p>Permuted Sign Language MNIST dataset. The <a href=\"https://www.kaggle.com/datasets/datamunge/sign-language-mnist\">Sign Language MNIST dataset</a> is a collection of hand gesture images representing ASL letters (A-Y, excluding J). It consists of 34,627 images of 24 classes, each 28x28 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.__init__", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Sign Language MNIST data 'SignLanguageMNIST/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.validation_percentage", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.prepare_data", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Sign Language MNIST dataset if haven't. Because the original dataset is published on Kaggle, we need to download it manually. This function will not download the original dataset automatically.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sign_language_mnist.PermutedSignLanguageMNIST.test_dataset", "modulename": "clarena.cl_datasets.permuted_sign_language_mnist", "qualname": "PermutedSignLanguageMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted Stanford Cars dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars", "kind": "class", "doc": "<p>Permuted Stanford Cars dataset. The <a href=\"https://pytorch.org/vision/stable/generated/torchvision.datasets.StanfordCars.html#torchvision.datasets.StanfordCars/\">Stanford Cars dataset</a> is a collection of car images. It consists of 16,185 images in 196 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.__init__", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Stanford Cars data 'StanfordCars/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.stanford_cars.StanfordCars&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.validation_percentage", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.prepare_data", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.prepare_data", "kind": "function", "doc": "<p>Download the original Stanford Cars dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_stanfordcars.PermutedStanfordCars.test_dataset", "modulename": "clarena.cl_datasets.permuted_stanfordcars", "qualname": "PermutedStanfordCars.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sun397", "modulename": "clarena.cl_datasets.permuted_sun397", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted SUN397 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397", "kind": "class", "doc": "<p>Permuted SUN397 dataset. The <a href=\"https://vision.princeton.edu/projects/2010/SUN\">SUN397 dataset</a> is a collection of scene images. It consists of 108,754 images of 397 classes, each color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.__init__", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original SUN397 data 'SUN397/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.sun397.SUN397&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.test_percentage", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.validation_percentage", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.prepare_data", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.prepare_data", "kind": "function", "doc": "<p>Download the original SUN397 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_sun397.PermutedSUN397.test_dataset", "modulename": "clarena.cl_datasets.permuted_sun397", "qualname": "PermutedSUN397.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_svhn", "modulename": "clarena.cl_datasets.permuted_svhn", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted SVHN dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN", "kind": "class", "doc": "<p>Permuted SVHN dataset. The <a href=\"http://ufldl.stanford.edu/housenumbers/\">SVHN dataset</a> is a collection of street view house number images. It consists 73,257 training and 26,032 test images of 10 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.__init__", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original SVHN data 'SVHN/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.svhn.SVHN&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.validation_percentage", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.prepare_data", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.prepare_data", "kind": "function", "doc": "<p>Download the original SVHN dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_svhn.PermutedSVHN.test_dataset", "modulename": "clarena.cl_datasets.permuted_svhn", "qualname": "PermutedSVHN.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted TinyImageNet dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet", "kind": "class", "doc": "<p>Permuted TinyImageNet dataset. The <a href=\"http://vision.stanford.edu/teaching/cs231n/reports/2015/pdfs/yle_project.pdf\">TinyImageNet dataset</a> is smaller, more manageable version of the <a href=\"https://www.image-net.org\">Imagenet dataset</a>. It consists of 100,000 training, 10,000 validation and 10,000 test images of 200 classes, each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.__init__", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original TinyImageNet data 'tiny-imagenet-200/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;tinyimagenet.TinyImageNet&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.validation_percentage", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.prepare_data", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.prepare_data", "kind": "function", "doc": "<p>Download the original TinyImageNet dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_tinyimagenet.PermutedTinyImageNet.test_dataset", "modulename": "clarena.cl_datasets.permuted_tinyimagenet", "qualname": "PermutedTinyImageNet.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_usps", "modulename": "clarena.cl_datasets.permuted_usps", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Permuted USPS dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS", "kind": "class", "doc": "<p>Permuted USPS dataset. The <a href=\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps\">USPS dataset</a> is a collection of handwritten digits. It consists of 9,298 handwritten digit images (10 classes), each 16x16 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLPermutedDataset"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.__init__", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original USPS data 'USPS/' live.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the CL dataset. This decides the valid task IDs from 1 to <code>num_tasks</code>.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n<li><strong>permutation_mode</strong> (<code>str</code>): the mode of permutation; one of:\n<ol>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ol></li>\n<li><strong>permutation_seeds</strong> (<code>dict[int, int]</code> | <code>None</code>): the dict of seeds for permutation operations used to construct each task. Keys are task IDs and the values are permutation seeds for each task. Default is <code>None</code>, which creates a dict of seeds from 0 to <code>num_tasks</code>-1.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">permutation_seeds</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.original_dataset_python_class", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.usps.USPS&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.validation_percentage", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.prepare_data", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.prepare_data", "kind": "function", "doc": "<p>Download the original USPS dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.train_and_val_dataset", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.permuted_usps.PermutedUSPS.test_dataset", "modulename": "clarena.cl_datasets.permuted_usps", "qualname": "PermutedUSPS.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar10", "modulename": "clarena.cl_datasets.split_cifar10", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split CIFAR-10 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10", "kind": "class", "doc": "<p>Split CIFAR-10 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 10 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.__init__", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-10 data 'cifar-10-python/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR10&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.validation_percentage", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.prepare_data", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR-10 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar10.SplitCIFAR10.test_dataset", "modulename": "clarena.cl_datasets.split_cifar10", "qualname": "SplitCIFAR10.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100", "modulename": "clarena.cl_datasets.split_cifar100", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split CIFAR-100 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100", "kind": "class", "doc": "<p>Split CIFAR-100 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-100 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 100 classes, each 32x32 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.__init__", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-100 data 'cifar-100-python/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR100&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.validation_percentage", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.prepare_data", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR-100 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cifar100.SplitCIFAR100.test_dataset", "modulename": "clarena.cl_datasets.split_cifar100", "qualname": "SplitCIFAR100.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cub2002011", "modulename": "clarena.cl_datasets.split_cub2002011", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split CUB-200-2011 dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011", "kind": "class", "doc": "<p>Split CUB-200-2011 dataset. The <a href=\"https://www.vision.caltech.edu/datasets/cub_200_2011/\">CUB (Caltech-UCSD Birds)-200-2011)</a> is a bird image dataset. It consists of 100,000 training, 10,000 validation, 10,000 test images of 200 bird species (classes), each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.__init__", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CUB-200-2011 data 'CUB_200_2011/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.cub2002011.CUB2002011&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.validation_percentage", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.prepare_data", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.prepare_data", "kind": "function", "doc": "<p>Download the original CUB-200-2011 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_cub2002011.SplitCUB2002011.test_dataset", "modulename": "clarena.cl_datasets.split_cub2002011", "qualname": "SplitCUB2002011.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_mnist", "modulename": "clarena.cl_datasets.split_mnist", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split MNIST dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST", "kind": "class", "doc": "<p>Split MNIST dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST dataset</a> is a collection of handwritten digits. It consists of 60,000 training and 10,000 test images of handwritten digit images (10 classes), each 28x28 grayscale image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.__init__", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original MNIST data 'MNIST/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.validation_percentage", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.prepare_data", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>Dataset</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_mnist.SplitMNIST.test_dataset", "modulename": "clarena.cl_datasets.split_mnist", "qualname": "SplitMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet", "modulename": "clarena.cl_datasets.split_tinyimagenet", "kind": "module", "doc": "<p>The submodule in <code>cl_datasets</code> for Split TinyImageNet dataset.</p>\n"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet", "kind": "class", "doc": "<p>Split TinyImageNet dataset. The <a href=\"http://vision.stanford.edu/teaching/cs231n/reports/2015/pdfs/yle_project.pdf\">TinyImageNet dataset</a> is smaller, more manageable version of the <a href=\"https://www.image-net.org\">Imagenet dataset</a>. It consists of 100,000 training, 10,000 validation and 10,000 test images of 200 classes, each 64x64 color image.</p>\n", "bases": "clarena.cl_datasets.base.CLSplitDataset"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.__init__", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original TinyImageNet data 'tiny-imagenet-200/' live.</li>\n<li><strong>class_split</strong> (<code>dict[int, list[int]]</code>): the dict of classes for each task. The keys are task IDs ane the values are lists of class labels (integers starting from 0) to split for each task.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): The percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code> | <code>dict[int, int]</code>): the batch size for train, val, and test dataloaders.\nIf it is a dict, the keys are task IDs and the values are the batch sizes for each task. If it is an <code>int</code>, it is the same batch size for all tasks.</li>\n<li><strong>num_workers</strong> (<code>int</code> | <code>dict[int, int]</code>): the number of workers for dataloaders.\nIf it is a dict, the keys are task IDs and the values are the number of workers for each task. If it is an <code>int</code>, it is the same number of workers for all tasks.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, permute, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">class_split</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.original_dataset_python_class", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;tinyimagenet.TinyImageNet&#x27;&gt;"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.validation_percentage", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.prepare_data", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.prepare_data", "kind": "function", "doc": "<p>Download the original TinyImagenet dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.get_subset_of_classes", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.get_subset_of_classes", "kind": "function", "doc": "<p>Get a subset of classes from the dataset of current classes of <code>self.task_id</code>. It is used when constructing the split.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>ImageFolder</code>): the dataset to retrieve subset from.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>subset</strong> (<code>ImageFolder</code>): the subset of classes from the dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">folder</span><span class=\"o\">.</span><span class=\"n\">ImageFolder</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">folder</span><span class=\"o\">.</span><span class=\"n\">ImageFolder</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.train_and_val_dataset", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_dataset</strong> (<code>Dataset</code>): the training dataset of task <code>self.task_id</code>.</li>\n<li><strong>val_dataset</strong> (<code>Dataset</code>): the validation dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cl_datasets.split_tinyimagenet.SplitTinyImageNet.test_dataset", "modulename": "clarena.cl_datasets.split_tinyimagenet", "qualname": "SplitTinyImageNet.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>self.task_id</code>.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>self.task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cul_algorithms", "modulename": "clarena.cul_algorithms", "kind": "module", "doc": "<h1 id=\"continual-unlearning-algorithms\">Continual Unlearning Algorithms</h1>\n\n<p>This submodule provides the <strong>continual unlearning algorithms</strong> in CLArena.</p>\n\n<p>Here are the base classes for CUL algorithms:</p>\n\n<ul>\n<li><code>CULAlgorithm</code>: the base class for all continual unlearning algorithms.</li>\n</ul>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about how to configure and implement CUL algorithms:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/cul-algorithm\"><strong>Configure CUL Algorithm</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/cul-algorithm\"><strong>Implement Custom CUL Algorithm</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm", "kind": "class", "doc": "<p>The base class of continual unlearning algorithms.</p>\n"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.__init__", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>model</strong> (<code>UnlearnableCLAlgorithm</code>): the continual learning model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">UnlearnableCLAlgorithm</span></span>)</span>"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.model", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.model", "kind": "variable", "doc": "<p>The continual learning model.</p>\n", "annotation": ": clarena.cl_algorithms.base.UnlearnableCLAlgorithm"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.task_id", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.processed_task_ids", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.unlearning_task_ids", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.unlearning_task_ids", "kind": "variable", "doc": "<p>The list of task IDs that are requested to be unlearned after training <code>self.task_id</code>.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.unlearned_task_ids", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.unlearned_task_ids", "kind": "variable", "doc": "<p>The list of task IDs that have been unlearned in the experiment.</p>\n", "annotation": ": set[int]"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.if_permanent_t", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.if_permanent_t", "kind": "variable", "doc": "<p>Whether the task is permanent or not. If <code>True</code>, the task will not be unlearned i.e. not shown in future unlearning requests.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.setup_task_id", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.setup_task_id", "kind": "function", "doc": "<p>Set up which task the CUL experiment is on. This must be done before <code>unlearn()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID to be set up.</li>\n<li><strong>unlearning_requests</strong> (<code>dict[int, list[int]]</code>): the entire unlearning requests. Keys are IDs of the tasks that request unlearning after their learning, and values are the list of the previous tasks to be unlearned.</li>\n<li><strong>if_permanent</strong> (<code>bool</code>): whether the task is permanent or not. If <code>True</code>, the task will not be unlearned i.e. not shown in future unlearning requests.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">unlearning_requests</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">if_permanent</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.setup_test_task_id", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.setup_test_task_id", "kind": "function", "doc": "<p>Set up before testing <code>self.task_id</code>. This must be done after <code>unlearn()</code> method is called.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cul_algorithms.CULAlgorithm.unlearn", "modulename": "clarena.cul_algorithms", "qualname": "CULAlgorithm.unlearn", "kind": "function", "doc": "<p>Unlearn the requested unlearning tasks after training <code>self.task_id</code>. <strong>It must be implemented in subclasses.</strong></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cul_algorithms.amnesiac_hat_unlearn", "modulename": "clarena.cul_algorithms.amnesiac_hat_unlearn", "kind": "module", "doc": "<p>The submoduule in <code>cul_algorithms</code> for AmnesiacHAT unlearning algorithm.</p>\n"}, {"fullname": "clarena.cul_algorithms.amnesiac_hat_unlearn.AmnesiacHATUnlearn", "modulename": "clarena.cul_algorithms.amnesiac_hat_unlearn", "qualname": "AmnesiacHATUnlearn", "kind": "class", "doc": "<p>The base class of the AmnesiacHAT unlearning algorithm.</p>\n", "bases": "clarena.cul_algorithms.base.CULAlgorithm"}, {"fullname": "clarena.cul_algorithms.amnesiac_hat_unlearn.AmnesiacHATUnlearn.__init__", "modulename": "clarena.cul_algorithms.amnesiac_hat_unlearn", "qualname": "AmnesiacHATUnlearn.__init__", "kind": "function", "doc": "<p>Initialize the unlearning algorithm with the continual learning model.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>model</strong> (<code>Independent</code>): the continual learning model (<code>CLAlgorithm</code> object which already contains the backbone and heads). It must be <code>HAT</code> algorithm.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">hat</span><span class=\"o\">.</span><span class=\"n\">HAT</span></span>)</span>"}, {"fullname": "clarena.cul_algorithms.amnesiac_hat_unlearn.AmnesiacHATUnlearn.delete_update", "modulename": "clarena.cul_algorithms.amnesiac_hat_unlearn", "qualname": "AmnesiacHATUnlearn.delete_update", "kind": "function", "doc": "<p>Delete the update of the specified unlearning task.\n<strong>Args:</strong></p>\n\n<ul>\n<li><strong>unlearning_task_id</strong> (<code>str</code>): the ID of the unlearning task to delete the update.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">unlearning_task_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cul_algorithms.amnesiac_hat_unlearn.AmnesiacHATUnlearn.compensate_layer_if_first_task", "modulename": "clarena.cul_algorithms.amnesiac_hat_unlearn", "qualname": "AmnesiacHATUnlearn.compensate_layer_if_first_task", "kind": "function", "doc": "<p>Compensate if the first task.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">layer_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">unlearning_task_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">if_first_task_layer</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">next_masked_task_layer</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cul_algorithms.amnesiac_hat_unlearn.AmnesiacHATUnlearn.unlearn", "modulename": "clarena.cul_algorithms.amnesiac_hat_unlearn", "qualname": "AmnesiacHATUnlearn.unlearn", "kind": "function", "doc": "<p>Unlearn the requested unlearning tasks in current task <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.cul_algorithms.independent_unlearn", "modulename": "clarena.cul_algorithms.independent_unlearn", "kind": "module", "doc": "<p>The submoduule in <code>cul_algorithms</code> for the vanilla unlearning algorithm for independent learning.</p>\n"}, {"fullname": "clarena.cul_algorithms.independent_unlearn.IndependentUnlearn", "modulename": "clarena.cul_algorithms.independent_unlearn", "qualname": "IndependentUnlearn", "kind": "class", "doc": "<p>Vanilla unlearning algorithm for independent learning.</p>\n", "bases": "clarena.cul_algorithms.base.CULAlgorithm"}, {"fullname": "clarena.cul_algorithms.independent_unlearn.IndependentUnlearn.__init__", "modulename": "clarena.cul_algorithms.independent_unlearn", "qualname": "IndependentUnlearn.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>model</strong> (<code>Independent</code>): the continual learning model (<code>CLAlgorithm</code> object which already contains the backbone and heads). It must be <code>Independent</code> algorithm.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">independent</span><span class=\"o\">.</span><span class=\"n\">Independent</span></span>)</span>"}, {"fullname": "clarena.cul_algorithms.independent_unlearn.IndependentUnlearn.unlearn", "modulename": "clarena.cul_algorithms.independent_unlearn", "qualname": "IndependentUnlearn.unlearn", "kind": "function", "doc": "<p>Unlearn the requested unlearning tasks after training <code>self.task_id</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads", "modulename": "clarena.heads", "kind": "module", "doc": "<h1 id=\"output-heads\">Output Heads</h1>\n\n<p>This submodule provides the <strong>output heads</strong> in CLArena.</p>\n\n<p>There are two types of continual learning / unlearning heads in CLArena: <code>HeadsTIL</code> and <code>HeadsCIL</code>, corresponding to two CL paradigms respectively: Task-Incremental Learning (TIL) and Class-Incremental Learning (CIL). For Multi-Task Learning (MTL), we have <code>HeadsMTL</code> which is a collection of independent heads for each task.</p>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about the heads.</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/configure-your-experiment/experiment-index-config\"><strong>Configure CL Paradigm in Experiment Index Config</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-beginners-guide#sec-CL-classification\">**A Beginners' Guide to Continual Learning (Multi-head Classifier)</a></li>\n</ul>\n"}, {"fullname": "clarena.heads.HeadsTIL", "modulename": "clarena.heads", "qualname": "HeadsTIL", "kind": "class", "doc": "<p>The output heads for Task-Incremental Learning (TIL). Independent head assigned to each TIL task takes the output from backbone network and forwards it into logits for predicting classes of the task.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.heads.HeadsTIL.__init__", "modulename": "clarena.heads", "qualname": "HeadsTIL.__init__", "kind": "function", "doc": "<p>Initializes TIL heads object with no heads.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.heads.HeadsTIL.heads", "modulename": "clarena.heads", "qualname": "HeadsTIL.heads", "kind": "variable", "doc": "<p>TIL output heads are stored independently in a <code>ModuleDict</code>. Keys are task IDs and values are the corresponding <code>nn.Linear</code> heads. We use <code>ModuleDict</code> rather than <code>dict</code> to make sure <code>LightningModule</code> can track these model parameters for the purpose of, such as automatically to device, recorded in model summaries.</p>\n\n<p>Note that the task IDs must be string type in order to let <code>LightningModule</code> identify this part of the model.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.heads.HeadsTIL.head_t", "modulename": "clarena.heads", "qualname": "HeadsTIL.head_t", "kind": "variable", "doc": "<p>The output head for the current task. It is created when the task arrives and stored in <code>self.heads</code>.</p>\n", "annotation": ": torch.nn.modules.linear.Linear | None"}, {"fullname": "clarena.heads.HeadsTIL.input_dim", "modulename": "clarena.heads", "qualname": "HeadsTIL.input_dim", "kind": "variable", "doc": "<p>Store the input dimension of the heads. Used when creating new heads.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsTIL.task_id", "modulename": "clarena.heads", "qualname": "HeadsTIL.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsTIL.setup_task_id", "modulename": "clarena.heads", "qualname": "HeadsTIL.setup_task_id", "kind": "function", "doc": "<p>Create the output head when task <code>task_id</code> arrives if there's no. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n<li><strong>num_classes_t</strong> (<code>int</code>): the number of classes in the task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">num_classes_t</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsTIL.get_head", "modulename": "clarena.heads", "qualname": "HeadsTIL.get_head", "kind": "function", "doc": "<p>Get the output head for task <code>task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>head_t</strong> (<code>nn.Linear</code>): the output head for task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">Linear</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsTIL.forward", "modulename": "clarena.heads", "qualname": "HeadsTIL.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. A head is selected according to the task_id and the feature is passed through the head.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID where the data are from, which is provided by task-incremental setting.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsCIL", "modulename": "clarena.heads", "qualname": "HeadsCIL", "kind": "class", "doc": "<p>The output heads for Class-Incremental Learning (CIL). Head of all classes from CIL tasks takes the output from backbone network and forwards it into logits for predicting classes of all tasks.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.heads.HeadsCIL.__init__", "modulename": "clarena.heads", "qualname": "HeadsCIL.__init__", "kind": "function", "doc": "<p>Initializes a CIL heads object with no heads.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.heads.HeadsCIL.heads", "modulename": "clarena.heads", "qualname": "HeadsCIL.heads", "kind": "variable", "doc": "<p>CIL output heads are stored in a <code>ModuleDict</code>. Keys are task IDs and values are the corresponding <code>nn.Linear</code> heads. We use <code>ModuleDict</code> rather than <code>dict</code> to make sure <code>LightningModule</code> can track these model parameters for the purpose of, such as automatically to device, recorded in model summaries.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.heads.HeadsCIL.input_dim", "modulename": "clarena.heads", "qualname": "HeadsCIL.input_dim", "kind": "variable", "doc": "<p>The input dimension of the heads. Used when creating new heads.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsCIL.task_id", "modulename": "clarena.heads", "qualname": "HeadsCIL.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Starting from 1.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsCIL.processed_task_ids", "modulename": "clarena.heads", "qualname": "HeadsCIL.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.heads.HeadsCIL.setup_task_id", "modulename": "clarena.heads", "qualname": "HeadsCIL.setup_task_id", "kind": "function", "doc": "<p>Create the output head when task <code>task_id</code> arrives if there's no. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n<li><strong>num_classes_t</strong> (<code>int</code>): the number of classes in the task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">num_classes_t</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsCIL.forward", "modulename": "clarena.heads", "qualname": "HeadsCIL.forward", "kind": "function", "doc": "<p>The forward pass for data. The information of which <code>task_id</code> the data are from is not provided. The head for all classes is selected and the feature is passed.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n<li><strong>task_id</strong> (<code>int</code> or <code>None</code>): the task ID where the data are from. In CIL, it is just a placeholder for API consistence with the TIL heads but never used. Best practices are not to provide this argument and leave it as the default value.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsMTL", "modulename": "clarena.heads", "qualname": "HeadsMTL", "kind": "class", "doc": "<p>The output heads for Multi-Task Learning (MTL). Independent head assigned to each task takes the output from backbone network and forwards it into logits for predicting classes of the task.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.heads.HeadsMTL.__init__", "modulename": "clarena.heads", "qualname": "HeadsMTL.__init__", "kind": "function", "doc": "<p>Initializes MTL heads object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.heads.HeadsMTL.heads", "modulename": "clarena.heads", "qualname": "HeadsMTL.heads", "kind": "variable", "doc": "<p>MTL output heads are stored independently in a <code>ModuleDict</code>. Keys are task IDs and values are the corresponding <code>nn.Linear</code> heads. We use <code>ModuleDict</code> rather than <code>dict</code> to make sure <code>LightningModule</code> can track these model parameters for the purpose of, such as automatically to device, recorded in model summaries.</p>\n\n<p>Note that the task IDs must be string type in order to let <code>LightningModule</code> identify this part of the model.</p>\n", "annotation": ": torch.nn.modules.container.ModuleDict"}, {"fullname": "clarena.heads.HeadsMTL.input_dim", "modulename": "clarena.heads", "qualname": "HeadsMTL.input_dim", "kind": "variable", "doc": "<p>Store the input dimension of the heads. Used when creating new heads.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadsMTL.setup_tasks", "modulename": "clarena.heads", "qualname": "HeadsMTL.setup_tasks", "kind": "function", "doc": "<p>Create the output heads. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>list[int]</code>): the target task IDs.</li>\n<li><strong>num_classes</strong> (<code>dict[int, int]</code>): the number of classes in each task. Keys are task IDs and values are the number of classes for the corresponding task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_ids</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">num_classes</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsMTL.get_head", "modulename": "clarena.heads", "qualname": "HeadsMTL.get_head", "kind": "function", "doc": "<p>Get the output head for task <code>task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the target task ID.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>head_t</strong> (<code>nn.Linear</code>): the output head for task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"o\">.</span><span class=\"n\">Linear</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadsMTL.forward", "modulename": "clarena.heads", "qualname": "HeadsMTL.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. A head is selected according to the task_id and the feature is passed through the head.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n<li><strong>task_ids</strong> (<code>int</code> | <code>Tensor</code>): the task ID(s) for the input data. If the input batch is from the same task, this can be a single integer.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">task_ids</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadSTL", "modulename": "clarena.heads", "qualname": "HeadSTL", "kind": "class", "doc": "<p>The output head for Single-Task Learning (STL).</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "clarena.heads.HeadSTL.__init__", "modulename": "clarena.heads", "qualname": "HeadSTL.__init__", "kind": "function", "doc": "<p>Initializes STL head object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the head. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.heads.HeadSTL.head", "modulename": "clarena.heads", "qualname": "HeadSTL.head", "kind": "variable", "doc": "<p>STL output head.</p>\n", "annotation": ": torch.nn.modules.linear.Linear"}, {"fullname": "clarena.heads.HeadSTL.input_dim", "modulename": "clarena.heads", "qualname": "HeadSTL.input_dim", "kind": "variable", "doc": "<p>Store the input dimension of the head. Used when creating new head.</p>\n", "annotation": ": int"}, {"fullname": "clarena.heads.HeadSTL.setup_task", "modulename": "clarena.heads", "qualname": "HeadSTL.setup_task", "kind": "function", "doc": "<p>Create the output head. This must be done before <code>forward()</code> is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>num_classes</strong> (<code>int</code>): the number of classes in the task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">num_classes</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.heads.HeadSTL.forward", "modulename": "clarena.heads", "qualname": "HeadSTL.forward", "kind": "function", "doc": "<p>The forward pass for data.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>feature</strong> (<code>Tensor</code>): the feature tensor from the backbone network.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">feature</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics", "modulename": "clarena.metrics", "kind": "module", "doc": "<h1 id=\"metrics\">Metrics</h1>\n\n<p>This submodule provides the <strong>metric callbacks</strong> in CLArena, which control each metric's computation, logging and visualization process.</p>\n\n<p>Here are the base classes for metric callbacks, which inherit from PyTorch Lightning <code>Callback</code>:</p>\n\n<ul>\n<li><code>MetricCallback</code>: the base class for all metric callbacks.</li>\n</ul>\n\n<p>Please note that this is an API documentation. Please refer to the main documentation pages for more information about how to configure and implement metric callbacks:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/metrics\"><strong>Configure Metrics</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/callback\"><strong>Implement Custom Callback</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics\"><strong>A Summary of Continual Learning Metrics</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.metrics.MetricCallback", "modulename": "clarena.metrics", "qualname": "MetricCallback", "kind": "class", "doc": "<p>The base class for all metrics callbacks in CLArena.</p>\n", "bases": "lightning.pytorch.callbacks.callback.Callback"}, {"fullname": "clarena.metrics.MetricCallback.__init__", "modulename": "clarena.metrics", "qualname": "MetricCallback.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.metrics.MetricCallback.save_dir", "modulename": "clarena.metrics", "qualname": "MetricCallback.save_dir", "kind": "variable", "doc": "<p>The directory where data and figures of metrics will be saved.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.cl_acc", "modulename": "clarena.metrics.cl_acc", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>CLAccuracy</code>.</p>\n"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy", "kind": "class", "doc": "<p>Provides all actions that are related to CL accuracy metric, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording accuracy metric.</li>\n<li>Logging training and validation accuracy metric to Lightning loggers in real time.</li>\n<li>Saving test accuracy metric to files.</li>\n<li>Visualizing test accuracy metric as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for test accuracy (lower triangular) matrix and average accuracy. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Coloured plot for test accuracy (lower triangular) matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Curve plots for test average accuracy over different training tasks. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-average-test-performance-over-tasks\">here</a> for details.</li>\n</ul>\n\n<p>Please refer to the <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics\">A Summary of Continual Learning Metrics</a> to learn about this metric.</p>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.__init__", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_acc_csv_name</strong> (<code>str</code>): file name to save test accuracy matrix and average accuracy as CSV file.</li>\n<li><strong>test_acc_matrix_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save accuracy matrix plot. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_ave_acc_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save average accuracy as curve plot over different training tasks. If <code>None</code>, no file will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;acc.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_matrix_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_ave_acc_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.test_acc_csv_path", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.test_acc_csv_path", "kind": "variable", "doc": "<p>The path to save test accuracy matrix and average accuracy CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.acc_training_epoch", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.acc_training_epoch", "kind": "variable", "doc": "<p>Classification accuracy of training epoch. Accumulated and calculated from the training batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-performance-of-training-epoch\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.acc_val", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.acc_val", "kind": "variable", "doc": "<p>Validation classification accuracy of the model after training epoch. Accumulated and calculated from the validation batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-validation-performace\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.acc_test", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.acc_test", "kind": "variable", "doc": "<p>Test classification accuracy of the current model (<code>self.task_id</code>) on current and previous tasks. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics. It is the last row of the lower triangular matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.task_id", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.on_fit_start", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.on_train_batch_end", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.on_train_epoch_end", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.on_validation_batch_end", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.on_validation_epoch_end", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.on_test_start", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.on_test_batch_end", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the test data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.on_test_epoch_end", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.update_test_acc_to_csv", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.update_test_acc_to_csv", "kind": "function", "doc": "<p>Update the test accuracy metrics of seen tasks at the last line to an existing CSV file. A new file will be created if not existing.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>after_training_task_id</strong> (<code>int</code>): the task ID after training.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/acc.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">after_training_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.plot_test_acc_matrix_from_csv", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.plot_test_acc_matrix_from_csv", "kind": "function", "doc": "<p>Plot the test accuracy matrix from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.update_test_acc_to_csv()</code> saved the test accuracy metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/acc_matrix.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_acc.CLAccuracy.plot_test_ave_acc_curve_from_csv", "modulename": "clarena.metrics.cl_acc", "qualname": "CLAccuracy.plot_test_ave_acc_curve_from_csv", "kind": "function", "doc": "<p>Plot the test average accuracy curve over different training tasks from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.update_test_acc_to_csv()</code> saved the test accuracy metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/ave_acc.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss", "modulename": "clarena.metrics.cl_loss", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>CLLoss</code>.</p>\n"}, {"fullname": "clarena.metrics.cl_loss.CLLoss", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss", "kind": "class", "doc": "<p>Provides all actions that are related to CL loss metrics, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording loss metrics.</li>\n<li>Logging training and validation loss metrics to Lightning loggers in real time.</li>\n<li>Saving test loss metrics to files.</li>\n<li>Visualizing test loss metrics as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for classification loss (lower triangular) matrix and average classification loss. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Coloured plot for test classification loss (lower triangular) matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</li>\n<li>Curve plots for test average classification loss over different training tasks. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-average-test-performance-over-tasks\">here</a> for details.</li>\n</ul>\n\n<p>Please refer to the <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics\">A Summary of Continual Learning Metrics</a> to learn about this metric.</p>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.__init__", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_loss_cls_csv_name</strong>(<code>str</code>): file name to save classification loss matrix and average classification loss as CSV file.</li>\n<li><strong>test_loss_cls_matrix_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save classification loss matrix plot. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_ave_loss_cls_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save average classification loss as curve plot over different training tasks. If <code>None</code>, no file will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;loss_cls.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_matrix_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_ave_loss_cls_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.test_loss_cls_csv_path", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.test_loss_cls_csv_path", "kind": "variable", "doc": "<p>The path to save test classification loss matrix and average classification loss CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.loss_cls_training_epoch", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.loss_cls_training_epoch", "kind": "variable", "doc": "<p>Classification loss of training epoch. Accumulated and calculated from the training batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-performance-of-training-epoch\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.loss_training_epoch", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.loss_training_epoch", "kind": "variable", "doc": "<p>Total loss of training epoch. Accumulated and calculated from the training batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-performance-of-training-epoch\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.loss_cls_val", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.loss_cls_val", "kind": "variable", "doc": "<p>Validation classification of the model loss after training epoch. Accumulated and calculated from the validation batches. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-validation-performace\">here</a> for details.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.loss_cls_test", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.loss_cls_test", "kind": "variable", "doc": "<p>Test classification loss of the current model (<code>self.task_id</code>) on current and previous tasks. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics. It is the last row of the lower triangular matrix. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#sec-test-performance-of-previous-tasks\">here</a> for details.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.task_id", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.on_fit_start", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.on_train_batch_end", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.on_train_epoch_end", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.on_validation_batch_end", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.on_validation_epoch_end", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.on_test_start", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.on_test_batch_end", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>CLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the test data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.on_test_epoch_end", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.update_test_loss_cls_to_csv", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.update_test_loss_cls_to_csv", "kind": "function", "doc": "<p>Update the test classification loss metrics of seen tasks at the last line to an existing CSV file. A new file will be created if not existing.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>after_training_task_id</strong> (<code>int</code>): the task ID after training.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/loss_cls.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">after_training_task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.plot_test_loss_cls_matrix_from_csv", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.plot_test_loss_cls_matrix_from_csv", "kind": "function", "doc": "<p>Plot the test classification loss matrix from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.update_loss_cls_to_csv()</code> saved the test classification loss metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/loss_cls_matrix.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cl_loss.CLLoss.plot_test_ave_loss_cls_curve_from_csv", "modulename": "clarena.metrics.cl_loss", "qualname": "CLLoss.plot_test_ave_loss_cls_curve_from_csv", "kind": "function", "doc": "<p>Plot the test average classfication loss curve over different training tasks from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.update_test_acc_to_csv()</code> saved the test classfication loss metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/ave_loss_cls.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_ad", "modulename": "clarena.metrics.cul_ad", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>CULAccuracyDifference</code>.</p>\n"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference", "kind": "class", "doc": "<p>Provides all actions that are related to CUL accuracy difference (AD) metric, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording AD metric.</li>\n<li>Saving AD metric to files.</li>\n<li>Visualizing AD metric as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for AD in each task.</li>\n<li>Coloured plot for AD in each task.</li>\n</ul>\n\n<p>Note that this callback is designed to be used with the <code>CULEvaluation</code> module, which is a special evaluation module for continual unlearning. It is not a typical test step in the algorithm, but rather a test protocol that evaluates the performance of the model on unlearned tasks.</p>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference.__init__", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>accuracy_difference_csv_name</strong> (<code>str</code>): file name to save test accuracy difference metrics as CSV file.</li>\n<li><strong>accuracy_difference_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save test accuracy difference metrics as plot. If <code>None</code>, no plot will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">accuracy_difference_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;ad.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">accuracy_difference_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference.accuracy_difference_csv_path", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference.accuracy_difference_csv_path", "kind": "variable", "doc": "<p>The path to save the test accuracy difference metrics CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference.accuracy_difference", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference.accuracy_difference", "kind": "variable", "doc": "<p>Accuracy difference (between main and full model) metrics for each seen task. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference.task_id", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference.on_test_start", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference.on_test_batch_end", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>CULEvaluation</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference.on_test_epoch_end", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference.update_unlearning_accuracy_difference_to_csv", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference.update_unlearning_accuracy_difference_to_csv", "kind": "function", "doc": "<p>Update the unlearning accuracy difference metrics of unlearning tasks to CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>accuracy_difference_metric</strong> (<code>dict[int, MeanMetricBatch]</code>): the accuracy difference metric. Accumulated and calculated from the unlearning test batches.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/unlearning_test_after_task_X/distance.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">accuracy_difference_metric</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">MeanMetricBatch</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_ad.CULAccuracyDifference.plot_unlearning_accuracy_difference_from_csv", "modulename": "clarena.metrics.cul_ad", "qualname": "CULAccuracyDifference.plot_unlearning_accuracy_difference_from_csv", "kind": "function", "doc": "<p>Plot the unlearning accuracy difference matrix over different unlearned tasks from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.save_accuracy_difference_to_csv()</code> saved the accuracy difference metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/unlearning_test_after_task_X/distance.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_dd", "modulename": "clarena.metrics.cul_dd", "kind": "module", "doc": "<p>The submodule in <code>callbacks</code> for <code>CULDistributionDistance</code>.</p>\n"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance", "kind": "class", "doc": "<p>Provides all actions that are related to CUL distribution distance (DD) metric, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording DD metric.</li>\n<li>Saving DD metric to files.</li>\n<li>Visualizing DD metric as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for DD in each task.</li>\n<li>Coloured plot for DD in each task.</li>\n</ul>\n\n<p>Note that this callback is designed to be used with the <code>CULEvaluation</code> module, which is a special evaluation module for continual unlearning. It is not a typical test step in the algorithm, but rather a test protocol that evaluates the performance of the model on unlearned tasks.</p>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.__init__", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>distribution_distance_type</strong> (<code>str</code>): the type of distribution distance to use; one of:\n<ul>\n<li>'euclidean': Eulidean distance.</li>\n<li>'cosine': Cosine distance.</li>\n<li>'manhattan': Manhattan distance.</li>\n</ul></li>\n<li><strong>distribution_distance_csv_name</strong> (<code>str</code>): file name to save test distribution distance metrics as CSV file.</li>\n<li><strong>distribution_distance_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save test distribution distance metrics as plot. If <code>None</code>, no plot will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">distribution_distance_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">distribution_distance_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;dd.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">distribution_distance_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.distribution_distance_type", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.distribution_distance_type", "kind": "variable", "doc": "<p>The type of distribution distance to use.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.distribution_distance_csv_path", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.distribution_distance_csv_path", "kind": "variable", "doc": "<p>The path to save the test distribution distance metrics CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.distribution_distance", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.distribution_distance", "kind": "variable", "doc": "<p>Distribution distance unlearning metrics for each seen task. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.task_id", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.on_test_start", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.on_test_batch_end", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>CULEvaluation</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.on_test_epoch_end", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">eval</span><span class=\"o\">.</span><span class=\"n\">CULEvaluation</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.update_distribution_distance_to_csv", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.update_distribution_distance_to_csv", "kind": "function", "doc": "<p>Update the unlearning distribution distance metrics of unlearning tasks to CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>distance_metric</strong> (<code>dict[int, MeanMetricBatch]</code>): the distribution distance metric of unlearned tasks. Accumulated and calculated from the unlearning test batches.</li>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/unlearning_test_after_task_X/distance.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">distance_metric</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">metrics</span><span class=\"o\">.</span><span class=\"n\">MeanMetricBatch</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.cul_dd.CULDistributionDistance.plot_distribution_distance_from_csv", "modulename": "clarena.metrics.cul_dd", "qualname": "CULDistributionDistance.plot_distribution_distance_from_csv", "kind": "function", "doc": "<p>Plot the unlearning test distance matrix over different unlearned tasks from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>utils.save_distribution_distance_to_csv()</code> saved the unlearning test distance metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/unlearning_test_after_task_X/distance.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.hat_adjustment_rate", "modulename": "clarena.metrics.hat_adjustment_rate", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>HATAdjustmentRate</code>.</p>\n"}, {"fullname": "clarena.metrics.hat_adjustment_rate.HATAdjustmentRate", "modulename": "clarena.metrics.hat_adjustment_rate", "qualname": "HATAdjustmentRate", "kind": "class", "doc": "<p>Provides all actions that are related to adjustment rate of <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> algorithm and its extensions, which include:</p>\n\n<ul>\n<li>Visualizing adjustment rate during training as figures.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>Figures of training adjustment rate.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.hat_adjustment_rate.HATAdjustmentRate.__init__", "modulename": "clarena.metrics.hat_adjustment_rate", "qualname": "HATAdjustmentRate.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code> | <code>None</code>): The directory to save the adjustment rate figures. Better inside the output folder.</li>\n<li><strong>plot_adjustment_rate_every_n_steps</strong> (<code>int</code> | <code>None</code>): the frequency of plotting adjustment rate figures in terms of number of batches during training.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_adjustment_rate_every_n_steps</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.hat_adjustment_rate.HATAdjustmentRate.plot_adjustment_rate_every_n_steps", "modulename": "clarena.metrics.hat_adjustment_rate", "qualname": "HATAdjustmentRate.plot_adjustment_rate_every_n_steps", "kind": "variable", "doc": "<p>The frequency of plotting adjustment rate in terms of number of batches.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.hat_adjustment_rate.HATAdjustmentRate.task_id", "modulename": "clarena.metrics.hat_adjustment_rate", "qualname": "HATAdjustmentRate.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.hat_adjustment_rate.HATAdjustmentRate.on_fit_start", "modulename": "clarena.metrics.hat_adjustment_rate", "qualname": "HATAdjustmentRate.on_fit_start", "kind": "function", "doc": "<p>Get the current task ID in the beginning of a task's fitting (training and validation). Sanity check the <code>pl_module</code> to be <code>HAT</code>.</p>\n\n<p><strong>Raises:</strong>\n-<strong>TypeError</strong>: when the <code>pl_module</code> is not <code>HAT</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">hat</span><span class=\"o\">.</span><span class=\"n\">HAT</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.hat_adjustment_rate.HATAdjustmentRate.on_train_batch_end", "modulename": "clarena.metrics.hat_adjustment_rate", "qualname": "HATAdjustmentRate.on_train_batch_end", "kind": "function", "doc": "<p>Plot adjustment rate after training batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>HAT</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">hat</span><span class=\"o\">.</span><span class=\"n\">HAT</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.hat_adjustment_rate.HATAdjustmentRate.plot_hat_adjustment_rate", "modulename": "clarena.metrics.hat_adjustment_rate", "qualname": "HATAdjustmentRate.plot_hat_adjustment_rate", "kind": "function", "doc": "<p>Plot adjustment rate in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a>) algorithm. This includes the adjustment rate weight and adjustment rate bias (if applicable).</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate</strong> (<code>dict[str, Tensor]</code>): the adjustment rate. Keys (<code>str</code>) are layer names and values (<code>Tensor</code>) are the adjustment rate tensors. If it's adjustment rate weight, it has size same as weights. If it's adjustment rate bias, it has size same as biases.</li>\n<li><strong>weight_or_bias</strong> (<code>str</code>): the type of adjustment rate. It can be either 'weight' or 'bias'. This is to form the plot name.</li>\n<li><strong>step</strong> (<code>int</code>): the training step (batch index) of the adjustment rate to be plotted. This is to form the plot name. Keep <code>None</code> for not showing the step in the plot name.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_rate</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">weight_or_bias</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">step</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.hat_masks", "modulename": "clarena.metrics.hat_masks", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>HATMasks</code>.</p>\n"}, {"fullname": "clarena.metrics.hat_masks.HATMasks", "modulename": "clarena.metrics.hat_masks", "qualname": "HATMasks", "kind": "class", "doc": "<p>Provides all actions that are related to masks of <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> algorithm and its extensions, which include:</p>\n\n<ul>\n<li>Visualizing mask and cumulative mask figures during training and testing as figures.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>Figures of both training and test, masks and cumulative masks.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.hat_masks.HATMasks.__init__", "modulename": "clarena.metrics.hat_masks", "qualname": "HATMasks.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): the directory to save the mask figures. Better inside the output folder.</li>\n<li><strong>test_masks_dir_name</strong> (<code>str</code> | <code>None</code>): the relative path to <code>save_dir</code> to save the test mask figures. If <code>None</code>, no file will be saved.</li>\n<li><strong>test_cumulative_masks_dir_name</strong> (<code>str</code> | <code>None</code>): the directory to save the test cumulative mask figures. If <code>None</code>, no file will be saved.</li>\n<li><strong>training_masks_dir_name</strong> (<code>str</code> | <code>None</code>): the directory to save the training mask figures. If <code>None</code>, no file will be saved.</li>\n<li><strong>plot_training_mask_every_n_steps</strong> (<code>int</code> | <code>None</code>): the frequency of plotting training mask figures in terms of number of batches during training. Only applies when <code>training_masks_dir_name</code> is not <code>None</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_masks_dir_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">test_cumulative_masks_dir_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">training_masks_dir_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">plot_training_mask_every_n_steps</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.hat_masks.HATMasks.plot_training_mask_every_n_steps", "modulename": "clarena.metrics.hat_masks", "qualname": "HATMasks.plot_training_mask_every_n_steps", "kind": "variable", "doc": "<p>The frequency of plotting training masks in terms of number of batches.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.hat_masks.HATMasks.task_id", "modulename": "clarena.metrics.hat_masks", "qualname": "HATMasks.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.hat_masks.HATMasks.on_fit_start", "modulename": "clarena.metrics.hat_masks", "qualname": "HATMasks.on_fit_start", "kind": "function", "doc": "<p>Get the current task ID in the beginning of a task's fitting (training and validation). Sanity check the <code>pl_module</code> to be <code>HAT</code>.</p>\n\n<p><strong>Raises:</strong>\n-<strong>TypeError</strong>: when the <code>pl_module</code> is not <code>HAT</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">hat</span><span class=\"o\">.</span><span class=\"n\">HAT</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.hat_masks.HATMasks.on_train_batch_end", "modulename": "clarena.metrics.hat_masks", "qualname": "HATMasks.on_train_batch_end", "kind": "function", "doc": "<p>Plot training mask after training batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>HAT</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">hat</span><span class=\"o\">.</span><span class=\"n\">HAT</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.hat_masks.HATMasks.on_test_start", "modulename": "clarena.metrics.hat_masks", "qualname": "HATMasks.on_test_start", "kind": "function", "doc": "<p>Plot test mask and cumulative mask figures.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">hat</span><span class=\"o\">.</span><span class=\"n\">HAT</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.hat_masks.HATMasks.plot_hat_mask", "modulename": "clarena.metrics.hat_masks", "qualname": "HATMasks.plot_hat_mask", "kind": "function", "doc": "<p>Plot mask in <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a>) algorithm. This includes the mask and cumulative mask.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>mask</strong> (<code>dict[str, Tensor]</code>): the hard attention (whose values are 0 or 1) mask. Keys (<code>str</code>) are layer name and values (<code>Tensor</code>) are the mask tensors. The mask tensor has size (number of units, ).</li>\n<li><strong>plot_dir</strong> (<code>str</code>): the directory to save plot. Better same as the output directory of the experiment.</li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID of the mask to be plotted. This is to form the plot name.</li>\n<li><strong>step</strong> (<code>int</code>): the training step (batch index) of the mask to be plotted. Apply to the training mask only. This is to form the plot name. Keep <code>None</code> for not showing the step in the plot name.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">plot_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">step</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.hat_network_capacity", "modulename": "clarena.metrics.hat_network_capacity", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>HATNetworkCapacity</code>.</p>\n"}, {"fullname": "clarena.metrics.hat_network_capacity.HATNetworkCapacity", "modulename": "clarena.metrics.hat_network_capacity", "qualname": "HATNetworkCapacity", "kind": "class", "doc": "<p>Provides all actions that are related to network capacity of <a href=\"http://proceedings.mlr.press/v80/serra18a\">HAT (Hard Attention to the Task)</a> algorithm and its extensions, which include:</p>\n\n<ul>\n<li>Logging network capacity during training. See the \"Evaluation Metrics\" section in Sec. 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a> for more details about network capacity.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.hat_network_capacity.HATNetworkCapacity.__init__", "modulename": "clarena.metrics.hat_network_capacity", "qualname": "HATNetworkCapacity.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): the directory to save the mask figures. Better inside the output folder.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "clarena.metrics.hat_network_capacity.HATNetworkCapacity.task_id", "modulename": "clarena.metrics.hat_network_capacity", "qualname": "HATNetworkCapacity.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to <code>cl_dataset.num_tasks</code>.</p>\n", "annotation": ": int"}, {"fullname": "clarena.metrics.hat_network_capacity.HATNetworkCapacity.on_fit_start", "modulename": "clarena.metrics.hat_network_capacity", "qualname": "HATNetworkCapacity.on_fit_start", "kind": "function", "doc": "<p>Get the current task ID in the beginning of a task's fitting (training and validation). Sanity check the <code>pl_module</code> to be <code>HAT</code>.</p>\n\n<p><strong>Raises:</strong>\n-<strong>TypeError</strong>: when the <code>pl_module</code> is not <code>HAT</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">hat</span><span class=\"o\">.</span><span class=\"n\">HAT</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.hat_network_capacity.HATNetworkCapacity.on_train_batch_end", "modulename": "clarena.metrics.hat_network_capacity", "qualname": "HATNetworkCapacity.on_train_batch_end", "kind": "function", "doc": "<p>Plot training mask, adjustment rate and log network capacity after training batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, which is the returns of the <code>training_step()</code> method in the <code>HAT</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n<li><strong>batch_idx</strong> (<code>int</code>): the index of the current batch. This is for the file name of mask figures.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">hat</span><span class=\"o\">.</span><span class=\"n\">HAT</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_acc", "modulename": "clarena.metrics.mtl_acc", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>MTLAccuracy</code>.</p>\n"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy", "kind": "class", "doc": "<p>Provides all actions that are related to MTL accuracy metrics, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording accuracy metric.</li>\n<li>Logging training and validation accuracy metric to Lightning loggers in real time.</li>\n<li>Saving test accuracy metric to files.</li>\n<li>Visualizing test accuracy metric as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for test accuracy of all tasks and average accuracy.</li>\n<li>Bar charts for test accuracy of all tasks.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.__init__", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_acc_csv_name</strong> (<code>str</code>): file name to save test accuracy of all tasks and average accuracy as CSV file.</li>\n<li><strong>test_acc_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save accuracy plot. If <code>None</code>, no file will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;acc.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">test_acc_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.test_acc_csv_path", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.test_acc_csv_path", "kind": "variable", "doc": "<p>The path to save test accuracy of all tasks and average accuracy CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.acc_training_epoch", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.acc_training_epoch", "kind": "variable", "doc": "<p>Classification accuracy of training epoch. Accumulated and calculated from the training batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.acc_val", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.acc_val", "kind": "variable", "doc": "<p>Validation classification accuracy of the model after training epoch. Accumulated and calculated from the validation batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.acc_test", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.acc_test", "kind": "variable", "doc": "<p>Test classification accuracy of all tasks. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.on_fit_start", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.on_train_batch_end", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.on_train_epoch_end", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.on_validation_batch_end", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of the validation dataloader. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.on_test_start", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.on_test_batch_end", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.on_test_epoch_end", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.save_test_acc_to_csv", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.save_test_acc_to_csv", "kind": "function", "doc": "<p>Save the test accuracy metrics of all tasks in multi-task learning to an CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/acc.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_acc.MTLAccuracy.plot_test_acc_from_csv", "modulename": "clarena.metrics.mtl_acc", "qualname": "MTLAccuracy.plot_test_acc_from_csv", "kind": "function", "doc": "<p>Plot the test accuracy bar chart of all tasks in multi-task learning from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the csv file where the <code>utils.save_test_acc_csv()</code> saved the test accuracy metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/acc.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss", "modulename": "clarena.metrics.mtl_loss", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>MTLLoss</code>.</p>\n"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss", "kind": "class", "doc": "<p>Provides all actions that are related to MTL loss metrics, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording loss metrics.</li>\n<li>Logging training and validation loss metrics to Lightning loggers in real time.</li>\n<li>Saving test loss metrics to files.</li>\n<li>Visualizing test loss metrics as plots.</li>\n</ul>\n\n<p>The callback is able to produce the following outputs:</p>\n\n<ul>\n<li>CSV files for test classification loss of all tasks and average classification loss.</li>\n<li>Bar charts for test classification loss of all tasks.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.__init__", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_loss_cls_csv_name</strong>(<code>str</code>): file name to save classification loss of all tasks and average classification loss as CSV file.</li>\n<li><strong>test_loss_cls_plot_name</strong> (<code>str</code> | <code>None</code>): file name to save classification loss plot. If <code>None</code>, no file will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;loss_cls.csv&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">test_loss_cls_plot_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.test_loss_cls_csv_path", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.test_loss_cls_csv_path", "kind": "variable", "doc": "<p>The path to save test classification loss of all tasks and average classification loss CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.loss_cls_training_epoch", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.loss_cls_training_epoch", "kind": "variable", "doc": "<p>Classification loss of training epoch. Accumulated and calculated from the training batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.loss_cls_val", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.loss_cls_val", "kind": "variable", "doc": "<p>Validation classification loss of the model after training epoch. Accumulated and calculated from the validation batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.loss_cls_test", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.loss_cls_test", "kind": "variable", "doc": "<p>Test classification loss of all tasks. Accumulated and calculated from the test batches. Keys are task IDs and values are the corresponding metrics.</p>\n", "annotation": ": dict[int, clarena.utils.metrics.MeanMetricBatch]"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.on_fit_start", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.on_train_batch_end", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.on_train_epoch_end", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.on_validation_batch_end", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of the validation dataloader. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.on_validation_epoch_end", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.on_test_start", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.on_test_batch_end", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>MTLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.on_test_epoch_end", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">mtl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">MTLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.save_test_loss_cls_to_csv", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.save_test_loss_cls_to_csv", "kind": "function", "doc": "<p>Save the test classification loss metrics of all tasks in multi-task learning to an CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/loss_cls.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.mtl_loss.MTLLoss.plot_test_loss_cls_from_csv", "modulename": "clarena.metrics.mtl_loss", "qualname": "MTLLoss.plot_test_loss_cls_from_csv", "kind": "function", "doc": "<p>Plot the test classification loss bar chart of all tasks in multi-task learning from saved CSV file and save the plot to the designated directory.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): the path to the csv file where the <code>utils.save_test_acc_csv()</code> saved the test classification loss metric.</li>\n<li><strong>plot_path</strong> (<code>str</code>): the path to save plot. Better same as the output directory of the experiment. E.g. './outputs/expr_name/1970-01-01_00-00-00/loss_cls.png'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">plot_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_acc", "modulename": "clarena.metrics.stl_acc", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>STLAccuracy</code>.</p>\n"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy", "kind": "class", "doc": "<p>Provides all actions that are related to STL accuracy metric, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording accuracy metric.</li>\n<li>Logging training and validation accuracy metric to Lightning loggers in real time.</li>\n</ul>\n\n<p>Saving test accuracy metric to files.</p>\n\n<ul>\n<li>The callback is able to produce the following outputs:</li>\n<li>CSV files for test accuracy.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.__init__", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_acc_csv_name</strong> (<code>str</code>): file name to save test accuracy of all tasks and average accuracy as CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">test_acc_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;acc.csv&#39;</span></span>)</span>"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.test_acc_csv_path", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.test_acc_csv_path", "kind": "variable", "doc": "<p>The path to save test accuracy of all tasks and average accuracy CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.acc_training_epoch", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.acc_training_epoch", "kind": "variable", "doc": "<p>Classification accuracy of training epoch. Accumulated and calculated from the training batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.acc_val", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.acc_val", "kind": "variable", "doc": "<p>Validation classification accuracy of the model after training epoch. Accumulated and calculated from the validation batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.acc_test", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.acc_test", "kind": "variable", "doc": "<p>Test classification accuracy. Accumulated and calculated from the test batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.on_fit_start", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.on_train_batch_end", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.on_train_epoch_end", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.on_validation_batch_end", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.on_validation_epoch_end", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.on_test_start", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.on_test_start", "kind": "function", "doc": "<p>Initialize the testing metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.on_test_batch_end", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the test data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.on_test_epoch_end", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_acc.STLAccuracy.save_test_acc_to_csv", "modulename": "clarena.metrics.stl_acc", "qualname": "STLAccuracy.save_test_acc_to_csv", "kind": "function", "doc": "<p>Save the test accuracy metrics of all tasks in single-task learning to an CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/acc.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_loss", "modulename": "clarena.metrics.stl_loss", "kind": "module", "doc": "<p>The submodule in <code>metrics</code> for <code>STLLoss</code>.</p>\n"}, {"fullname": "clarena.metrics.stl_loss.STLLoss", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss", "kind": "class", "doc": "<p>Provides all actions that are related to STL loss metrics, which include:</p>\n\n<ul>\n<li>Defining, initializing and recording loss metrics.</li>\n<li>Logging training and validation loss metrics to Lightning loggers in real time.</li>\n</ul>\n\n<p>Saving test loss metrics to files.</p>\n\n<ul>\n<li>The callback is able to produce the following outputs:</li>\n<li>CSV files for test classification loss.</li>\n</ul>\n", "bases": "clarena.metrics.base.MetricCallback"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.__init__", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>save_dir</strong> (<code>str</code>): The directory where data and figures of metrics will be saved. Better inside the output folder.</li>\n<li><strong>test_loss_cls_csv_name</strong>(<code>str</code>): file name to save classification loss of all tasks and average classification loss as CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">test_loss_cls_csv_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;loss_cls.csv&#39;</span></span>)</span>"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.test_loss_cls_csv_path", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.test_loss_cls_csv_path", "kind": "variable", "doc": "<p>The path to save test classification loss of all tasks and average classification loss CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.loss_cls_training_epoch", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.loss_cls_training_epoch", "kind": "variable", "doc": "<p>Classification loss of training epoch. Accumulated and calculated from the training batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.loss_cls_val", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.loss_cls_val", "kind": "variable", "doc": "<p>Validation classification of the model loss after training epoch. Accumulated and calculated from the validation batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.loss_cls_test", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.loss_cls_test", "kind": "variable", "doc": "<p>Test classification loss. Accumulated and calculated from the test batches.</p>\n", "annotation": ": clarena.utils.metrics.MeanMetricBatch"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.on_fit_start", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.on_fit_start", "kind": "function", "doc": "<p>Initialize training and validation metrics.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.on_train_batch_end", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.on_train_batch_end", "kind": "function", "doc": "<p>Record training metrics from training batch, log metrics of training batch and accumulated metrics of the epoch to Lightning loggers.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the training step, the returns of the <code>training_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the training data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.on_train_epoch_end", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.on_train_epoch_end", "kind": "function", "doc": "<p>Log metrics of training epoch to plot learning curves and reset the metrics accumulation at the end of training epoch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.on_validation_batch_end", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.on_validation_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from validation batch. We don't need to log and monitor the metrics of validation batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the validation step, which is the returns of the <code>validation_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the validation data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.on_validation_epoch_end", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.on_validation_epoch_end", "kind": "function", "doc": "<p>Log validation metrics to plot learning curves.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.on_test_start", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.on_test_start", "kind": "function", "doc": "<p>Initialize the metrics for testing each seen task in the beginning of a task's testing.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.on_test_batch_end", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.on_test_batch_end", "kind": "function", "doc": "<p>Accumulating metrics from test batch. We don't need to log and monitor the metrics of test batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Any]</code>): the outputs of the test step, which is the returns of the <code>test_step()</code> method in the <code>STLAlgorithm</code>.</li>\n<li><strong>batch</strong> (<code>Any</code>): the test data batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.on_test_epoch_end", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.on_test_epoch_end", "kind": "function", "doc": "<p>Save and plot test metrics at the end of test.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">Trainer</span>,</span><span class=\"param\">\t<span class=\"n\">pl_module</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">stl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">STLAlgorithm</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.metrics.stl_loss.STLLoss.save_test_loss_cls_to_csv", "modulename": "clarena.metrics.stl_loss", "qualname": "STLLoss.save_test_loss_cls_to_csv", "kind": "function", "doc": "<p>Save the test classification loss metrics in single-task learning to an CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>csv_path</strong> (<code>str</code>): save the test metric to path. E.g. './outputs/expr_name/1970-01-01_00-00-00/results/loss_cls.csv'.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms", "modulename": "clarena.mtl_algorithms", "kind": "module", "doc": "<h1 id=\"multi-task-learning-algorithms\">Multi-Task Learning Algorithms</h1>\n\n<p>This submodule provides the <strong>multi-task learning algorithms</strong> in CLArena.</p>\n\n<p>Here are the base classes for MTL algorithms, which inherit from PyTorch Lightning <code>LightningModule</code>:</p>\n\n<ul>\n<li><code>MTLAlgorithm</code>: the base class for all multi-task learning algorithms.</li>\n</ul>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about how to configure and implement MTL algorithms:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/mtl-algorithm\"><strong>Configure MTL Algorithm</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/mtl-algorithm\"><strong>Implement Custom MTL Algorithm</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm", "kind": "class", "doc": "<p>The base class of multi-task learning algorithms.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.__init__", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>Backbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsMTL</code>): output heads.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Backbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_mtl</span><span class=\"o\">.</span><span class=\"n\">HeadsMTL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.backbone", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.backbone", "kind": "variable", "doc": "<p>The backbone network.</p>\n", "annotation": ": clarena.backbones.base.Backbone"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.heads", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.heads", "kind": "variable", "doc": "<p>The output heads.</p>\n", "annotation": ": clarena.heads.heads_mtl.HeadsMTL"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.optimizer", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.optimizer", "kind": "variable", "doc": "<p>Optimizer (partially initialized) for the backpropagation. Will be equipped with parameters in <code>configure_optimizers()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.lr_scheduler", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.lr_scheduler", "kind": "variable", "doc": "<p>The learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler | None"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.criterion", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.criterion", "kind": "variable", "doc": "<p>The loss function bewteen the output logits and the target labels. Default is cross-entropy loss.</p>\n"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.if_forward_func_return_logits_only", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.if_forward_func_return_logits_only", "kind": "variable", "doc": "<p>Whether the <code>forward()</code> method returns logits only. If <code>False</code>, it returns a dictionary containing logits and other information. Default is <code>False</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.sanity_check", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.setup_tasks", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.setup_tasks", "kind": "function", "doc": "<p>Set up tasks for the MTL algorithm. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_ids</strong> (<code>list[int]</code>): the list of task IDs.</li>\n<li><strong>num_classes</strong> (<code>dict[int, int]</code>): a dictionary mapping each task ID to its number of classes.</li>\n<li><strong>optimizer</strong> (<code>Optimizer</code>): the optimizer object (partially initialized).</li>\n<li><strong>lr_scheduler</strong> (<code>LRScheduler</code> | None): the learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">task_ids</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">LRScheduler</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.get_val_task_id_from_dataloader_idx", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.get_val_task_id_from_dataloader_idx", "kind": "function", "doc": "<p>Get the validation task ID from the dataloader index.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the dataloader index.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>val_task_id</strong> (<code>int</code>): the validation task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.get_test_task_id_from_dataloader_idx", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.get_test_task_id_from_dataloader_idx", "kind": "function", "doc": "<p>Get the test task ID from the dataloader index.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the dataloader index.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_task_id</strong> (<code>int</code>): the test task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.set_forward_func_return_logits_only", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.set_forward_func_return_logits_only", "kind": "function", "doc": "<p>Set whether the <code>forward()</code> method returns logits only. This is useful for some CL algorithms that require the forward function to return logits only, such as FG-AdaHAT.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>forward_func_return_logits_only</strong> (<code>bool</code>): whether the <code>forward()</code> method returns logits only. If <code>False</code>, it returns a dictionary containing logits and other information.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">forward_func_return_logits_only</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.forward", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.forward", "kind": "function", "doc": "<p>The forward pass for data. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>. This definition provides a template that many MTL algorithm including the vanilla JointLearning algorithm use.</p>\n\n<p>This forward pass does not accept input batch in different tasks. Please make sure the input batch is from the same task. If you want to use this forward pass for different tasks, please divide the input batch by tasks and call this forward pass for each task separately.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>task_ids</strong> (<code>int</code> | <code>Tensor</code>): the task ID(s) for the input data. If the input batch is from the same task, this can be a single integer.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">task_ids</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.MTLAlgorithm.configure_optimizers", "modulename": "clarena.mtl_algorithms", "qualname": "MTLAlgorithm.configure_optimizers", "kind": "function", "doc": "<p>Configure optimizer hooks by Lightning. See <a href=\"https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers\">Lightning docs</a> for more details.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.joint_learning", "modulename": "clarena.mtl_algorithms.joint_learning", "kind": "module", "doc": "<p>The submodule in <code>mtl_algorithms</code> for joint learning algorithm.</p>\n"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning", "kind": "class", "doc": "<p>Joint learning algorithm.</p>\n\n<p>The most naive way for multi-task learning. It directly trains all tasks.</p>\n", "bases": "clarena.mtl_algorithms.base.MTLAlgorithm"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning.__init__", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning.__init__", "kind": "function", "doc": "<p>Initialize the JointLearning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>Backbone</code>): backbone network.</li>\n<li><strong>heads</strong> (<code>HeadsMTL</code>): output heads.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Backbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_mtl</span><span class=\"o\">.</span><span class=\"n\">HeadsMTL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning.training_step", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning.training_step", "kind": "function", "doc": "<p>Training step.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data, which can be from any mixed tasks. Must include task IDs in the batch.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning.validation_step", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning.validation_step", "kind": "function", "doc": "<p>Validation step. This is done task by task rather than mixing the tasks in batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be validated. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this validation step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_algorithms.joint_learning.JointLearning.test_step", "modulename": "clarena.mtl_algorithms.joint_learning", "qualname": "JointLearning.test_step", "kind": "function", "doc": "<p>Test step. This is done task by task rather than mixing the tasks in batches.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets", "modulename": "clarena.mtl_datasets", "kind": "module", "doc": "<h1 id=\"multi-task-learning-datasets\">Multi-Task Learning Datasets</h1>\n\n<p>This submodule provides the <strong>multi-task learning datasets</strong> that can be used in CLArena.</p>\n\n<p>Here are the base classes for multi-task learning datasets, which inherit from Lightning <code>LightningDataModule</code>:</p>\n\n<ul>\n<li><code>MTLDataset</code>: The base class for all multi-task learning datasets.\n<ul>\n<li><code>MTLCombinedDataset</code>: The base class for combined multi-task learning datasets. A child class of <code>MTLDataset</code>.</li>\n<li><code>MTLDatasetFromCL</code>: The base class for constructing multi-task learning datasets from continual learning datasets. A child class of <code>MTLDataset</code>.</li>\n</ul></li>\n</ul>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about how to configure and implement MTL datasets:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/mtl-dataset\"><strong>Configure MTL Dataset</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/mtl_dataset\"><strong>Implement Custom MTL Dataset</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.mtl_datasets.MTLDataset", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset", "kind": "class", "doc": "<p>The base class of multi-task learning datasets.</p>\n", "bases": "lightning.pytorch.core.datamodule.LightningDataModule"}, {"fullname": "clarena.mtl_datasets.MTLDataset.__init__", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code> | <code>list[str]</code>): the root directory where the original data files for constructing the MTL dataset physically live. If <code>list[str]</code>, it should be a list of strings, each string is the root directory for each task.</li>\n<li><strong>num_tasks</strong> (<code>int</code>): the maximum number of tasks supported by the MTL dataset.</li>\n<li><strong>sampling_strategy</strong> (<code>str</code>): the sampling strategy that construct training batch from each task's dataset; one of:\n<ul>\n<li>'mixed': mixed sampling strategy, which samples from all tasks' datasets.</li>\n</ul></li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize.\nIf it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">num_tasks</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">sampling_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mixed&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.mtl_datasets.MTLDataset.root", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.root", "kind": "variable", "doc": "<p>The dict of root directories of the original data files for each task.</p>\n", "annotation": ": dict[int, str]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.num_tasks", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.num_tasks", "kind": "variable", "doc": "<p>The maximum number of tasks supported by the dataset.</p>\n", "annotation": ": int"}, {"fullname": "clarena.mtl_datasets.MTLDataset.sampling_strategy", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.sampling_strategy", "kind": "variable", "doc": "<p>The sampling strategy for constructing training batch from each task's dataset.</p>\n", "annotation": ": str"}, {"fullname": "clarena.mtl_datasets.MTLDataset.batch_size", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.batch_size", "kind": "variable", "doc": "<p>The batch size for dataloaders.</p>\n", "annotation": ": int"}, {"fullname": "clarena.mtl_datasets.MTLDataset.num_workers", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.num_workers", "kind": "variable", "doc": "<p>The number of workers for dataloaders.</p>\n", "annotation": ": int"}, {"fullname": "clarena.mtl_datasets.MTLDataset.custom_transforms", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.custom_transforms", "kind": "variable", "doc": "<p>The dict of custom transforms for each task.</p>\n", "annotation": ": dict[int, typing.Union[typing.Callable, torchvision.transforms.transforms.Compose, NoneType]]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.repeat_channels", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.repeat_channels", "kind": "variable", "doc": "<p>The dict of number of channels to repeat for each task.</p>\n", "annotation": ": dict[int, int | None]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.to_tensor", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.to_tensor", "kind": "variable", "doc": "<p>The dict of to_tensor flag for each task.</p>\n", "annotation": ": dict[int, bool]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.resize", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.resize", "kind": "variable", "doc": "<p>The dict of sizes to resize to for each task.</p>\n", "annotation": ": dict[int, tuple[int, int] | None]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.dataset_train", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.dataset_train", "kind": "variable", "doc": "<p>The dictionary to store training dataset object of each task. Keys are task IDs and values are the dataset objects. Can be PyTorch Dataset objects or any other dataset objects. </p>\n\n<p>Note that they must be task labelled, i.e., the elements in the dataset objects must be tuples of (input, target, task_id). Use <code>TaskLabelledDataset</code> wrapper if necessary.</p>\n", "annotation": ": dict[int, typing.Any]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.dataset_val", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.dataset_val", "kind": "variable", "doc": "<p>The dictionary to store validation dataset object of each task. Keys are task IDs and values are the dataset objects. Can be PyTorch Dataset objects or any other dataset objects.</p>\n\n<p>Note that they must be task labelled, i.e., the elements in the dataset objects must be tuples of (input, target, task_id). Use <code>TaskLabelledDataset</code> wrapper if necessary.</p>\n", "annotation": ": dict[int, typing.Any]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.dataset_test", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.dataset_test", "kind": "variable", "doc": "<p>The dictionary to store test dataset object of each task. Keys are task IDs and values are the dataset objects. Can be PyTorch Dataset objects or any other dataset objects.</p>\n\n<p>Note that they must be task labelled, i.e., the elements in the dataset objects must be tuples of (input, target, task_id). Use <code>TaskLabelledDataset</code> wrapper if necessary.</p>\n", "annotation": ": dict[int, typing.Any]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.mean", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.mean", "kind": "variable", "doc": "<p>Tthe list of mean values for normalization for all tasks. Used when constructing the transforms.</p>\n", "annotation": ": dict[int, float]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.std", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.std", "kind": "variable", "doc": "<p>The list of standard deviation values for normalization for all tasks. Used when constructing the transforms.</p>\n", "annotation": ": dict[int, float]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.train_tasks", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.train_tasks", "kind": "variable", "doc": "<p>\"The list of task IDs to be trained. It should be a list of integers, each integer is the task ID.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.eval_tasks", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.eval_tasks", "kind": "variable", "doc": "<p>The list of task IDs to be evaluated. It should be a list of integers, each integer is the task ID.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.mtl_datasets.MTLDataset.sanity_check", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.get_mtl_class_map", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.get_mtl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit multi-task learning. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The task ID to query class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>class_map</strong>(<code>dict[str | int, int]</code>): the class map of the task. Keys are original class labels and values are integer class labels for multi-task learning. The mapped class labels of each task should be continuous integers from 0 to the number of classes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.prepare_data", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.prepare_data", "kind": "function", "doc": "<p>Use this to download and prepare data. It must be implemented by subclasses, as required by <code>LightningDatamodule</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.setup", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.setup", "kind": "function", "doc": "<p>Set up the dataset for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage of the experiment; one of:\n<ul>\n<li>'fit': training and validation dataset should be assigned to <code>self.dataset_train</code> and <code>self.dataset_val</code>.</li>\n<li>'test': test dataset should be assigned to <code>self.dataset_test</code>.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.setup_tasks_expr", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.setup_tasks_expr", "kind": "function", "doc": "<p>Set up tasks for the multi-task learning experiment.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>train_tasks</strong> (<code>list[int]</code>): the list of task IDs to be trained. It should be a list of integers, each integer is the task ID. This is used when constructing the train/val dataloader.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated. It should be a list of integers, each integer is the task ID. This is used when constructing the test dataloader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">train_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.setup_tasks_eval", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.setup_tasks_eval", "kind": "function", "doc": "<p>Set up evaluation tasks for the multi-task learning evaluation.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.train_and_val_transforms", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms for train and validation datasets of task <code>task_id</code>, incorporating the custom transforms with basic transforms like <code>normalization</code> and <code>ToTensor()</code>. It can be used in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID of training and validation dataset to get the transforms for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed train/val transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.test_transforms", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.test_transforms", "kind": "function", "doc": "<p>Transforms for test dataset of task <code>task_id</code>. Only basic transforms like <code>normalization</code> and <code>ToTensor()</code> are included. It can be used in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID of test dataset to get the transforms for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed test transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.target_transform", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.target_transform", "kind": "function", "doc": "<p>Target transform for task <code>task_id</code>, which maps the original class labels to the integer class labels for multi-task learning. It can be used in subclasses when constructing the dataset.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID of dataset to get the target transform for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>target_transform</strong> (<code>Callable</code>): the target transform function.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.train_and_val_dataset", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to get the training and validation dataset for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Any, Any]</code>): the train and validation dataset of task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.test_dataset", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>task_id</code>. It must be implemented by subclasses.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to get the test dataset for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Any</code>): the test dataset of task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.train_dataloader", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.train_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage train. It is automatically called before training.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_dataloader</strong> (<code>DataLoader</code>): the train DataLoader of task.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.val_dataloader", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.val_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the validation stage. It is automatically called before validation.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>val_dataloader</strong> (<code>dict[int, DataLoader]</code>): the validation DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDataset.test_dataloader", "modulename": "clarena.mtl_datasets", "qualname": "MTLDataset.test_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage test. It is automatically called before testing.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataloader</strong> (<code>dict[int, DataLoader]</code>): the test DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLCombinedDataset", "modulename": "clarena.mtl_datasets", "qualname": "MTLCombinedDataset", "kind": "class", "doc": "<p>The base class of multi-task learning datasets constructed as combinations of several single-task datasets (one dataset per task).</p>\n", "bases": "clarena.mtl_datasets.base.MTLDataset"}, {"fullname": "clarena.mtl_datasets.MTLCombinedDataset.__init__", "modulename": "clarena.mtl_datasets", "qualname": "MTLCombinedDataset.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>datasets</strong> (<code>dict[int, str]</code>): the dict of dataset class paths for each task. The keys are task IDs and the values are the dataset class paths (as strings) to use for each task.</li>\n<li><strong>root</strong> (<code>str</code> | <code>dict[int, str]</code>): the root directory where the original data files for constructing the MTL dataset physically live. If <code>dict[int, str]</code>, it should be a dict of task IDs and their corresponding root directories.</li>\n<li><strong>sampling_strategy</strong> (<code>str</code>): the sampling strategy that construct training batch from each task's dataset; one of:\n<ul>\n<li>'mixed': mixed sampling strategy, which samples from all tasks' datasets.</li>\n</ul></li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization, and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize. If it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">datasets</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">sampling_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mixed&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.mtl_datasets.MTLCombinedDataset.original_dataset_python_classes", "modulename": "clarena.mtl_datasets", "qualname": "MTLCombinedDataset.original_dataset_python_classes", "kind": "variable", "doc": "<p>The dict of dataset classes for each task.</p>\n", "annotation": ": dict[int, torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.mtl_datasets.MTLCombinedDataset.get_mtl_class_map", "modulename": "clarena.mtl_datasets", "qualname": "MTLCombinedDataset.get_mtl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit multi-task learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to query the class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>class_map</strong> (<code>dict[str | int, int]</code>): the class map of the task. Keys are the original class label and values are the integer class labels for multi-task learning. For multi-task learning, the mapped class labels of a task should be continuous integers from 0 to the number of classes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLCombinedDataset.setup_tasks_expr", "modulename": "clarena.mtl_datasets", "qualname": "MTLCombinedDataset.setup_tasks_expr", "kind": "function", "doc": "<p>Set up tasks for the multi-task learning experiment.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>train_tasks</strong> (<code>list[int]</code>): the list of task IDs to be trained. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">train_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLCombinedDataset.setup_tasks_eval", "modulename": "clarena.mtl_datasets", "qualname": "MTLCombinedDataset.setup_tasks_eval", "kind": "function", "doc": "<p>Set up evaluation tasks for the multi-task learning evaluation.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDatasetFromCL", "modulename": "clarena.mtl_datasets", "qualname": "MTLDatasetFromCL", "kind": "class", "doc": "<p>Multi-task learning datasets constructed from the CL datasets.</p>\n\n<p>This is usually for constructing the reference joint learning experiment for continual learning.</p>\n", "bases": "clarena.mtl_datasets.base.MTLDataset"}, {"fullname": "clarena.mtl_datasets.MTLDatasetFromCL.__init__", "modulename": "clarena.mtl_datasets", "qualname": "MTLDatasetFromCL.__init__", "kind": "function", "doc": "<p>Initialize the <code>MTLDatasetFromCL</code> object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_dataset</strong> (<code>CLDataset</code>): the CL dataset object to be used for constructing the MTL dataset.</li>\n<li><strong>sampling_strategy</strong> (<code>str</code>): the sampling strategy that construct training batch from each task's dataset; one of:\n<ul>\n<li>'mixed': mixed sampling strategy, which samples from all tasks' datasets.</li>\n</ul></li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">cl_dataset</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_datasets</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLDataset</span>,</span><span class=\"param\">\t<span class=\"n\">sampling_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mixed&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span>)</span>"}, {"fullname": "clarena.mtl_datasets.MTLDatasetFromCL.cl_dataset", "modulename": "clarena.mtl_datasets", "qualname": "MTLDatasetFromCL.cl_dataset", "kind": "variable", "doc": "<p>The CL dataset for constructing the MTL dataset.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.mtl_datasets.MTLDatasetFromCL.prepare_data", "modulename": "clarena.mtl_datasets", "qualname": "MTLDatasetFromCL.prepare_data", "kind": "function", "doc": "<p>Download and prepare data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDatasetFromCL.setup", "modulename": "clarena.mtl_datasets", "qualname": "MTLDatasetFromCL.setup", "kind": "function", "doc": "<p>Set up the dataset for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage of the experiment; one of:\n<ul>\n<li>'fit': training and validation dataset should be assigned to <code>self.dataset_train</code> and <code>self.dataset_val</code>.</li>\n<li>'test': test dataset should be assigned to <code>self.dataset_test</code>.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDatasetFromCL.get_mtl_class_map", "modulename": "clarena.mtl_datasets", "qualname": "MTLDatasetFromCL.get_mtl_class_map", "kind": "function", "doc": "<p>Get the mapping of classes of task <code>task_id</code> to fit multi-task learning.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): The task ID to query class map.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>class_map</strong>(<code>dict[str | int, int]</code>): the class map of the task. Keys are original class labels and values are integer class labels for multi-task learning. The mapped class labels of each task should be continuous integers from 0 to the number of classes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDatasetFromCL.setup_tasks_expr", "modulename": "clarena.mtl_datasets", "qualname": "MTLDatasetFromCL.setup_tasks_expr", "kind": "function", "doc": "<p>Set up tasks for the multi-task learning experiment.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>train_tasks</strong> (<code>list[int]</code>): the list of task IDs to be trained. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated. It should be a list of integers, each integer is the task ID. This is used when constructing the dataloader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">train_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.MTLDatasetFromCL.setup_tasks_eval", "modulename": "clarena.mtl_datasets", "qualname": "MTLDatasetFromCL.setup_tasks_eval", "kind": "function", "doc": "<p>Set up evaluation tasks for the multi-task learning evaluation.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of task IDs to be evaluated.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.combined", "modulename": "clarena.mtl_datasets.combined", "kind": "module", "doc": "<p>The submodule in <code>mtl_datasets</code> for CelebA dataset used for multi-task learning.</p>\n"}, {"fullname": "clarena.mtl_datasets.combined.Combined", "modulename": "clarena.mtl_datasets.combined", "qualname": "Combined", "kind": "class", "doc": "<p>Combined MTL dataset from available datasets.</p>\n", "bases": "clarena.mtl_datasets.base.MTLCombinedDataset"}, {"fullname": "clarena.mtl_datasets.combined.Combined.__init__", "modulename": "clarena.mtl_datasets.combined", "qualname": "Combined.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>datasets</strong> (<code>dict[int, str]</code>): the dict of dataset class paths for each task. The keys are task IDs and the values are the dataset class paths (as strings) to use for each task.</li>\n<li><strong>root</strong> (<code>str</code> | <code>dict[int, str]</code>): the root directory where the original data files for constructing the MTL dataset physically live. If <code>dict[int, str]</code>, it should be a dict of task IDs and their corresponding root directories.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some of the training data into validation data (only if validation set is not provided in the dataset).</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some of the entire data into test data (only if test set is not provided in the dataset).</li>\n<li><strong>sampling_strategy</strong> (<code>str</code>): the sampling strategy that construct training batch from each task's dataset; one of:\n<ul>\n<li>'mixed': mixed sampling strategy, which samples from all tasks' datasets.</li>\n</ul></li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code> or dict of them): the custom transforms to apply ONLY to the TRAIN dataset. Can be a single transform, composed transforms, or no transform. <code>ToTensor()</code>, normalization and so on are not included.\nIf it is a dict, the keys are task IDs and the values are the custom transforms for each task. If it is a single transform or composed transforms, it is applied to all tasks. If it is <code>None</code>, no custom transforms are applied.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code> | dict of them): the number of channels to repeat for each task. Default is <code>None</code>, which means no repeat.\nIf it is a dict, the keys are task IDs and the values are the number of channels to repeat for each task. If it is an <code>int</code>, it is the same number of channels to repeat for all tasks. If it is <code>None</code>, no repeat is applied.</li>\n<li><strong>to_tensor</strong> (<code>bool</code> | <code>dict[int, bool]</code>): whether to include the <code>ToTensor()</code> transform. Default is <code>True</code>.\nIf it is a dict, the keys are task IDs and the values are whether to include the <code>ToTensor()</code> transform for each task. If it is a single boolean value, it is applied to all tasks.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or dict of them): the size to resize the images to. Default is <code>None</code>, which means no resize. If it is a dict, the keys are task IDs and the values are the sizes to resize for each task. If it is a single tuple of two integers, it is applied to all tasks. If it is <code>None</code>, no resize is applied.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">datasets</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">sampling_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mixed&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">|</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.mtl_datasets.combined.Combined.AVAILABLE_DATASETS", "modulename": "clarena.mtl_datasets.combined", "qualname": "Combined.AVAILABLE_DATASETS", "kind": "variable", "doc": "<p>The list of available datasets.</p>\n", "annotation": ": list[torchvision.datasets.vision.VisionDataset]", "default_value": "[&lt;class &#x27;clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits&#x27;&gt;, &lt;class &#x27;torchvision.datasets.cifar.CIFAR10&#x27;&gt;, &lt;class &#x27;torchvision.datasets.cifar.CIFAR100&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.cub2002011.CUB2002011&#x27;&gt;, &lt;class &#x27;torchvision.datasets.caltech.Caltech101&#x27;&gt;, &lt;class &#x27;torchvision.datasets.caltech.Caltech256&#x27;&gt;, &lt;class &#x27;torchvision.datasets.celeba.CelebA&#x27;&gt;, &lt;class &#x27;torchvision.datasets.country211.Country211&#x27;&gt;, &lt;class &#x27;torchvision.datasets.dtd.DTD&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTBalanced&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTByClass&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTByMerge&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTDigits&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.emnist.EMNISTLetters&#x27;&gt;, &lt;class &#x27;torchvision.datasets.eurosat.EuroSAT&#x27;&gt;, &lt;class &#x27;torchvision.datasets.fer2013.FER2013&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftFamily&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftManufacturer&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.fgvc_aircraft.FGVCAircraftVariant&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub10&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub100&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrubFromHAT&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub20&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.facescrub.FaceScrub50&#x27;&gt;, &lt;class &#x27;torchvision.datasets.mnist.FashionMNIST&#x27;&gt;, &lt;class &#x27;torchvision.datasets.flowers102.Flowers102&#x27;&gt;, &lt;class &#x27;torchvision.datasets.food101.Food101&#x27;&gt;, &lt;class &#x27;torchvision.datasets.gtsrb.GTSRB&#x27;&gt;, &lt;class &#x27;torchvision.datasets.mnist.KMNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_128&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_256&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_32&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.linnaeus5.Linnaeus5_64&#x27;&gt;, &lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.notmnist.NotMNIST&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.notmnist.NotMNISTFromHAT&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet2&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.oxford_iiit_pet.OxfordIIITPet37&#x27;&gt;, &lt;class &#x27;torchvision.datasets.pcam.PCAM&#x27;&gt;, &lt;class &#x27;torchvision.datasets.rendered_sst2.RenderedSST2&#x27;&gt;, &lt;class &#x27;torchvision.datasets.semeion.SEMEION&#x27;&gt;, &lt;class &#x27;torchvision.datasets.sun397.SUN397&#x27;&gt;, &lt;class &#x27;torchvision.datasets.svhn.SVHN&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST&#x27;&gt;, &lt;class &#x27;torchvision.datasets.stanford_cars.StanfordCars&#x27;&gt;, &lt;class &#x27;clarena.stl_datasets.raw.traffic_signs.TrafficSignsFromHAT&#x27;&gt;, &lt;class &#x27;tinyimagenet.TinyImageNet&#x27;&gt;, &lt;class &#x27;torchvision.datasets.usps.USPS&#x27;&gt;]"}, {"fullname": "clarena.mtl_datasets.combined.Combined.test_percentage", "modulename": "clarena.mtl_datasets.combined", "qualname": "Combined.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.mtl_datasets.combined.Combined.validation_percentage", "modulename": "clarena.mtl_datasets.combined", "qualname": "Combined.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.mtl_datasets.combined.Combined.prepare_data", "modulename": "clarena.mtl_datasets.combined", "qualname": "Combined.prepare_data", "kind": "function", "doc": "<p>Download the original datasets if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.combined.Combined.train_and_val_dataset", "modulename": "clarena.mtl_datasets.combined", "qualname": "Combined.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset of task <code>task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to get the training and validation dataset for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset of task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.mtl_datasets.combined.Combined.test_dataset", "modulename": "clarena.mtl_datasets.combined", "qualname": "Combined.test_dataset", "kind": "function", "doc": "<p>Get the test dataset of task <code>task_id</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to get the test dataset for.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset of task <code>task_id</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines", "modulename": "clarena.pipelines", "kind": "module", "doc": "<h1 id=\"experiment-and-evaluation-pipelines\">Experiment and Evaluation Pipelines</h1>\n\n<p>This submodule provides <strong>experiment and evaluation pipeline classes</strong> that manage the index configs and process of experiments and evaluations.</p>\n\n<p>We provide experiment pipeline classes:</p>\n\n<ul>\n<li><code>CLMainExperiment</code>: Continual Learning Main Experiment.</li>\n<li><code>CULMainExperiment</code>: Continual Unlearning Main Experiment.</li>\n<li><code>MTLExperiment</code>: Multi-Task Learning Experiment.</li>\n<li><code>STLExperiment</code>: Single-Task Learning Experiment.</li>\n</ul>\n\n<p>And evaluation pipeline classes:</p>\n\n<ul>\n<li><code>CLMainEvaluation</code>: Continual Learning Main Evaluation.</li>\n<li><code>CLFullEvaluation</code>: Continual Learning Full Evaluation.</li>\n<li><code>CULFullEvaluation</code>: Continual Unlearning Full Evaluation.</li>\n<li><code>MTLEvaluation</code>: Multi-Task Learning Evaluation.</li>\n<li><code>STLEvaluation</code>: Single-Task Learning Evaluation.</li>\n</ul>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation for more information about experiments and evaluations:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/experiments-and-evaluations\"><strong>Experiments and Evaluations</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.pipelines.cl_full_eval", "modulename": "clarena.pipelines.cl_full_eval", "kind": "module", "doc": "<p>The submodule in <code>pipelines</code> for continual learning full evaluation.</p>\n"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation", "kind": "class", "doc": "<p>The base class for continual learning full evaluation.</p>\n"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.__init__", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the config dict for the continual learning full evaluation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.cfg", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.eval_tasks", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.eval_tasks", "kind": "variable", "doc": "<p>The list of task IDs to evaluate.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.main_acc_csv_path", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.main_acc_csv_path", "kind": "variable", "doc": "<p>The path to the main experiment accuracy CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.output_dir", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.output_dir", "kind": "variable", "doc": "<p>The folder for storing the evaluation results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.bwt_save_dir", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.bwt_save_dir", "kind": "variable", "doc": "<p>The folder storing the BWT metric results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.bwt_csv_path", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.bwt_csv_path", "kind": "variable", "doc": "<p>The file path to store the BWT metrics as CSV file.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.sanity_check", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.sanity_check", "kind": "function", "doc": "<p>Sanity check for config.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.run", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.run", "kind": "function", "doc": "<p>The main method to run the continual learning full evaluation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.evaluate_and_save_bwt_to_csv", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.evaluate_and_save_bwt_to_csv", "kind": "function", "doc": "<p>Evaluate the backward transfer (BWT) from the main experiment accuracy CSV file and save it to a CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>main_acc_csv_path</strong> (<code>str</code>): the path to the main experiment accuracy CSV file.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of tasks to evaluate BWT.</li>\n<li><strong>save_path</strong> (<code>str</code>): the path to save the BWT CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">main_acc_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.evaluate_and_save_fwt_to_csv", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.evaluate_and_save_fwt_to_csv", "kind": "function", "doc": "<p>Evaluate the forward transfer (FWT) from the main experiment accuracy CSV file and reference independenet learning experiment accuracy CSV file, and save it to a CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>main_acc_csv_path</strong> (<code>str</code>): the path to the main experiment accuracy CSV file.</li>\n<li><strong>refindependent_acc_csv_path</strong> (<code>str</code>): the path to the reference independent learning accuracy CSV file.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of tasks to evaluate FWT.</li>\n<li><strong>save_path</strong> (<code>str</code>): the path to save the FWT CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">main_acc_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">refindependent_acc_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.evaluate_and_save_fr_to_csv", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.evaluate_and_save_fr_to_csv", "kind": "function", "doc": "<p>evaluate the forgetting rate (FR) from the main experiment accuracy CSV file and save it to a CSV file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>main_acc_csv_path</strong> (<code>str</code>): the path to the main experiment accuracy CSV file.</li>\n<li><strong>refjoint_acc_csv_path</strong> (<code>str</code>): the path to the reference joint learning accuracy CSV file.</li>\n<li><strong>refrandom_acc_csv_path</strong> (<code>str</code>): the path to the reference random learning experiment accuracy CSV file.</li>\n<li><strong>eval_tasks</strong> (<code>list[int]</code>): the list of tasks to evaluate FR.</li>\n<li><strong>save_path</strong> (<code>str</code>): the path to save the FR CSV file.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">main_acc_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">refjoint_acc_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">refrandom_acc_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">eval_tasks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.plot_bwt_curve_from_csv", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.plot_bwt_curve_from_csv", "kind": "function", "doc": "<p>Plot the backward transfer (BWT) barchart from saved CSV file and save the plot.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>bwt_csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>evaluate_and_save_bwt_to_csv()</code> method saved the BWT metric.</li>\n<li><strong>save_path</strong> (<code>str</code>): the path to save plot.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">bwt_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_full_eval.CLFullEvaluation.plot_fwt_curve_from_csv", "modulename": "clarena.pipelines.cl_full_eval", "qualname": "CLFullEvaluation.plot_fwt_curve_from_csv", "kind": "function", "doc": "<p>Plot the forward transfer (FWT) barchart from saved CSV file and save the plot.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>bwt_csv_path</strong> (<code>str</code>): the path to the CSV file where the <code>evaluate_and_save_fwt_to_csv()</code> method saved the FWT metric.</li>\n<li><strong>save_path</strong> (<code>str</code>): the path to save plot.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">fwt_csv_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_eval", "modulename": "clarena.pipelines.cl_main_eval", "kind": "module", "doc": "<p>The submodule in <code>pipelines</code> for continual learning main evaluation.</p>\n"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation", "kind": "class", "doc": "<p>The base class for continual learning main evaluation.</p>\n"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.__init__", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the config dict for the continual learning main evaluation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.cfg", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.main_model_path", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.main_model_path", "kind": "variable", "doc": "<p>The file path of the model to evaluate.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.cl_paradigm", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.cl_paradigm", "kind": "variable", "doc": "<p>The continual learning paradigm.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.eval_tasks", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.eval_tasks", "kind": "variable", "doc": "<p>The list of task IDs to evaluate.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.global_seed", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.output_dir", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.output_dir", "kind": "variable", "doc": "<p>The folder for storing the experiment results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.cl_dataset", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.cl_dataset", "kind": "variable", "doc": "<p>CL dataset object.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.callbacks", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.callbacks", "kind": "variable", "doc": "<p>Callback objects.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.trainer", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.trainer", "kind": "variable", "doc": "<p>Trainer object.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.sanity_check", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.sanity_check", "kind": "function", "doc": "<p>Sanity check for config.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.instantiate_cl_dataset", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.instantiate_cl_dataset", "kind": "function", "doc": "<p>Instantiate the CL dataset object from <code>cl_dataset_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.instantiate_callbacks", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from <code>metrics_cfg</code> and <code>callbacks_cfg</code>. Note that <code>metrics_cfg</code> is a list of metric callbacks and <code>callbacks_cfg</code> is a list of callbacks other the metric callbacks. The instantiated callbacks contain both metric callbacks and other callbacks.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.instantiate_trainer", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from <code>trainer_cfg</code> and <code>callbacks</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">callback</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.set_global_seed", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.set_global_seed", "kind": "function", "doc": "<p>Set the <code>global_seed</code> for the entire evaluation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">global_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_eval.CLMainEvaluation.run", "modulename": "clarena.pipelines.cl_main_eval", "qualname": "CLMainEvaluation.run", "kind": "function", "doc": "<p>The main method to run the continual learning main evaluation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr", "modulename": "clarena.pipelines.cl_main_expr", "kind": "module", "doc": "<p>The submodule in <code>pipelines</code> for continual learning main experiment.</p>\n"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment", "kind": "class", "doc": "<p>The base class for continual learning main experiment.</p>\n"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.__init__", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the continual learning main experiment.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.cfg", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.cl_paradigm", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.cl_paradigm", "kind": "variable", "doc": "<p>The continual learning paradigm.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.train_tasks", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.train_tasks", "kind": "variable", "doc": "<p>The list of task IDs to train.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.eval_after_tasks", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.eval_after_tasks", "kind": "variable", "doc": "<p>If task ID $t$ is in this list, run the evaluation process for all seen tasks after training task $t$.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.global_seed", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.output_dir", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.output_dir", "kind": "variable", "doc": "<p>The folder for storing the experiment results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.cl_dataset", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.cl_dataset", "kind": "variable", "doc": "<p>CL dataset object.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.backbone", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.backbone", "kind": "variable", "doc": "<p>Backbone network object.</p>\n", "annotation": ": clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.heads", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.heads", "kind": "variable", "doc": "<p>CL output heads object.</p>\n", "annotation": ": clarena.heads.heads_til.HeadsTIL | clarena.heads.heads_cil.HeadsCIL"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.model", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.model", "kind": "variable", "doc": "<p>CL model object.</p>\n", "annotation": ": clarena.cl_algorithms.base.CLAlgorithm"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.lightning_loggers", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.lightning_loggers", "kind": "variable", "doc": "<p>Lightning logger objects.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.callbacks", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.callbacks", "kind": "variable", "doc": "<p>Callback objects.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.optimizer_t", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.optimizer_t", "kind": "variable", "doc": "<p>Optimizer object for the current task <code>self.task_id</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.lr_scheduler_t", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.lr_scheduler_t", "kind": "variable", "doc": "<p>Learning rate scheduler object for the current task <code>self.task_id</code>.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler | None"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.trainer_t", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.trainer_t", "kind": "variable", "doc": "<p>Trainer object for the current task <code>self.task_id</code>.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.task_id", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.task_id", "kind": "variable", "doc": "<p>Task ID counter indicating which task is being processed. Self updated during the task loop. Valid from 1 to the number of tasks in the CL dataset.</p>\n", "annotation": ": int"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.processed_task_ids", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.processed_task_ids", "kind": "variable", "doc": "<p>Task IDs that have been processed.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.sanity_check", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.sanity_check", "kind": "function", "doc": "<p>Sanity check for config.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.instantiate_cl_dataset", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.instantiate_cl_dataset", "kind": "function", "doc": "<p>Instantiate the CL dataset object from <code>cl_dataset_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.instantiate_backbone", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.instantiate_backbone", "kind": "function", "doc": "<p>Instantiate the CL backbone network object from <code>backbone_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">backbone_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.instantiate_heads", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.instantiate_heads", "kind": "function", "doc": "<p>Instantiate the CL output heads object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_paradigm</strong> (<code>str</code>): the CL paradigm, either 'TIL' or 'CIL'. 'TIL' uses <code>HeadsTIL</code>, while 'CIL' uses <code>HeadsCIL</code>.</li>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_paradigm</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.instantiate_cl_algorithm", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.instantiate_cl_algorithm", "kind": "function", "doc": "<p>Instantiate the cl_algorithm object from <code>cl_algorithm_cfg</code>, <code>backbone</code>, <code>heads</code> and <code>non_algorithmic_hparams</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">cl_algorithm_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLBackbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_til</span><span class=\"o\">.</span><span class=\"n\">HeadsTIL</span> <span class=\"o\">|</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_cil</span><span class=\"o\">.</span><span class=\"n\">HeadsCIL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.instantiate_optimizer", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.instantiate_optimizer", "kind": "function", "doc": "<p>Instantiate the optimizer object for task <code>task_id</code> from <code>optimizer_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.instantiate_lr_scheduler", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.instantiate_lr_scheduler", "kind": "function", "doc": "<p>Instantiate the learning rate scheduler object for task <code>task_id</code> from <code>lr_scheduler_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.instantiate_lightning_loggers", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects from <code>lightning_loggers_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.instantiate_callbacks", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from <code>metrics_cfg</code> and <code>callbacks_cfg</code>. Note that <code>metrics_cfg</code> is a list of metric callbacks and <code>callbacks_cfg</code> is a list of callbacks other the metric callbacks. The instantiated callbacks contain both metric callbacks and other callbacks.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">listconfig</span><span class=\"o\">.</span><span class=\"n\">ListConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">listconfig</span><span class=\"o\">.</span><span class=\"n\">ListConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.instantiate_trainer", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object for task <code>task_id</code> from <code>trainer_cfg</code>, <code>lightning_loggers</code>, and <code>callbacks</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">lightning_loggers</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">loggers</span><span class=\"o\">.</span><span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">Logger</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">callback</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.set_global_seed", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.set_global_seed", "kind": "function", "doc": "<p>Set the <code>global_seed</code> for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">global_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cl_main_expr.CLMainExperiment.run", "modulename": "clarena.pipelines.cl_main_expr", "qualname": "CLMainExperiment.run", "kind": "function", "doc": "<p>The main method to run the continual learning main experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_full_eval", "modulename": "clarena.pipelines.cul_full_eval", "kind": "module", "doc": "<p>The submodule in <code>pipelines</code> for continual unlearning full evaluation.</p>\n"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation", "kind": "class", "doc": "<p>The base class for continual unlearning full evaluation.</p>\n"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.__init__", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the continual unlearning main evaluation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.cfg", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.main_model_path", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.main_model_path", "kind": "variable", "doc": "<p>The path to the model file to load the main model from.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.refretrain_model_path", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.refretrain_model_path", "kind": "variable", "doc": "<p>The path to the model file to load the reference retrain model from.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.reforiginal_model_path", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.reforiginal_model_path", "kind": "variable", "doc": "<p>The path to the model file to load the reference original model from.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.cl_paradigm", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.cl_paradigm", "kind": "variable", "doc": "<p>The continual learning paradigm.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.dd_eval_tasks", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.dd_eval_tasks", "kind": "variable", "doc": "<p>The list of tasks to be evaluated for DD.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.ad_eval_tasks", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.ad_eval_tasks", "kind": "variable", "doc": "<p>The list of tasks to be evaluated for AD.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.global_seed", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.output_dir", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.output_dir", "kind": "variable", "doc": "<p>The folder for storing the experiment results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.cl_dataset", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.cl_dataset", "kind": "variable", "doc": "<p>CL dataset object.</p>\n", "annotation": ": clarena.cl_datasets.base.CLDataset"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.evaluation_module", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.evaluation_module", "kind": "variable", "doc": "<p>Evaluation module for continual unlearning full evaluation.</p>\n", "annotation": ": clarena.utils.eval.CULEvaluation"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.lightning_loggers", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.lightning_loggers", "kind": "variable", "doc": "<p>Lightning logger objects.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.callbacks", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.callbacks", "kind": "variable", "doc": "<p>Callback objects.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.trainer", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.trainer", "kind": "variable", "doc": "<p>Trainer object.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.sanity_check", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.sanity_check", "kind": "function", "doc": "<p>Sanity check for config.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.instantiate_cl_dataset", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.instantiate_cl_dataset", "kind": "function", "doc": "<p>Instantiate the CL dataset object from <code>cl_dataset_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.instantiate_evaluation_module", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.instantiate_evaluation_module", "kind": "function", "doc": "<p>Instantiate the evaluation module object.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.instantiate_callbacks", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from <code>metrics_cfg</code> and <code>callbacks_cfg</code>. Note that <code>metrics_cfg</code> is a list of metric callbacks and <code>callbacks_cfg</code> is a list of callbacks other the metric callbacks. The instantiated callbacks contain both metric callbacks and other callbacks.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">listconfig</span><span class=\"o\">.</span><span class=\"n\">ListConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">listconfig</span><span class=\"o\">.</span><span class=\"n\">ListConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.instantiate_trainer", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from <code>trainer_cfg</code> and <code>callbacks</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">callback</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.set_global_seed", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.set_global_seed", "kind": "function", "doc": "<p>Set the <code>global_seed</code> for the entire evaluation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">global_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_full_eval.CULFullEvaluation.run", "modulename": "clarena.pipelines.cul_full_eval", "qualname": "CULFullEvaluation.run", "kind": "function", "doc": "<p>The main method to run the continual unlearning full evaluation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_main_expr", "modulename": "clarena.pipelines.cul_main_expr", "kind": "module", "doc": "<p>The submodule in <code>pipelines</code> for continual unlearning main experiment.</p>\n"}, {"fullname": "clarena.pipelines.cul_main_expr.CULMainExperiment", "modulename": "clarena.pipelines.cul_main_expr", "qualname": "CULMainExperiment", "kind": "class", "doc": "<p>The base class for continual unlearning main experiment.</p>\n", "bases": "clarena.pipelines.cl_main_expr.CLMainExperiment"}, {"fullname": "clarena.pipelines.cul_main_expr.CULMainExperiment.__init__", "modulename": "clarena.pipelines.cul_main_expr", "qualname": "CULMainExperiment.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the CUL experiment.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.pipelines.cul_main_expr.CULMainExperiment.cul_algorithm", "modulename": "clarena.pipelines.cul_main_expr", "qualname": "CULMainExperiment.cul_algorithm", "kind": "variable", "doc": "<p>Continual unlearning algorithm object.</p>\n", "annotation": ": clarena.cul_algorithms.base.CULAlgorithm"}, {"fullname": "clarena.pipelines.cul_main_expr.CULMainExperiment.unlearning_requests", "modulename": "clarena.pipelines.cul_main_expr", "qualname": "CULMainExperiment.unlearning_requests", "kind": "variable", "doc": "<p>The unlearning requests for each task in the experiment. Keys are IDs of the tasks that request unlearning after their learning, and values are the list of the previous tasks to be unlearned. Parsed from config and used in the tasks loop.</p>\n", "annotation": ": dict[int, list[int]]"}, {"fullname": "clarena.pipelines.cul_main_expr.CULMainExperiment.unlearned_task_ids", "modulename": "clarena.pipelines.cul_main_expr", "qualname": "CULMainExperiment.unlearned_task_ids", "kind": "variable", "doc": "<p>The list of task IDs that have been unlearned in the experiment. Updated in the tasks loop when unlearning requests are made.</p>\n", "annotation": ": set[int]"}, {"fullname": "clarena.pipelines.cul_main_expr.CULMainExperiment.permanent_mark", "modulename": "clarena.pipelines.cul_main_expr", "qualname": "CULMainExperiment.permanent_mark", "kind": "variable", "doc": "<p>Whether a task is permanent for each task in the experiment. If a task is permanent, it will not be unlearned i.e. not shown in future unlearning requests. This applies to some unlearning algorithms that need to know whether a task is permanent.</p>\n", "annotation": ": dict[int, bool]"}, {"fullname": "clarena.pipelines.cul_main_expr.CULMainExperiment.sanity_check", "modulename": "clarena.pipelines.cul_main_expr", "qualname": "CULMainExperiment.sanity_check", "kind": "function", "doc": "<p>Check the sanity of the config dict <code>self.cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_main_expr.CULMainExperiment.instantiate_cul_algorithm", "modulename": "clarena.pipelines.cul_main_expr", "qualname": "CULMainExperiment.instantiate_cul_algorithm", "kind": "function", "doc": "<p>Instantiate the CUL algorithm object from <code>cul_algorithm_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">cul_algorithm_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.cul_main_expr.CULMainExperiment.run", "modulename": "clarena.pipelines.cul_main_expr", "qualname": "CULMainExperiment.run", "kind": "function", "doc": "<p>The main method to run the continual unlearning main experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_eval", "modulename": "clarena.pipelines.mtl_eval", "kind": "module", "doc": "<p>The submodule in <code>pipelines</code> for multi-task learning evaluation.</p>\n"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation", "kind": "class", "doc": "<p>The base class for multi-task learning evaluation.</p>\n"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.__init__", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the config dict for the multi-task learning evaluation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.cfg", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.eval_tasks", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.eval_tasks", "kind": "variable", "doc": "<p>The list of task IDs to evaluate.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.global_seed", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.output_dir", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.output_dir", "kind": "variable", "doc": "<p>The folder for storing the experiment results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.model_path", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.model_path", "kind": "variable", "doc": "<p>The file path of the model to evaluate.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.mtl_dataset", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.mtl_dataset", "kind": "variable", "doc": "<p>MTL dataset object.</p>\n", "annotation": ": clarena.mtl_datasets.base.MTLDataset"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.lightning_loggers", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.lightning_loggers", "kind": "variable", "doc": "<p>Lightning logger objects.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.callbacks", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.callbacks", "kind": "variable", "doc": "<p>Callback objects.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.trainer", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.trainer", "kind": "variable", "doc": "<p>Trainer object.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.sanity_check", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.sanity_check", "kind": "function", "doc": "<p>Sanity check for config.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.instantiate_mtl_dataset", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.instantiate_mtl_dataset", "kind": "function", "doc": "<p>Instantiate the MTL dataset object from <code>mtl_dataset_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mtl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.instantiate_callbacks", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from <code>metrics_cfg</code> and <code>callbacks_cfg</code>. Note that <code>metrics_cfg</code> is a list of metric callbacks and <code>callbacks_cfg</code> is a list of callbacks other the metric callbacks. The instantiated callbacks contain both metric callbacks and other callbacks.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.instantiate_trainer", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from <code>trainer_cfg</code> and <code>callbacks</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">callback</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.set_global_seed", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.set_global_seed", "kind": "function", "doc": "<p>Set the <code>global_seed</code> for the entire evaluation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">global_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_eval.MTLEvaluation.run", "modulename": "clarena.pipelines.mtl_eval", "qualname": "MTLEvaluation.run", "kind": "function", "doc": "<p>The main method to run the multi-task learning evaluation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr", "modulename": "clarena.pipelines.mtl_expr", "kind": "module", "doc": "<p>The submodule in <code>pipelines</code> for multi-task learning experiment.</p>\n"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment", "kind": "class", "doc": "<p>The base class for multi-task learning experiment.</p>\n"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.__init__", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the multi-task learning experiment.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.cfg", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.train_tasks", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.train_tasks", "kind": "variable", "doc": "<p>The list of tasks to train.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.eval_tasks", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.eval_tasks", "kind": "variable", "doc": "<p>The list of tasks to evaluate.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.global_seed", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.output_dir", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.output_dir", "kind": "variable", "doc": "<p>The folder for storing the experiment results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.mtl_dataset", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.mtl_dataset", "kind": "variable", "doc": "<p>MTL dataset object.</p>\n", "annotation": ": clarena.mtl_datasets.base.MTLDataset"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.backbone", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.backbone", "kind": "variable", "doc": "<p>Backbone network object.</p>\n", "annotation": ": clarena.backbones.base.CLBackbone"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.heads", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.heads", "kind": "variable", "doc": "<p>MTL output heads object.</p>\n", "annotation": ": clarena.heads.heads_mtl.HeadsMTL"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.model", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.model", "kind": "variable", "doc": "<p>MTL model object.</p>\n", "annotation": ": clarena.mtl_algorithms.base.MTLAlgorithm"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.optimizer", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.optimizer", "kind": "variable", "doc": "<p>Optimizer object.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.lr_scheduler", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.lr_scheduler", "kind": "variable", "doc": "<p>Learning rate scheduler object.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler | None"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.lightning_loggers", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.lightning_loggers", "kind": "variable", "doc": "<p>The list of initialized lightning loggers objects.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.callbacks", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.callbacks", "kind": "variable", "doc": "<p>The list of initialized callbacks objects.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.trainer", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.trainer", "kind": "variable", "doc": "<p>Trainer object.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.sanity_check", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.sanity_check", "kind": "function", "doc": "<p>Sanity check for config.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.instantiate_mtl_dataset", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.instantiate_mtl_dataset", "kind": "function", "doc": "<p>Instantiate the MTL dataset object from <code>mtl_dataset_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">mtl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.instantiate_backbone", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.instantiate_backbone", "kind": "function", "doc": "<p>Instantiate the MTL backbone network object from <code>backbone_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">backbone_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.instantiate_heads", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.instantiate_heads", "kind": "function", "doc": "<p>Instantiate the MTL output heads object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the heads. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.instantiate_mtl_algorithm", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.instantiate_mtl_algorithm", "kind": "function", "doc": "<p>Instantiate the mtl_algorithm object from <code>mtl_algorithm_cfg</code>, <code>backbone</code>, <code>heads</code> and <code>non_algorithmic_hparams</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">mtl_algorithm_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Backbone</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">heads_mtl</span><span class=\"o\">.</span><span class=\"n\">HeadsMTL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.instantiate_optimizer", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.instantiate_optimizer", "kind": "function", "doc": "<p>Instantiate the optimizer object from <code>optimizer_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">optimizer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.instantiate_lr_scheduler", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.instantiate_lr_scheduler", "kind": "function", "doc": "<p>Instantiate the learning rate scheduler object from <code>lr_scheduler_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lr_scheduler_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.instantiate_lightning_loggers", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects from <code>lightning_loggers_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.instantiate_callbacks", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from <code>metrics_cfg</code> and <code>callbacks_cfg</code>. Note that <code>metrics_cfg</code> is a list of metric callbacks and <code>callbacks_cfg</code> is a list of callbacks other the metric callbacks. The instantiated callbacks contain both metric callbacks and other callbacks.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.instantiate_trainer", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from <code>trainer_cfg</code>, <code>lightning_loggers</code>, and <code>callbacks</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">lightning_loggers</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">loggers</span><span class=\"o\">.</span><span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">Logger</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">callback</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.set_global_seed", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.set_global_seed", "kind": "function", "doc": "<p>Set the <code>global_seed</code> for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">global_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.mtl_expr.MTLExperiment.run", "modulename": "clarena.pipelines.mtl_expr", "qualname": "MTLExperiment.run", "kind": "function", "doc": "<p>The main method to run the multi-task learning experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_eval", "modulename": "clarena.pipelines.stl_eval", "kind": "module", "doc": "<p>The submodule in <code>pipelines</code> for single-task learning evaluation.</p>\n"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation", "kind": "class", "doc": "<p>The base class for single-task learning evaluation.</p>\n"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.__init__", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the config dict for the single-task learning evaluation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.cfg", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.global_seed", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.output_dir", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.output_dir", "kind": "variable", "doc": "<p>The folder for storing the experiment results.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.model_path", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.model_path", "kind": "variable", "doc": "<p>The file path of the model to evaluate.</p>\n", "annotation": ": str"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.stl_dataset", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.stl_dataset", "kind": "variable", "doc": "<p>STL dataset object.</p>\n", "annotation": ": clarena.stl_datasets.base.STLDataset"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.lightning_loggers", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.lightning_loggers", "kind": "variable", "doc": "<p>Lightning logger objects.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.callbacks", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.callbacks", "kind": "variable", "doc": "<p>Callback objects.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.trainer", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.trainer", "kind": "variable", "doc": "<p>Trainer object.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.sanity_check", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.sanity_check", "kind": "function", "doc": "<p>Sanity check for config.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.instantiate_stl_dataset", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.instantiate_stl_dataset", "kind": "function", "doc": "<p>Instantiate the STL dataset object from <code>stl_dataset_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.instantiate_callbacks", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from <code>metrics_cfg</code> and <code>callbacks_cfg</code>. Note that <code>metrics_cfg</code> is a list of metric callbacks and <code>callbacks_cfg</code> is a list of callbacks other the metric callbacks. The instantiated callbacks contain both metric callbacks and other callbacks.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.instantiate_trainer", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from <code>trainer_cfg</code>, <code>lightning_loggers</code>, and <code>callbacks</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">callback</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.set_global_seed", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.set_global_seed", "kind": "function", "doc": "<p>Set the <code>global_seed</code> for the entire evaluation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">global_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_eval.STLEvaluation.run", "modulename": "clarena.pipelines.stl_eval", "qualname": "STLEvaluation.run", "kind": "function", "doc": "<p>The main method to run the single-task learning experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr", "modulename": "clarena.pipelines.stl_expr", "kind": "module", "doc": "<p>The submodule in <code>pipelines</code> for single-task learning experiment.</p>\n"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment", "kind": "class", "doc": "<p>The base class for single-task learning experiment.</p>\n"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.__init__", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the complete config dict for the single-task learning experiment.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span>)</span>"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.cfg", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.cfg", "kind": "variable", "doc": "<p>The complete config dict.</p>\n", "annotation": ": omegaconf.dictconfig.DictConfig"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.eval", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.eval", "kind": "variable", "doc": "<p>Whether to include evaluation phase.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.global_seed", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.global_seed", "kind": "variable", "doc": "<p>The global seed for the entire experiment.</p>\n", "annotation": ": int"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.stl_dataset", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.stl_dataset", "kind": "variable", "doc": "<p>STL dataset object.</p>\n", "annotation": ": clarena.stl_datasets.base.STLDataset"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.backbone", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.backbone", "kind": "variable", "doc": "<p>Backbone network object.</p>\n", "annotation": ": clarena.backbones.base.Backbone"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.head", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.head", "kind": "variable", "doc": "<p>STL output heads object.</p>\n", "annotation": ": clarena.heads.head_stl.HeadSTL"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.model", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.model", "kind": "variable", "doc": "<p>STL model object.</p>\n", "annotation": ": clarena.stl_algorithms.base.STLAlgorithm"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.optimizer", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.optimizer", "kind": "variable", "doc": "<p>Optimizer object.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.lr_scheduler", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.lr_scheduler", "kind": "variable", "doc": "<p>Learning rate scheduler object.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler | None"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.lightning_loggers", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.lightning_loggers", "kind": "variable", "doc": "<p>The list of initialized lightning loggers objects.</p>\n", "annotation": ": list[lightning.pytorch.loggers.logger.Logger]"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.callbacks", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.callbacks", "kind": "variable", "doc": "<p>The list of initialized callbacks objects.</p>\n", "annotation": ": list[lightning.pytorch.callbacks.callback.Callback]"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.trainer", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.trainer", "kind": "variable", "doc": "<p>Trainer object.</p>\n", "annotation": ": lightning.pytorch.trainer.trainer.Trainer"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.sanity_check", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.sanity_check", "kind": "function", "doc": "<p>Sanity check for config.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.instantiate_stl_dataset", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.instantiate_stl_dataset", "kind": "function", "doc": "<p>Instantiate the STL dataset object from <code>stl_dataset_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stl_dataset_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.instantiate_backbone", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.instantiate_backbone", "kind": "function", "doc": "<p>Instantiate the MTL backbone network object from <code>backbone_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">backbone_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.instantiate_head", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.instantiate_head", "kind": "function", "doc": "<p>Instantiate the STL output head object.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input_dim</strong> (<code>int</code>): the input dimension of the head. Must be equal to the <code>output_dim</code> of the connected backbone.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.instantiate_stl_algorithm", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.instantiate_stl_algorithm", "kind": "function", "doc": "<p>Instantiate the stl_algorithm object from <code>stl_algorithm_cfg</code>, <code>backbone</code>, <code>heads</code> and <code>non_algorithmic_hparams</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">stl_algorithm_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Backbone</span>,</span><span class=\"param\">\t<span class=\"n\">head</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">head_stl</span><span class=\"o\">.</span><span class=\"n\">HeadSTL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.instantiate_optimizer", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.instantiate_optimizer", "kind": "function", "doc": "<p>Instantiate the optimizer object from <code>optimizer_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">optimizer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.instantiate_lr_scheduler", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.instantiate_lr_scheduler", "kind": "function", "doc": "<p>Instantiate the learning rate scheduler object from <code>lr_scheduler_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lr_scheduler_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.instantiate_lightning_loggers", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.instantiate_lightning_loggers", "kind": "function", "doc": "<p>Instantiate the list of lightning loggers objects from <code>lightning_loggers_cfg</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">lightning_loggers_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.instantiate_callbacks", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.instantiate_callbacks", "kind": "function", "doc": "<p>Instantiate the list of callbacks objects from <code>metrics_cfg</code> and <code>callbacks_cfg</code>. Note that <code>metrics_cfg</code> is a list of metric callbacks and <code>callbacks_cfg</code> is a list of callbacks other the metric callbacks. The instantiated callbacks contain both metric callbacks and other callbacks.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">metrics_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.instantiate_trainer", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.instantiate_trainer", "kind": "function", "doc": "<p>Instantiate the trainer object from <code>trainer_cfg</code>, <code>lightning_loggers</code>, and <code>callbacks</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">trainer_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">lightning_loggers</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">loggers</span><span class=\"o\">.</span><span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">Logger</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">callbacks</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"n\">lightning</span><span class=\"o\">.</span><span class=\"n\">pytorch</span><span class=\"o\">.</span><span class=\"n\">callbacks</span><span class=\"o\">.</span><span class=\"n\">callback</span><span class=\"o\">.</span><span class=\"n\">Callback</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.set_global_seed", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.set_global_seed", "kind": "function", "doc": "<p>Set the <code>global_seed</code> for the entire experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">global_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.pipelines.stl_expr.STLExperiment.run", "modulename": "clarena.pipelines.stl_expr", "qualname": "STLExperiment.run", "kind": "function", "doc": "<p>The main method to run the single-task learning experiment.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms", "modulename": "clarena.stl_algorithms", "kind": "module", "doc": "<h1 id=\"single-task-learning-algorithms\">Single-Task Learning Algorithms</h1>\n\n<p>This submodule provides the <strong>single-task learning algorithms</strong> in CLArena.</p>\n\n<p>Here are the base classes for STL algorithms, which inherit from PyTorch Lightning <code>LightningModule</code>:</p>\n\n<ul>\n<li><code>STLAlgorithm</code>: the base class for all single-task learning algorithms.</li>\n</ul>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about how to configure and implement STL algorithms:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/stl-algorithm\"><strong>Configure STL Algorithm</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/stl-algorithm\"><strong>Implement Custom STL Algorithm</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm", "kind": "class", "doc": "<p>The base class of single-task learning algorithms.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.__init__", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>Backbone</code>): backbone network.</li>\n<li><strong>head</strong> (<code>HeadsSTL</code>): output head.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Backbone</span>,</span><span class=\"param\">\t<span class=\"n\">head</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">head_stl</span><span class=\"o\">.</span><span class=\"n\">HeadSTL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.backbone", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.backbone", "kind": "variable", "doc": "<p>The backbone network.</p>\n", "annotation": ": clarena.backbones.base.Backbone"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.head", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.head", "kind": "variable", "doc": "<p>The output head.</p>\n", "annotation": ": clarena.heads.head_stl.HeadSTL"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.optimizer", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.optimizer", "kind": "variable", "doc": "<p>Optimizer (partially initialized). Will be equipped with parameters in <code>configure_optimizers()</code>.</p>\n", "annotation": ": torch.optim.optimizer.Optimizer"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.lr_scheduler", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.lr_scheduler", "kind": "variable", "doc": "<p>The learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</p>\n", "annotation": ": torch.optim.lr_scheduler.LRScheduler | None"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.criterion", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.criterion", "kind": "variable", "doc": "<p>The loss function bewteen the output logits and the target labels. Default is cross-entropy loss.</p>\n"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.if_forward_func_return_logits_only", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.if_forward_func_return_logits_only", "kind": "variable", "doc": "<p>Whether the <code>forward()</code> method returns logits only. If <code>False</code>, it returns a dictionary containing logits and other information. Default is <code>False</code>.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.sanity_check", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.setup_task", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.setup_task", "kind": "function", "doc": "<p>Setup the components for the STL algorithm. This must be done before <code>forward()</code> method is called.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>num_classes</strong> (<code>int</code>): the number of classes for the single-task learning.</li>\n<li><strong>optimizer</strong> (<code>Optimizer</code>): the optimizer object (partially initialized).</li>\n<li><strong>lr_scheduler</strong> (<code>LRScheduler</code> | None): the learning rate scheduler for the optimizer. If <code>None</code>, no scheduler is used.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">lr_scheduler</span><span class=\"o\">.</span><span class=\"n\">LRScheduler</span> <span class=\"o\">|</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.forward", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.forward", "kind": "function", "doc": "<p>The forward pass for data from task <code>task_id</code>. Note that it is nothing to do with <code>forward()</code> method in <code>nn.Module</code>. This definition provides a template that many STL algorithm including the vanilla SingleLearning algorithm use.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>input</strong> (<code>Tensor</code>): The input tensor from data.</li>\n<li><strong>stage</strong> (<code>str</code>): the stage of the forward pass; one of:\n<ol>\n<li>'train': training stage.</li>\n<li>'validation': validation stage.</li>\n<li>'test': testing stage.</li>\n</ol></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>logits</strong> (<code>Tensor</code>): the output logits tensor.</li>\n<li><strong>activations</strong> (<code>dict[str, Tensor]</code>): the hidden features (after activation) in each weighted layer. Key (<code>str</code>) is the weighted layer name, value (<code>Tensor</code>) is the hidden feature tensor. This is used for the continual learning algorithms that need to use the hidden features for various purposes.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.STLAlgorithm.configure_optimizers", "modulename": "clarena.stl_algorithms", "qualname": "STLAlgorithm.configure_optimizers", "kind": "function", "doc": "<p>Configure optimizer hooks by Lightning. See <a href=\"https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers\">Lightning docs</a> for more details.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">Optimizer</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.single_learning", "modulename": "clarena.stl_algorithms.single_learning", "kind": "module", "doc": "<p>The submodule in <code>stl_algorithms</code> for single learning algorithm.</p>\n"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning", "kind": "class", "doc": "<p>Single learning algorithm.</p>\n\n<p>The most naive way for single-task learning. It directly trains the task.</p>\n", "bases": "clarena.stl_algorithms.base.STLAlgorithm"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning.__init__", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning.__init__", "kind": "function", "doc": "<p>Initialize the SingleLearning algorithm with the network. It has no additional hyperparameters.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>backbone</strong> (<code>Backbone</code>): backbone network.</li>\n<li><strong>head</strong> (<code>HeadSTL</code>): output head.</li>\n<li><strong>non_algorithmic_hparams</strong> (<code>dict[str, Any]</code>): non-algorithmic hyperparameters that are not related to the algorithm itself are passed to this <code>LightningModule</code> object from the config, such as optimizer and learning rate scheduler configurations. They are saved for Lightning APIs from <code>save_hyperparameters()</code> method. This is useful for the experiment configuration and reproducibility.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">backbone</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">backbones</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Backbone</span>,</span><span class=\"param\">\t<span class=\"n\">head</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">heads</span><span class=\"o\">.</span><span class=\"n\">head_stl</span><span class=\"o\">.</span><span class=\"n\">HeadSTL</span>,</span><span class=\"param\">\t<span class=\"n\">non_algorithmic_hparams</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">{}</span></span>)</span>"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning.training_step", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning.training_step", "kind": "function", "doc": "<p>Training step.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of training data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this training step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics. Must include the key 'loss' which is total loss in the case of automatic optimization, according to PyTorch Lightning docs.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning.validation_step", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning.validation_step", "kind": "function", "doc": "<p>Validation step.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of validation data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this validation step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_algorithms.single_learning.SingleLearning.test_step", "modulename": "clarena.stl_algorithms.single_learning", "qualname": "SingleLearning.test_step", "kind": "function", "doc": "<p>Test step.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and accuracy from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets", "modulename": "clarena.stl_datasets", "kind": "module", "doc": "<h1 id=\"single-task-learning-datasets\">Single-Task Learning Datasets</h1>\n\n<p>This submodule provides the <strong>single-task learning datasets</strong> that can be used in CLArena.</p>\n\n<p>Here are the base classes for single-task learning datasets, which inherit from Lightning <code>LightningDataModule</code>:</p>\n\n<ul>\n<li><code>STLDataset</code>: The base class for all single-task learning datasets.\n<ul>\n<li><code>STLDatasetFromRaw</code>: The base class for constructing single-task learning datasets from raw datasets. A child class of <code>STLDataset</code>.</li>\n</ul></li>\n</ul>\n\n<p>Please note that this is an API documantation. Please refer to the main documentation pages for more information about how to configure and implement single-task learning datasets:</p>\n\n<ul>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/components/STL-dataset\"><strong>Configure STL Dataset</strong></a></li>\n<li><a href=\"https://pengxiang-wang.com/projects/continual-learning-arena/docs/custom-implementation/STL-dataset\"><strong>Implement Custom STL Dataset</strong></a></li>\n</ul>\n"}, {"fullname": "clarena.stl_datasets.STLDataset", "modulename": "clarena.stl_datasets", "qualname": "STLDataset", "kind": "class", "doc": "<p>The base class of single-task learning datasets.</p>\n", "bases": "lightning.pytorch.core.datamodule.LightningDataModule"}, {"fullname": "clarena.stl_datasets.STLDataset.__init__", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original data files for constructing the STL dataset physically live.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code>): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.STLDataset.root", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.root", "kind": "variable", "doc": "<p>The root directory of the original data files.</p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.STLDataset.batch_size", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.batch_size", "kind": "variable", "doc": "<p>The batch size for dataloaders.</p>\n", "annotation": ": int"}, {"fullname": "clarena.stl_datasets.STLDataset.num_workers", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.num_workers", "kind": "variable", "doc": "<p>The number of workers for dataloaders.</p>\n", "annotation": ": int"}, {"fullname": "clarena.stl_datasets.STLDataset.custom_transforms", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.custom_transforms", "kind": "variable", "doc": "<p>The custom transforms.</p>\n", "annotation": ": Union[Callable, torchvision.transforms.transforms.Compose]"}, {"fullname": "clarena.stl_datasets.STLDataset.repeat_channels", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.repeat_channels", "kind": "variable", "doc": "<p>The number of channels to repeat.</p>\n", "annotation": ": int | None"}, {"fullname": "clarena.stl_datasets.STLDataset.to_tensor", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.to_tensor", "kind": "variable", "doc": "<p>The to_tensor flag.</p>\n", "annotation": ": bool"}, {"fullname": "clarena.stl_datasets.STLDataset.resize", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.resize", "kind": "variable", "doc": "<p>The size to resize.</p>\n", "annotation": ": tuple[int, int] | None"}, {"fullname": "clarena.stl_datasets.STLDataset.dataset_train", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.dataset_train", "kind": "variable", "doc": "<p>Training dataset object. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.stl_datasets.STLDataset.dataset_val", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.dataset_val", "kind": "variable", "doc": "<p>Validation dataset object. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.stl_datasets.STLDataset.dataset_test", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.dataset_test", "kind": "variable", "doc": "<p>Test dataset object. Can be PyTorch Dataset objects or any other dataset objects.</p>\n", "annotation": ": Any"}, {"fullname": "clarena.stl_datasets.STLDataset.mean", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.mean", "kind": "variable", "doc": "<p>Mean value for normalization.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.STLDataset.std", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.std", "kind": "variable", "doc": "<p>Standard deviation value for normalization.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.STLDataset.sanity_check", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.sanity_check", "kind": "function", "doc": "<p>Sanity check.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.get_class_map", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.get_class_map", "kind": "function", "doc": "<p>Get the mapping of classes. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>class_map</strong>(<code>dict[str | int, int]</code>): the class map. Keys are original class labels and values are integer class labels for single-task learning.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.prepare_data", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.prepare_data", "kind": "function", "doc": "<p>Use this to download and prepare data. It must be implemented by subclasses, as required by <code>LightningDatamodule</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.setup", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.setup", "kind": "function", "doc": "<p>Set up the dataset for different stages.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>stage</strong> (<code>str</code>): the stage of the experiment; one of:\n<ul>\n<li>'fit': training and validation dataset should be assigned to <code>self.dataset_train</code> and <code>self.dataset_val</code>.</li>\n<li>'test': test dataset should be assigned to <code>self.dataset_test</code>.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.setup_task", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.setup_task", "kind": "function", "doc": "<p>Set up the task for the dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.train_and_val_transforms", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.train_and_val_transforms", "kind": "function", "doc": "<p>Transforms for training and validation dataset, incorporating the custom transforms with basic transforms like normalization and <code>ToTensor()</code>. It can be used in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_transforms</strong> (<code>transforms.Compose</code>): the composed train/val transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.test_transforms", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.test_transforms", "kind": "function", "doc": "<p>Transforms for test dataset. Only basic transforms like normalization and <code>ToTensor()</code> are included. It can be used in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_transforms</strong> (<code>transforms.Compose</code>): the composed test transforms.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.target_transform", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.target_transform", "kind": "function", "doc": "<p>Target transform to map the original class labels to CL class labels. It can be used in subclasses when constructing the dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>target_transform</strong> (<code>Callable</code>): the target transform.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Callable</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.train_and_val_dataset", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Any, Any]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.test_dataset", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.test_dataset", "kind": "function", "doc": "<p>Get the test dataset. It must be implemented by subclasses.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Any</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.train_dataloader", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.train_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the stage train. It is automatically called before training.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_dataloader</strong> (<code>DataLoader</code>): the train DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.val_dataloader", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.val_dataloader", "kind": "function", "doc": "<p>DataLoader generator for the validation stage. It is automatically called before validation.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>val_dataloader</strong> (<code>DataLoader</code>): the validation DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDataset.test_dataloader", "modulename": "clarena.stl_datasets", "qualname": "STLDataset.test_dataloader", "kind": "function", "doc": "<p>DataLoader generator for stage test. It is automatically called before testing.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataloader</strong> (<code>DataLoader</code>): the test DataLoader.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDatasetFromRaw", "modulename": "clarena.stl_datasets", "qualname": "STLDatasetFromRaw", "kind": "class", "doc": "<p>The base class of single-task learning datasets from raw PyTorch Dataset.</p>\n", "bases": "clarena.stl_datasets.base.STLDataset"}, {"fullname": "clarena.stl_datasets.STLDatasetFromRaw.__init__", "modulename": "clarena.stl_datasets", "qualname": "STLDatasetFromRaw.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original data files for constructing the STL dataset physically live.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.STLDatasetFromRaw.original_dataset_python_class", "modulename": "clarena.stl_datasets", "qualname": "STLDatasetFromRaw.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class. <strong>It must be provided in subclasses.</strong></p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]"}, {"fullname": "clarena.stl_datasets.STLDatasetFromRaw.original_dataset_constants", "modulename": "clarena.stl_datasets", "qualname": "STLDatasetFromRaw.original_dataset_constants", "kind": "variable", "doc": "<p>The original dataset constants class.</p>\n", "annotation": ": type[clarena.stl_datasets.raw.constants.DatasetConstants]"}, {"fullname": "clarena.stl_datasets.STLDatasetFromRaw.get_class_map", "modulename": "clarena.stl_datasets", "qualname": "STLDatasetFromRaw.get_class_map", "kind": "function", "doc": "<p>Get the mapping of classes.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>class_map</strong>(<code>dict[str | int, int]</code>): the class map. Key is original class label, value is integer class label for single-task learning.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.STLDatasetFromRaw.setup_task", "modulename": "clarena.stl_datasets", "qualname": "STLDatasetFromRaw.setup_task", "kind": "function", "doc": "<p>Set up the task for the dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.TaskLabelledDataset", "modulename": "clarena.stl_datasets", "qualname": "TaskLabelledDataset", "kind": "class", "doc": "<p>The dataset class that labels the a task's dataset with the given task ID. It is used to label the dataset with the task ID for MTL experiment.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "clarena.stl_datasets.TaskLabelledDataset.__init__", "modulename": "clarena.stl_datasets", "qualname": "TaskLabelledDataset.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataset</strong> (<code>Dataset</code>): the dataset to be labelled.</li>\n<li><strong>task_id</strong> (<code>int</code>): the task ID to be labelled.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>, </span><span class=\"param\"><span class=\"n\">task_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.TaskLabelledDataset.dataset", "modulename": "clarena.stl_datasets", "qualname": "TaskLabelledDataset.dataset", "kind": "variable", "doc": "<p>The original dataset object.</p>\n", "annotation": ": torch.utils.data.dataset.Dataset"}, {"fullname": "clarena.stl_datasets.TaskLabelledDataset.task_id", "modulename": "clarena.stl_datasets", "qualname": "TaskLabelledDataset.task_id", "kind": "variable", "doc": "<p>The task ID.</p>\n", "annotation": ": int"}, {"fullname": "clarena.stl_datasets.SEMEION", "modulename": "clarena.stl_datasets", "qualname": "SEMEION", "kind": "class", "doc": "<p>SEMEION dataset. The <a href=\"https://archive.ics.uci.edu/dataset/178/semeion+handwritten+digit\">SEMEION dataset</a> is a collection of handwritten digits. It consists of 1,593 handwritten digit images (10 classes), each 16x16 grayscale image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.SEMEION.__init__", "modulename": "clarena.stl_datasets", "qualname": "SEMEION.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original SEMEION data 'SEMEION/' live.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.SEMEION.original_dataset_python_class", "modulename": "clarena.stl_datasets", "qualname": "SEMEION.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.semeion.SEMEION&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.SEMEION.test_percentage", "modulename": "clarena.stl_datasets", "qualname": "SEMEION.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.SEMEION.validation_percentage", "modulename": "clarena.stl_datasets", "qualname": "SEMEION.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.SEMEION.prepare_data", "modulename": "clarena.stl_datasets", "qualname": "SEMEION.prepare_data", "kind": "function", "doc": "<p>Download the original SEMEION dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.SEMEION.train_and_val_dataset", "modulename": "clarena.stl_datasets", "qualname": "SEMEION.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.SEMEION.test_dataset", "modulename": "clarena.stl_datasets", "qualname": "SEMEION.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.SEMEION", "modulename": "clarena.stl_datasets.SEMEION", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for SEMEION dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.SEMEION.SEMEION", "modulename": "clarena.stl_datasets.SEMEION", "qualname": "SEMEION", "kind": "class", "doc": "<p>SEMEION dataset. The <a href=\"https://archive.ics.uci.edu/dataset/178/semeion+handwritten+digit\">SEMEION dataset</a> is a collection of handwritten digits. It consists of 1,593 handwritten digit images (10 classes), each 16x16 grayscale image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.SEMEION.SEMEION.__init__", "modulename": "clarena.stl_datasets.SEMEION", "qualname": "SEMEION.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original SEMEION data 'SEMEION/' live.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.SEMEION.SEMEION.original_dataset_python_class", "modulename": "clarena.stl_datasets.SEMEION", "qualname": "SEMEION.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.semeion.SEMEION&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.SEMEION.SEMEION.test_percentage", "modulename": "clarena.stl_datasets.SEMEION", "qualname": "SEMEION.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.SEMEION.SEMEION.validation_percentage", "modulename": "clarena.stl_datasets.SEMEION", "qualname": "SEMEION.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.SEMEION.SEMEION.prepare_data", "modulename": "clarena.stl_datasets.SEMEION", "qualname": "SEMEION.prepare_data", "kind": "function", "doc": "<p>Download the original SEMEION dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.SEMEION.SEMEION.train_and_val_dataset", "modulename": "clarena.stl_datasets.SEMEION", "qualname": "SEMEION.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.SEMEION.SEMEION.test_dataset", "modulename": "clarena.stl_datasets.SEMEION", "qualname": "SEMEION.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.ahdd", "modulename": "clarena.stl_datasets.ahdd", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Arabic Handwritten Digits dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.ahdd.ArabicHandwrittenDigits", "modulename": "clarena.stl_datasets.ahdd", "qualname": "ArabicHandwrittenDigits", "kind": "class", "doc": "<p>Arabic Handwritten Digits dataset. The <a href=\"https://www.kaggle.com/datasets/mloey1/ahdd1\">Arabic Handwritten Digits dataset</a> is a collection of handwritten Arabic digits (0-9). It consists of 60,000 training and 10,000 test images of handwritten Arabic digits (10 classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.ahdd.ArabicHandwrittenDigits.__init__", "modulename": "clarena.stl_datasets.ahdd", "qualname": "ArabicHandwrittenDigits.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Arabic Handwritten Digits data 'ArabicHandwrittenDigits/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.ahdd.ArabicHandwrittenDigits.original_dataset_python_class", "modulename": "clarena.stl_datasets.ahdd", "qualname": "ArabicHandwrittenDigits.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class. <strong>It must be provided in subclasses.</strong></p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.ahdd.ArabicHandwrittenDigits&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.ahdd.ArabicHandwrittenDigits.validation_percentage", "modulename": "clarena.stl_datasets.ahdd", "qualname": "ArabicHandwrittenDigits.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.ahdd.ArabicHandwrittenDigits.prepare_data", "modulename": "clarena.stl_datasets.ahdd", "qualname": "ArabicHandwrittenDigits.prepare_data", "kind": "function", "doc": "<p>Download the original Arabic Handwritten Digits dataset if haven't. Because the original dataset is published on Kaggle, we need to download it manually. This function will not download the original dataset automatically.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.ahdd.ArabicHandwrittenDigits.train_and_val_dataset", "modulename": "clarena.stl_datasets.ahdd", "qualname": "ArabicHandwrittenDigits.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.ahdd.ArabicHandwrittenDigits.test_dataset", "modulename": "clarena.stl_datasets.ahdd", "qualname": "ArabicHandwrittenDigits.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.caltech101", "modulename": "clarena.stl_datasets.caltech101", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Caltech 101 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.caltech101.Caltech101", "modulename": "clarena.stl_datasets.caltech101", "qualname": "Caltech101", "kind": "class", "doc": "<p>Caltech 101 dataset. The <a href=\"https://data.caltech.edu/records/mzrjq-6wc02\">Caltech 101 dataset</a> is a collection of pictures of objects. It consists of 9,146 images of 101 classes, each color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.caltech101.Caltech101.__init__", "modulename": "clarena.stl_datasets.caltech101", "qualname": "Caltech101.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Caltech data 'Caltech/' live.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.caltech101.Caltech101.original_dataset_python_class", "modulename": "clarena.stl_datasets.caltech101", "qualname": "Caltech101.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.caltech.Caltech101&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.caltech101.Caltech101.test_percentage", "modulename": "clarena.stl_datasets.caltech101", "qualname": "Caltech101.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.caltech101.Caltech101.validation_percentage", "modulename": "clarena.stl_datasets.caltech101", "qualname": "Caltech101.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.caltech101.Caltech101.prepare_data", "modulename": "clarena.stl_datasets.caltech101", "qualname": "Caltech101.prepare_data", "kind": "function", "doc": "<p>Download the original Caltech 101 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.caltech101.Caltech101.train_and_val_dataset", "modulename": "clarena.stl_datasets.caltech101", "qualname": "Caltech101.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.caltech101.Caltech101.test_dataset", "modulename": "clarena.stl_datasets.caltech101", "qualname": "Caltech101.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.caltech256", "modulename": "clarena.stl_datasets.caltech256", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Caltech 256 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.caltech256.Caltech256", "modulename": "clarena.stl_datasets.caltech256", "qualname": "Caltech256", "kind": "class", "doc": "<p>Caltech 256 dataset. The <a href=\"https://data.caltech.edu/records/nyy15-4j048\">Caltech 256 dataset</a> is a collection of pictures of objects. It consists of 30,607 images of 256 classes, each color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.caltech256.Caltech256.__init__", "modulename": "clarena.stl_datasets.caltech256", "qualname": "Caltech256.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Caltech data 'Caltech/' live.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.caltech256.Caltech256.original_dataset_python_class", "modulename": "clarena.stl_datasets.caltech256", "qualname": "Caltech256.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.caltech.Caltech256&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.caltech256.Caltech256.test_percentage", "modulename": "clarena.stl_datasets.caltech256", "qualname": "Caltech256.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.caltech256.Caltech256.validation_percentage", "modulename": "clarena.stl_datasets.caltech256", "qualname": "Caltech256.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.caltech256.Caltech256.prepare_data", "modulename": "clarena.stl_datasets.caltech256", "qualname": "Caltech256.prepare_data", "kind": "function", "doc": "<p>Download the original Caltech 256 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.caltech256.Caltech256.train_and_val_dataset", "modulename": "clarena.stl_datasets.caltech256", "qualname": "Caltech256.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.caltech256.Caltech256.test_dataset", "modulename": "clarena.stl_datasets.caltech256", "qualname": "Caltech256.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.celeba", "modulename": "clarena.stl_datasets.celeba", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for CelebA dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.celeba.CelebA", "modulename": "clarena.stl_datasets.celeba", "qualname": "CelebA", "kind": "class", "doc": "<p>CelebA dataset. The <a href=\"https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\">CelebFaces Attributes Dataset (CelebA)</a> is a large-scale celebrity faces dataset. It consists of 202,599 face images of 10,177 celebrity identities (classes), each 178x218 color image.</p>\n\n<p>Note that the original CelebA dataset is not a classification dataset but an attributes dataset. We only use the identity of each face as the class label for classification.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.celeba.CelebA.__init__", "modulename": "clarena.stl_datasets.celeba", "qualname": "CelebA.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CelebA data 'CelebA/' live.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.celeba.CelebA.original_dataset_python_class", "modulename": "clarena.stl_datasets.celeba", "qualname": "CelebA.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.celeba.CelebA&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.celeba.CelebA.prepare_data", "modulename": "clarena.stl_datasets.celeba", "qualname": "CelebA.prepare_data", "kind": "function", "doc": "<p>Download the original CelebA dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.celeba.CelebA.train_and_val_dataset", "modulename": "clarena.stl_datasets.celeba", "qualname": "CelebA.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.celeba.CelebA.test_dataset", "modulename": "clarena.stl_datasets.celeba", "qualname": "CelebA.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.cifar10", "modulename": "clarena.stl_datasets.cifar10", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for CIFAR-10 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.cifar10.CIFAR10", "modulename": "clarena.stl_datasets.cifar10", "qualname": "CIFAR10", "kind": "class", "doc": "<p>CIFAR-10 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 10 classes, each 32x32 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.cifar10.CIFAR10.__init__", "modulename": "clarena.stl_datasets.cifar10", "qualname": "CIFAR10.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-10 data 'cifar-10-python/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.cifar10.CIFAR10.original_dataset_python_class", "modulename": "clarena.stl_datasets.cifar10", "qualname": "CIFAR10.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR10&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.cifar10.CIFAR10.validation_percentage", "modulename": "clarena.stl_datasets.cifar10", "qualname": "CIFAR10.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.cifar10.CIFAR10.prepare_data", "modulename": "clarena.stl_datasets.cifar10", "qualname": "CIFAR10.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.cifar10.CIFAR10.train_and_val_dataset", "modulename": "clarena.stl_datasets.cifar10", "qualname": "CIFAR10.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.cifar10.CIFAR10.test_dataset", "modulename": "clarena.stl_datasets.cifar10", "qualname": "CIFAR10.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.cifar100", "modulename": "clarena.stl_datasets.cifar100", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for CIFAR-100 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.cifar100.CIFAR100", "modulename": "clarena.stl_datasets.cifar100", "qualname": "CIFAR100", "kind": "class", "doc": "<p>CIFAR-100 dataset. The <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-100 dataset</a> is a subset of the <a href=\"https://people.csail.mit.edu/torralba/tinyimages/\">80 million tiny images dataset</a>. It consists of 50,000 training and 10,000 test images of 100 classes, each 32x32 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.cifar100.CIFAR100.__init__", "modulename": "clarena.stl_datasets.cifar100", "qualname": "CIFAR100.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original CIFAR-100 data 'cifar-100-python/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.cifar100.CIFAR100.original_dataset_python_class", "modulename": "clarena.stl_datasets.cifar100", "qualname": "CIFAR100.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.cifar.CIFAR100&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.cifar100.CIFAR100.validation_percentage", "modulename": "clarena.stl_datasets.cifar100", "qualname": "CIFAR100.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.cifar100.CIFAR100.prepare_data", "modulename": "clarena.stl_datasets.cifar100", "qualname": "CIFAR100.prepare_data", "kind": "function", "doc": "<p>Download the original CIFAR dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.cifar100.CIFAR100.train_and_val_dataset", "modulename": "clarena.stl_datasets.cifar100", "qualname": "CIFAR100.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.cifar100.CIFAR100.test_dataset", "modulename": "clarena.stl_datasets.cifar100", "qualname": "CIFAR100.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.country211", "modulename": "clarena.stl_datasets.country211", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Country211 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.country211.Country211", "modulename": "clarena.stl_datasets.country211", "qualname": "Country211", "kind": "class", "doc": "<p>Country211 dataset. The <a href=\"https://github.com/openai/CLIP/blob/main/data/country211.md\">Country211 dataset</a> is a collection of geolocation pictures of different countries. It consists of 31,650 training, 10,550 validation, and 21,100 test images of 211 countries (classes), each 256x256 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.country211.Country211.__init__", "modulename": "clarena.stl_datasets.country211", "qualname": "Country211.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Country211 data 'Country211/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.country211.Country211.original_dataset_python_class", "modulename": "clarena.stl_datasets.country211", "qualname": "Country211.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.country211.Country211&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.country211.Country211.prepare_data", "modulename": "clarena.stl_datasets.country211", "qualname": "Country211.prepare_data", "kind": "function", "doc": "<p>Download the original Country211 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.country211.Country211.train_and_val_dataset", "modulename": "clarena.stl_datasets.country211", "qualname": "Country211.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.country211.Country211.test_dataset", "modulename": "clarena.stl_datasets.country211", "qualname": "Country211.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.cub2002011", "modulename": "clarena.stl_datasets.cub2002011", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for CUB-200-2011 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.cub2002011.CUB2002011", "modulename": "clarena.stl_datasets.cub2002011", "qualname": "CUB2002011", "kind": "class", "doc": "<p>CUB-200-2011 dataset. The <a href=\"https://www.vision.caltech.edu/datasets/cub_200_2011/\">CUB (Caltech-UCSD Birds)-200-2011)</a> is a bird image dataset. It consists of 100,000 training, 10,000 validation, 10,000 test images of 200 bird species (classes), each 64x64 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.cub2002011.CUB2002011.__init__", "modulename": "clarena.stl_datasets.cub2002011", "qualname": "CUB2002011.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original MNIST data 'MNIST/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.cub2002011.CUB2002011.original_dataset_python_class", "modulename": "clarena.stl_datasets.cub2002011", "qualname": "CUB2002011.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.cub2002011.CUB2002011&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.cub2002011.CUB2002011.validation_percentage", "modulename": "clarena.stl_datasets.cub2002011", "qualname": "CUB2002011.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.cub2002011.CUB2002011.prepare_data", "modulename": "clarena.stl_datasets.cub2002011", "qualname": "CUB2002011.prepare_data", "kind": "function", "doc": "<p>Download the original CUB-200-2011 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.cub2002011.CUB2002011.train_and_val_dataset", "modulename": "clarena.stl_datasets.cub2002011", "qualname": "CUB2002011.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.cub2002011.CUB2002011.test_dataset", "modulename": "clarena.stl_datasets.cub2002011", "qualname": "CUB2002011.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.dtd", "modulename": "clarena.stl_datasets.dtd", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for DTD dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.dtd.DTD", "modulename": "clarena.stl_datasets.dtd", "qualname": "DTD", "kind": "class", "doc": "<p>DTD dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/dtd/\">DTD dataset</a> is a collection of describable texture pictures. It consists of 5,640 images of 47 kinds of textures (classes), each 300x300-640x640 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.dtd.DTD.__init__", "modulename": "clarena.stl_datasets.dtd", "qualname": "DTD.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original DTD data 'DTD/' live.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.dtd.DTD.original_dataset_python_class", "modulename": "clarena.stl_datasets.dtd", "qualname": "DTD.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.dtd.DTD&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.dtd.DTD.prepare_data", "modulename": "clarena.stl_datasets.dtd", "qualname": "DTD.prepare_data", "kind": "function", "doc": "<p>Download the original DTD dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.dtd.DTD.train_and_val_dataset", "modulename": "clarena.stl_datasets.dtd", "qualname": "DTD.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.dtd.DTD.test_dataset", "modulename": "clarena.stl_datasets.dtd", "qualname": "DTD.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.emnist", "modulename": "clarena.stl_datasets.emnist", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for EMNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.emnist.EMNIST", "modulename": "clarena.stl_datasets.emnist", "qualname": "EMNIST", "kind": "class", "doc": "<p>EMNIST dataset. The <a href=\"https://www.nist.gov/itl/products-and-services/emnist-dataset/\">EMNIST dataset</a> is a collection of handwritten letters and digits (including A-Z, a-z, 0-9). It consists of 814,255 images in 62 classes, each 28x28 grayscale image.</p>\n\n<p>EMNIST has 6 different splits: <code>byclass</code>, <code>bymerge</code>, <code>balanced</code>, <code>letters</code>, <code>digits</code> and <code>mnist</code>, each containing a different subset of the original collection. We support all of them in Permuted EMNIST.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.emnist.EMNIST.__init__", "modulename": "clarena.stl_datasets.emnist", "qualname": "EMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original EMNIST data 'EMNIST/' live.</li>\n<li><strong>split</strong> (<code>str</code>): the original EMNIST dataset has 6 different splits: <code>byclass</code>, <code>bymerge</code>, <code>balanced</code>, <code>letters</code>, <code>digits</code> and <code>mnist</code>. This argument specifies which one to use.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">split</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.emnist.EMNIST.split", "modulename": "clarena.stl_datasets.emnist", "qualname": "EMNIST.split", "kind": "variable", "doc": "<p>The split of the original EMNIST dataset. It can be <code>byclass</code>, <code>bymerge</code>, <code>balanced</code>, <code>letters</code>, <code>digits</code> or <code>mnist</code>.</p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.emnist.EMNIST.validation_percentage", "modulename": "clarena.stl_datasets.emnist", "qualname": "EMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.emnist.EMNIST.prepare_data", "modulename": "clarena.stl_datasets.emnist", "qualname": "EMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original EMNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.emnist.EMNIST.train_and_val_dataset", "modulename": "clarena.stl_datasets.emnist", "qualname": "EMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.emnist.EMNIST.test_dataset", "modulename": "clarena.stl_datasets.emnist", "qualname": "EMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.eurosat", "modulename": "clarena.stl_datasets.eurosat", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for EuroSAT dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.eurosat.EuroSAT", "modulename": "clarena.stl_datasets.eurosat", "qualname": "EuroSAT", "kind": "class", "doc": "<p>EuroSAT dataset. The <a href=\"https://github.com/phelber/eurosat\">EuroSAT dataset</a> is a collection of satellite images of lands. It consists of 27,000 images of 10 classes, each 64x64 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.eurosat.EuroSAT.__init__", "modulename": "clarena.stl_datasets.eurosat", "qualname": "EuroSAT.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Caltech data 'EuroSAT/' live.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.eurosat.EuroSAT.original_dataset_python_class", "modulename": "clarena.stl_datasets.eurosat", "qualname": "EuroSAT.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.eurosat.EuroSAT&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.eurosat.EuroSAT.test_percentage", "modulename": "clarena.stl_datasets.eurosat", "qualname": "EuroSAT.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.eurosat.EuroSAT.validation_percentage", "modulename": "clarena.stl_datasets.eurosat", "qualname": "EuroSAT.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.eurosat.EuroSAT.prepare_data", "modulename": "clarena.stl_datasets.eurosat", "qualname": "EuroSAT.prepare_data", "kind": "function", "doc": "<p>Download the original EuroSAT dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.eurosat.EuroSAT.train_and_val_dataset", "modulename": "clarena.stl_datasets.eurosat", "qualname": "EuroSAT.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.eurosat.EuroSAT.test_dataset", "modulename": "clarena.stl_datasets.eurosat", "qualname": "EuroSAT.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.facescrub", "modulename": "clarena.stl_datasets.facescrub", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for FaceScrub dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.facescrub.FaceScrub", "modulename": "clarena.stl_datasets.facescrub", "qualname": "FaceScrub", "kind": "class", "doc": "<p>FaceScrub dataset. The <a href=\"https://vintage.winklerbros.net/facescrub.html\">original FaceScrub dataset</a> is a collection of human face images. It consists 106,863 images of 530 people (classes), each high resolution color image.</p>\n\n<p>To make it simple, <a href=\"https://github.com/nkundiushuti/facescrub_subset\">this version</a> uses subset of the official <a href=\"http://megaface.cs.washington.edu/participate/challenge.html\">Megaface FaceScrub challenge</a>, cropped and resized to 32x32. We have <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_10.zip\">FaceScrub-10</a>, <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_20.zip\">FaceScrub-20</a>, <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_50.zip\">FaceScrub-50</a>, <a href=\"https://github.com/nkundiushuti/facescrub_subset/blob/master/data/facescrub_100.zip\">FaceScrub-100</a> datasets where the number of classes are 10, 20, 50 and 100 respectively.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.facescrub.FaceScrub.__init__", "modulename": "clarena.stl_datasets.facescrub", "qualname": "FaceScrub.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original FaceScrub data 'FaceScrub/' live.</li>\n<li><strong>size</strong> (<code>str</code>): the size of the dataset; one of:\n<ol>\n<li>'10': 10 classes (10 people).</li>\n<li>'20': 20 classes (20 people).</li>\n<li>'50': 50 classes (50 people).</li>\n<li>'100': 100 classes (100 people).</li>\n</ol></li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">size</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.facescrub.FaceScrub.validation_percentage", "modulename": "clarena.stl_datasets.facescrub", "qualname": "FaceScrub.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.facescrub.FaceScrub.prepare_data", "modulename": "clarena.stl_datasets.facescrub", "qualname": "FaceScrub.prepare_data", "kind": "function", "doc": "<p>Download the original FaceScrub dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.facescrub.FaceScrub.train_and_val_dataset", "modulename": "clarena.stl_datasets.facescrub", "qualname": "FaceScrub.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.facescrub.FaceScrub.test_dataset", "modulename": "clarena.stl_datasets.facescrub", "qualname": "FaceScrub.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.fashionmnist", "modulename": "clarena.stl_datasets.fashionmnist", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Fashion-MNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.fashionmnist.FashionMNIST", "modulename": "clarena.stl_datasets.fashionmnist", "qualname": "FashionMNIST", "kind": "class", "doc": "<p>Fashion-MNIST dataset. The <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST dataset</a> is a collection of fashion images. It consists of 60,000 training and 10,000 test images of 10 types of clothing (classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.fashionmnist.FashionMNIST.__init__", "modulename": "clarena.stl_datasets.fashionmnist", "qualname": "FashionMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Fashion-MNIST data 'FashionMNIST/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.fashionmnist.FashionMNIST.original_dataset_python_class", "modulename": "clarena.stl_datasets.fashionmnist", "qualname": "FashionMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.FashionMNIST&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.fashionmnist.FashionMNIST.validation_percentage", "modulename": "clarena.stl_datasets.fashionmnist", "qualname": "FashionMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.fashionmnist.FashionMNIST.prepare_data", "modulename": "clarena.stl_datasets.fashionmnist", "qualname": "FashionMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Fashion-MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.fashionmnist.FashionMNIST.train_and_val_dataset", "modulename": "clarena.stl_datasets.fashionmnist", "qualname": "FashionMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.fashionmnist.FashionMNIST.test_dataset", "modulename": "clarena.stl_datasets.fashionmnist", "qualname": "FashionMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.fer2013", "modulename": "clarena.stl_datasets.fer2013", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for FER2013 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.fer2013.FER2013", "modulename": "clarena.stl_datasets.fer2013", "qualname": "FER2013", "kind": "class", "doc": "<p>FER2013 dataset. The <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0893608014002159\">FER2013 dataset</a> is a collection of facial expression images. It consists of 35,887 images of 7 facial expressions (classes), each 48x48 grayscale image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.fer2013.FER2013.__init__", "modulename": "clarena.stl_datasets.fer2013", "qualname": "FER2013.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original FER2013 data 'FER2013/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.fer2013.FER2013.original_dataset_python_class", "modulename": "clarena.stl_datasets.fer2013", "qualname": "FER2013.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.fer2013.FER2013&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.fer2013.FER2013.validation_percentage", "modulename": "clarena.stl_datasets.fer2013", "qualname": "FER2013.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.fer2013.FER2013.prepare_data", "modulename": "clarena.stl_datasets.fer2013", "qualname": "FER2013.prepare_data", "kind": "function", "doc": "<p>Download the original FER2013 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.fer2013.FER2013.train_and_val_dataset", "modulename": "clarena.stl_datasets.fer2013", "qualname": "FER2013.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.fer2013.FER2013.test_dataset", "modulename": "clarena.stl_datasets.fer2013", "qualname": "FER2013.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.fgvc_aircraft", "modulename": "clarena.stl_datasets.fgvc_aircraft", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for FGVC-Aircraft dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.fgvc_aircraft.FGVCAircraft", "modulename": "clarena.stl_datasets.fgvc_aircraft", "qualname": "FGVCAircraft", "kind": "class", "doc": "<p>FGVC-Aircraft dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/\">FGVC-Aircraft dataset</a> is a collection of aircraft images. It consists of 10,200 images, each color image.</p>\n\n<p>FGVC-Aircraft has 3 different class labels by variant, family and manufacturer, which has 102, 70, 41 classes respectively. We support all of them in Permuted FGVC-Aircraft.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.fgvc_aircraft.FGVCAircraft.__init__", "modulename": "clarena.stl_datasets.fgvc_aircraft", "qualname": "FGVCAircraft.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original FGVCAircraft data 'FGVCAircraft/' live.</li>\n<li><strong>annotation_level</strong> (<code>str</code>): The annotation level, supports 'variant', 'family' and 'manufacturer'.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">annotation_level</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.fgvc_aircraft.FGVCAircraft.annotation_level", "modulename": "clarena.stl_datasets.fgvc_aircraft", "qualname": "FGVCAircraft.annotation_level", "kind": "variable", "doc": "<p>The annotation level, supports 'variant', 'family' and 'manufacturer'.</p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.fgvc_aircraft.FGVCAircraft.prepare_data", "modulename": "clarena.stl_datasets.fgvc_aircraft", "qualname": "FGVCAircraft.prepare_data", "kind": "function", "doc": "<p>Download the original FGVC-Aircraft dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.fgvc_aircraft.FGVCAircraft.train_and_val_dataset", "modulename": "clarena.stl_datasets.fgvc_aircraft", "qualname": "FGVCAircraft.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.fgvc_aircraft.FGVCAircraft.test_dataset", "modulename": "clarena.stl_datasets.fgvc_aircraft", "qualname": "FGVCAircraft.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.flowers102", "modulename": "clarena.stl_datasets.flowers102", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Oxford 102 Flower dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.flowers102.Flowers102", "modulename": "clarena.stl_datasets.flowers102", "qualname": "Flowers102", "kind": "class", "doc": "<p>Oxford 102 Flower dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Oxford 102 Flower dataset</a> is a collection of flower pictures. It consists of 8,189 images of 102 kinds of flowers (classes), each color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.flowers102.Flowers102.__init__", "modulename": "clarena.stl_datasets.flowers102", "qualname": "Flowers102.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Oxford 102 Flower data 'Flower102/' live.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.flowers102.Flowers102.original_dataset_python_class", "modulename": "clarena.stl_datasets.flowers102", "qualname": "Flowers102.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.flowers102.Flowers102&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.flowers102.Flowers102.prepare_data", "modulename": "clarena.stl_datasets.flowers102", "qualname": "Flowers102.prepare_data", "kind": "function", "doc": "<p>Download the original Oxford 102 Flower dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.flowers102.Flowers102.train_and_val_dataset", "modulename": "clarena.stl_datasets.flowers102", "qualname": "Flowers102.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.flowers102.Flowers102.test_dataset", "modulename": "clarena.stl_datasets.flowers102", "qualname": "Flowers102.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.food101", "modulename": "clarena.stl_datasets.food101", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Food-101 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.food101.Food101", "modulename": "clarena.stl_datasets.food101", "qualname": "Food101", "kind": "class", "doc": "<p>Food-101 dataset. The <a href=\"https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/\">Food-101 dataset</a> is a collection of food images. It consists of 101,000 images of 101 classes, each color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.food101.Food101.__init__", "modulename": "clarena.stl_datasets.food101", "qualname": "Food101.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Food-101 data 'Food101/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.food101.Food101.original_dataset_python_class", "modulename": "clarena.stl_datasets.food101", "qualname": "Food101.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.food101.Food101&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.food101.Food101.validation_percentage", "modulename": "clarena.stl_datasets.food101", "qualname": "Food101.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.food101.Food101.prepare_data", "modulename": "clarena.stl_datasets.food101", "qualname": "Food101.prepare_data", "kind": "function", "doc": "<p>Download the original Food-101 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.food101.Food101.train_and_val_dataset", "modulename": "clarena.stl_datasets.food101", "qualname": "Food101.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.food101.Food101.test_dataset", "modulename": "clarena.stl_datasets.food101", "qualname": "Food101.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.gtsrb", "modulename": "clarena.stl_datasets.gtsrb", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for GTSRB dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.gtsrb.GTSRB", "modulename": "clarena.stl_datasets.gtsrb", "qualname": "GTSRB", "kind": "class", "doc": "<p>GTSRB dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">GTSRB dataset</a> is a collection of traffic sign images. It consists of 51,839 images of 43 different traffic signs (classes), each color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.gtsrb.GTSRB.__init__", "modulename": "clarena.stl_datasets.gtsrb", "qualname": "GTSRB.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original GTSRB data 'GTSRB/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.gtsrb.GTSRB.original_dataset_python_class", "modulename": "clarena.stl_datasets.gtsrb", "qualname": "GTSRB.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.gtsrb.GTSRB&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.gtsrb.GTSRB.validation_percentage", "modulename": "clarena.stl_datasets.gtsrb", "qualname": "GTSRB.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.gtsrb.GTSRB.prepare_data", "modulename": "clarena.stl_datasets.gtsrb", "qualname": "GTSRB.prepare_data", "kind": "function", "doc": "<p>Download the original GTSRB dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.gtsrb.GTSRB.train_and_val_dataset", "modulename": "clarena.stl_datasets.gtsrb", "qualname": "GTSRB.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.gtsrb.GTSRB.test_dataset", "modulename": "clarena.stl_datasets.gtsrb", "qualname": "GTSRB.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.imagenette", "modulename": "clarena.stl_datasets.imagenette", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Imagenette dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.imagenette.Imagenette", "modulename": "clarena.stl_datasets.imagenette", "qualname": "Imagenette", "kind": "class", "doc": "<p>Imagenette dataset. The <a href=\"https://github.com/fastai/imagenette\">Imagenette dataset</a> is a subset of 10 easily classified classes from <a href=\"https://www.image-net.org\">Imagenet</a>. It provides full sizes (as Imagenet), and resized 320x320 and 160x160. We support all of them in Permuted Imagenette.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.imagenette.Imagenette.__init__", "modulename": "clarena.stl_datasets.imagenette", "qualname": "Imagenette.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Imagenette data 'Imagenette/' live.</li>\n<li><strong>size</strong> (<code>str</code>): image size type. Supports \"full\" (default), \"320px\", and \"160px\".</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">size</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.imagenette.Imagenette.original_dataset_python_class", "modulename": "clarena.stl_datasets.imagenette", "qualname": "Imagenette.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.imagenette.Imagenette&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.imagenette.Imagenette.size", "modulename": "clarena.stl_datasets.imagenette", "qualname": "Imagenette.size", "kind": "variable", "doc": "<p>The size type of image.</p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.imagenette.Imagenette.validation_percentage", "modulename": "clarena.stl_datasets.imagenette", "qualname": "Imagenette.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.imagenette.Imagenette.prepare_data", "modulename": "clarena.stl_datasets.imagenette", "qualname": "Imagenette.prepare_data", "kind": "function", "doc": "<p>Download the original Imagenette dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.imagenette.Imagenette.train_and_val_dataset", "modulename": "clarena.stl_datasets.imagenette", "qualname": "Imagenette.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.imagenette.Imagenette.test_dataset", "modulename": "clarena.stl_datasets.imagenette", "qualname": "Imagenette.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.kannadamnist", "modulename": "clarena.stl_datasets.kannadamnist", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Kannada-MNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.kannadamnist.KannadaMNIST", "modulename": "clarena.stl_datasets.kannadamnist", "qualname": "KannadaMNIST", "kind": "class", "doc": "<p>Kannada-MNIST dataset. The <a href=\"https://github.com/vinayprabhu/Kannada_MNIST\">Kannada-MNIST dataset</a> is a collection of handwritten Kannada digits (0-9). It consists of 60,000 training and 10,000 test images of handwritten Kannada digits (10 classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.kannadamnist.KannadaMNIST.__init__", "modulename": "clarena.stl_datasets.kannadamnist", "qualname": "KannadaMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Kannada-MNIST data 'KannadaMNIST/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.kannadamnist.KannadaMNIST.original_dataset_python_class", "modulename": "clarena.stl_datasets.kannadamnist", "qualname": "KannadaMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.kannada_mnist.KannadaMNIST&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.kannadamnist.KannadaMNIST.validation_percentage", "modulename": "clarena.stl_datasets.kannadamnist", "qualname": "KannadaMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.kannadamnist.KannadaMNIST.prepare_data", "modulename": "clarena.stl_datasets.kannadamnist", "qualname": "KannadaMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Kannada-MNIST dataset if haven't. Because the original dataset is published on Kaggle, we need to download it manually. This function will not download the original dataset automatically.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.kannadamnist.KannadaMNIST.train_and_val_dataset", "modulename": "clarena.stl_datasets.kannadamnist", "qualname": "KannadaMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.kannadamnist.KannadaMNIST.test_dataset", "modulename": "clarena.stl_datasets.kannadamnist", "qualname": "KannadaMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.kmnist", "modulename": "clarena.stl_datasets.kmnist", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for KMNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.kmnist.KMNIST", "modulename": "clarena.stl_datasets.kmnist", "qualname": "KMNIST", "kind": "class", "doc": "<p>Kuzushiji-MNIST dataset. The <a href=\"https://github.com/rois-codh/kmnist\">Kuzushiji-MNIST dataset</a> is a collection of Japanese Kuzushiji character images. It consists of 60,000 training and 10,000 test images of Japanese Kuzushiji images (10 classes), each 28x28 grayscale image (similar to MNIST).</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.kmnist.KMNIST.__init__", "modulename": "clarena.stl_datasets.kmnist", "qualname": "KMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Kuzushiji-MNIST data 'KMNIST/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.kmnist.KMNIST.original_dataset_python_class", "modulename": "clarena.stl_datasets.kmnist", "qualname": "KMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.KMNIST&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.kmnist.KMNIST.validation_percentage", "modulename": "clarena.stl_datasets.kmnist", "qualname": "KMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.kmnist.KMNIST.prepare_data", "modulename": "clarena.stl_datasets.kmnist", "qualname": "KMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Kuzushiji-MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.kmnist.KMNIST.train_and_val_dataset", "modulename": "clarena.stl_datasets.kmnist", "qualname": "KMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.kmnist.KMNIST.test_dataset", "modulename": "clarena.stl_datasets.kmnist", "qualname": "KMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.linnaeus5", "modulename": "clarena.stl_datasets.linnaeus5", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Linnaeus 5 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.linnaeus5.Linnaeus5", "modulename": "clarena.stl_datasets.linnaeus5", "qualname": "Linnaeus5", "kind": "class", "doc": "<p>Linnaeus 5 dataset. The <a href=\"https://chaladze.com/l5/\">Linnaeus 5 dataset</a> is a collection of flower images. It consists of 8,000 images of 5 flower species (classes). It provides 256x256, 128x128, 64x64, and 32x32 color images. We support all of them in Permuted Linnaeus 5.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.linnaeus5.Linnaeus5.__init__", "modulename": "clarena.stl_datasets.linnaeus5", "qualname": "Linnaeus5.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Linnaeus 5 data 'Linnaeus5/' live.</li>\n<li><strong>resolution</strong> (<code>str</code>): Image resolution, one of [\"256\", \"128\", \"64\", \"32\"].</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">resolution</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.linnaeus5.Linnaeus5.resolution", "modulename": "clarena.stl_datasets.linnaeus5", "qualname": "Linnaeus5.resolution", "kind": "variable", "doc": "<p>Store the resolution of the original dataset.</p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.linnaeus5.Linnaeus5.validation_percentage", "modulename": "clarena.stl_datasets.linnaeus5", "qualname": "Linnaeus5.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.linnaeus5.Linnaeus5.prepare_data", "modulename": "clarena.stl_datasets.linnaeus5", "qualname": "Linnaeus5.prepare_data", "kind": "function", "doc": "<p>Download the original Linnaeus 5 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.linnaeus5.Linnaeus5.train_and_val_dataset", "modulename": "clarena.stl_datasets.linnaeus5", "qualname": "Linnaeus5.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.linnaeus5.Linnaeus5.test_dataset", "modulename": "clarena.stl_datasets.linnaeus5", "qualname": "Linnaeus5.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.mnist", "modulename": "clarena.stl_datasets.mnist", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for MNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.mnist.MNIST", "modulename": "clarena.stl_datasets.mnist", "qualname": "MNIST", "kind": "class", "doc": "<p>MNIST dataset. The <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST dataset</a> is a collection of handwritten digits. It consists of 60,000 training and 10,000 test images of handwritten digit images (10 classes), each 28x28 grayscale image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.mnist.MNIST.__init__", "modulename": "clarena.stl_datasets.mnist", "qualname": "MNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original MNIST data 'MNIST/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.mnist.MNIST.original_dataset_python_class", "modulename": "clarena.stl_datasets.mnist", "qualname": "MNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.mnist.MNIST&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.mnist.MNIST.validation_percentage", "modulename": "clarena.stl_datasets.mnist", "qualname": "MNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.mnist.MNIST.prepare_data", "modulename": "clarena.stl_datasets.mnist", "qualname": "MNIST.prepare_data", "kind": "function", "doc": "<p>Download the original MNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.mnist.MNIST.train_and_val_dataset", "modulename": "clarena.stl_datasets.mnist", "qualname": "MNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.mnist.MNIST.test_dataset", "modulename": "clarena.stl_datasets.mnist", "qualname": "MNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.notmnist", "modulename": "clarena.stl_datasets.notmnist", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for NotMNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.notmnist.NotMNIST", "modulename": "clarena.stl_datasets.notmnist", "qualname": "NotMNIST", "kind": "class", "doc": "<p>NotMNIST dataset. The <a href=\"https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\">NotMNIST dataset</a> is a collection of letters (A-J). Permuted MNIST dataset. This version uses the smaller set, which consists of about 19,000 images of 10 classes, each 28x28 grayscale image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.notmnist.NotMNIST.__init__", "modulename": "clarena.stl_datasets.notmnist", "qualname": "NotMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original NotMNIST data 'NotMNIST/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.notmnist.NotMNIST.original_dataset_python_class", "modulename": "clarena.stl_datasets.notmnist", "qualname": "NotMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.notmnist.NotMNIST&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.notmnist.NotMNIST.validation_percentage", "modulename": "clarena.stl_datasets.notmnist", "qualname": "NotMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.notmnist.NotMNIST.prepare_data", "modulename": "clarena.stl_datasets.notmnist", "qualname": "NotMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original NotMNIST dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.notmnist.NotMNIST.train_and_val_dataset", "modulename": "clarena.stl_datasets.notmnist", "qualname": "NotMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.notmnist.NotMNIST.test_dataset", "modulename": "clarena.stl_datasets.notmnist", "qualname": "NotMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.oxford_iiit_pet", "modulename": "clarena.stl_datasets.oxford_iiit_pet", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Oxford-IIIT Pet dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.oxford_iiit_pet.OxfordIIITPet", "modulename": "clarena.stl_datasets.oxford_iiit_pet", "qualname": "OxfordIIITPet", "kind": "class", "doc": "<p>Permuted Oxford-IIIT Pet dataset. The <a href=\"https://www.robots.ox.ac.uk/~vgg/data/pets/\">Oxford-IIIT Pet dataset</a> is a collection of cat and dog pictures. It consists of 7,349 images of 37 breeds (classes), each color image. It also provides a binary classification version with 2 classes (cat or dog). We support both versions in Permuted Oxford-IIIT Pet.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.oxford_iiit_pet.OxfordIIITPet.__init__", "modulename": "clarena.stl_datasets.oxford_iiit_pet", "qualname": "OxfordIIITPet.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Oxford-IIIT Pet data 'OxfordIIITPet/' live.</li>\n<li><strong>target_type</strong> (<code>str</code>): the target type; one of:\n<ol>\n<li>'category': Label for one of the 37 pet categories.</li>\n<li>'binary-category': Binary label for cat or dog.</li>\n</ol></li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">target_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.oxford_iiit_pet.OxfordIIITPet.target_type", "modulename": "clarena.stl_datasets.oxford_iiit_pet", "qualname": "OxfordIIITPet.target_type", "kind": "variable", "doc": "<p>The target type.</p>\n", "annotation": ": str"}, {"fullname": "clarena.stl_datasets.oxford_iiit_pet.OxfordIIITPet.validation_percentage", "modulename": "clarena.stl_datasets.oxford_iiit_pet", "qualname": "OxfordIIITPet.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.oxford_iiit_pet.OxfordIIITPet.prepare_data", "modulename": "clarena.stl_datasets.oxford_iiit_pet", "qualname": "OxfordIIITPet.prepare_data", "kind": "function", "doc": "<p>Download the original Oxford-IIIT Pet dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.oxford_iiit_pet.OxfordIIITPet.train_and_val_dataset", "modulename": "clarena.stl_datasets.oxford_iiit_pet", "qualname": "OxfordIIITPet.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.oxford_iiit_pet.OxfordIIITPet.test_dataset", "modulename": "clarena.stl_datasets.oxford_iiit_pet", "qualname": "OxfordIIITPet.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.pcam", "modulename": "clarena.stl_datasets.pcam", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for PCAM dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.pcam.PCAM", "modulename": "clarena.stl_datasets.pcam", "qualname": "PCAM", "kind": "class", "doc": "<p>PCAM dataset. The <a href=\"https://github.com/basveeling/pcam\">PCAM dataset</a> is a collection of medical images of breast cancer. It consists of 327,680 images in 2 classes (benign and malignant), each 96x96 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.pcam.PCAM.__init__", "modulename": "clarena.stl_datasets.pcam", "qualname": "PCAM.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original PCAM data 'PCAM/' live.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.pcam.PCAM.original_dataset_python_class", "modulename": "clarena.stl_datasets.pcam", "qualname": "PCAM.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.pcam.PCAM&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.pcam.PCAM.prepare_data", "modulename": "clarena.stl_datasets.pcam", "qualname": "PCAM.prepare_data", "kind": "function", "doc": "<p>Download the original PCAM dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.pcam.PCAM.train_and_val_dataset", "modulename": "clarena.stl_datasets.pcam", "qualname": "PCAM.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.pcam.PCAM.test_dataset", "modulename": "clarena.stl_datasets.pcam", "qualname": "PCAM.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.renderedsst2", "modulename": "clarena.stl_datasets.renderedsst2", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Rendered SST2 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.renderedsst2.RenderedSST2", "modulename": "clarena.stl_datasets.renderedsst2", "qualname": "RenderedSST2", "kind": "class", "doc": "<p>Rendered SST2 dataset. The <a href=\"https://github.com/openai/CLIP/blob/main/data/rendered-sst2.md\">Rendered SST2 dataset</a> is a collection of optical character recognition images. It consists of 9,613 images in 2 classes (positive and negative sentiment), each 448x448 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.renderedsst2.RenderedSST2.__init__", "modulename": "clarena.stl_datasets.renderedsst2", "qualname": "RenderedSST2.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Rendered SST2 data 'RenderedSST2/' live.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.renderedsst2.RenderedSST2.original_dataset_python_class", "modulename": "clarena.stl_datasets.renderedsst2", "qualname": "RenderedSST2.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.rendered_sst2.RenderedSST2&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.renderedsst2.RenderedSST2.prepare_data", "modulename": "clarena.stl_datasets.renderedsst2", "qualname": "RenderedSST2.prepare_data", "kind": "function", "doc": "<p>Download the original Rendered SST2 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.renderedsst2.RenderedSST2.train_and_val_dataset", "modulename": "clarena.stl_datasets.renderedsst2", "qualname": "RenderedSST2.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.renderedsst2.RenderedSST2.test_dataset", "modulename": "clarena.stl_datasets.renderedsst2", "qualname": "RenderedSST2.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.sign_language_mnist", "modulename": "clarena.stl_datasets.sign_language_mnist", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Sign Language MNIST dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.sign_language_mnist.SignLanguageMNIST", "modulename": "clarena.stl_datasets.sign_language_mnist", "qualname": "SignLanguageMNIST", "kind": "class", "doc": "<p>Sign Language MNIST dataset. The <a href=\"https://www.kaggle.com/datasets/datamunge/sign-language-mnist\">Sign Language MNIST dataset</a> is a collection of hand gesture images representing ASL letters (A-Y, excluding J). It consists of 34,627 images of 24 classes, each 28x28 grayscale image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.sign_language_mnist.SignLanguageMNIST.__init__", "modulename": "clarena.stl_datasets.sign_language_mnist", "qualname": "SignLanguageMNIST.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Sign Language MNIST data 'SignLanguageMNIST/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.sign_language_mnist.SignLanguageMNIST.original_dataset_python_class", "modulename": "clarena.stl_datasets.sign_language_mnist", "qualname": "SignLanguageMNIST.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;clarena.stl_datasets.raw.sign_language_mnist.SignLanguageMNIST&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.sign_language_mnist.SignLanguageMNIST.validation_percentage", "modulename": "clarena.stl_datasets.sign_language_mnist", "qualname": "SignLanguageMNIST.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.sign_language_mnist.SignLanguageMNIST.prepare_data", "modulename": "clarena.stl_datasets.sign_language_mnist", "qualname": "SignLanguageMNIST.prepare_data", "kind": "function", "doc": "<p>Download the original Sign Language MNIST dataset if haven't. Because the original dataset is published on Kaggle, we need to download it manually. This function will not download the original dataset automatically.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.sign_language_mnist.SignLanguageMNIST.train_and_val_dataset", "modulename": "clarena.stl_datasets.sign_language_mnist", "qualname": "SignLanguageMNIST.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.sign_language_mnist.SignLanguageMNIST.test_dataset", "modulename": "clarena.stl_datasets.sign_language_mnist", "qualname": "SignLanguageMNIST.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.stanfordcars", "modulename": "clarena.stl_datasets.stanfordcars", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for Stanford Cars dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.stanfordcars.StanfordCars", "modulename": "clarena.stl_datasets.stanfordcars", "qualname": "StanfordCars", "kind": "class", "doc": "<p>Stanford Cars dataset. The <a href=\"https://pytorch.org/vision/stable/generated/torchvision.datasets.StanfordCars.html#torchvision.datasets.StanfordCars/\">Stanford Cars dataset</a> is a collection of car images. It consists of 16,185 images in 196 classes, each color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.stanfordcars.StanfordCars.__init__", "modulename": "clarena.stl_datasets.stanfordcars", "qualname": "StanfordCars.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original Stanford Cars data 'StanfordCars/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.stanfordcars.StanfordCars.original_dataset_python_class", "modulename": "clarena.stl_datasets.stanfordcars", "qualname": "StanfordCars.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.stanford_cars.StanfordCars&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.stanfordcars.StanfordCars.validation_percentage", "modulename": "clarena.stl_datasets.stanfordcars", "qualname": "StanfordCars.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.stanfordcars.StanfordCars.prepare_data", "modulename": "clarena.stl_datasets.stanfordcars", "qualname": "StanfordCars.prepare_data", "kind": "function", "doc": "<p>Download the original Stanford Cars dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.stanfordcars.StanfordCars.train_and_val_dataset", "modulename": "clarena.stl_datasets.stanfordcars", "qualname": "StanfordCars.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.stanfordcars.StanfordCars.test_dataset", "modulename": "clarena.stl_datasets.stanfordcars", "qualname": "StanfordCars.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.sun397", "modulename": "clarena.stl_datasets.sun397", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for SUN397 dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.sun397.SUN397", "modulename": "clarena.stl_datasets.sun397", "qualname": "SUN397", "kind": "class", "doc": "<p>SUN397 dataset. The <a href=\"https://vision.princeton.edu/projects/2010/SUN\">SUN397 dataset</a> is a collection of scene images. It consists of 108,754 images of 397 classes, each color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.sun397.SUN397.__init__", "modulename": "clarena.stl_datasets.sun397", "qualname": "SUN397.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original SUN397 data 'SUN397/' live.</li>\n<li><strong>test_percentage</strong> (<code>float</code>): the percentage to randomly split some data into test data.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">test_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.sun397.SUN397.original_dataset_python_class", "modulename": "clarena.stl_datasets.sun397", "qualname": "SUN397.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.sun397.SUN397&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.sun397.SUN397.test_percentage", "modulename": "clarena.stl_datasets.sun397", "qualname": "SUN397.test_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some data into test data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.sun397.SUN397.validation_percentage", "modulename": "clarena.stl_datasets.sun397", "qualname": "SUN397.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.sun397.SUN397.prepare_data", "modulename": "clarena.stl_datasets.sun397", "qualname": "SUN397.prepare_data", "kind": "function", "doc": "<p>Download the original SUN397 dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.sun397.SUN397.train_and_val_dataset", "modulename": "clarena.stl_datasets.sun397", "qualname": "SUN397.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.sun397.SUN397.test_dataset", "modulename": "clarena.stl_datasets.sun397", "qualname": "SUN397.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.svhn", "modulename": "clarena.stl_datasets.svhn", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for SVHN dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.svhn.SVHN", "modulename": "clarena.stl_datasets.svhn", "qualname": "SVHN", "kind": "class", "doc": "<p>SVHN dataset. The <a href=\"http://ufldl.stanford.edu/housenumbers/\">SVHN dataset</a> is a collection of street view house number images. It consists 73,257 training and 26,032 test images of 10 classes, each 32x32 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.svhn.SVHN.__init__", "modulename": "clarena.stl_datasets.svhn", "qualname": "SVHN.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original SVHN data 'SVHN/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.svhn.SVHN.original_dataset_python_class", "modulename": "clarena.stl_datasets.svhn", "qualname": "SVHN.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.svhn.SVHN&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.svhn.SVHN.validation_percentage", "modulename": "clarena.stl_datasets.svhn", "qualname": "SVHN.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.svhn.SVHN.prepare_data", "modulename": "clarena.stl_datasets.svhn", "qualname": "SVHN.prepare_data", "kind": "function", "doc": "<p>Download the original SVHN dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.svhn.SVHN.train_and_val_dataset", "modulename": "clarena.stl_datasets.svhn", "qualname": "SVHN.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.svhn.SVHN.test_dataset", "modulename": "clarena.stl_datasets.svhn", "qualname": "SVHN.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.tinyimagenet", "modulename": "clarena.stl_datasets.tinyimagenet", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for TinyImageNet dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.tinyimagenet.TinyImageNet", "modulename": "clarena.stl_datasets.tinyimagenet", "qualname": "TinyImageNet", "kind": "class", "doc": "<p>TinyImageNet dataset. The <a href=\"http://vision.stanford.edu/teaching/cs231n/reports/2015/pdfs/yle_project.pdf\">TinyImageNet dataset</a> is smaller, more manageable version of the <a href=\"https://www.image-net.org\">Imagenet dataset</a>. It consists of 100,000 training, 10,000 validation and 10,000 test images of 200 classes, each 64x64 color image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.tinyimagenet.TinyImageNet.__init__", "modulename": "clarena.stl_datasets.tinyimagenet", "qualname": "TinyImageNet.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original TinyImageNet data 'tiny-imagenet-200/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.tinyimagenet.TinyImageNet.original_dataset_python_class", "modulename": "clarena.stl_datasets.tinyimagenet", "qualname": "TinyImageNet.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;tinyimagenet.TinyImageNet&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.tinyimagenet.TinyImageNet.validation_percentage", "modulename": "clarena.stl_datasets.tinyimagenet", "qualname": "TinyImageNet.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.tinyimagenet.TinyImageNet.prepare_data", "modulename": "clarena.stl_datasets.tinyimagenet", "qualname": "TinyImageNet.prepare_data", "kind": "function", "doc": "<p>Download the original TinyImageNet dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.tinyimagenet.TinyImageNet.train_and_val_dataset", "modulename": "clarena.stl_datasets.tinyimagenet", "qualname": "TinyImageNet.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.tinyimagenet.TinyImageNet.test_dataset", "modulename": "clarena.stl_datasets.tinyimagenet", "qualname": "TinyImageNet.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.usps", "modulename": "clarena.stl_datasets.usps", "kind": "module", "doc": "<p>The submodule in <code>stl_datasets</code> for USPS dataset.</p>\n"}, {"fullname": "clarena.stl_datasets.usps.USPS", "modulename": "clarena.stl_datasets.usps", "qualname": "USPS", "kind": "class", "doc": "<p>USPS dataset. The <a href=\"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps\">USPS dataset</a> is a collection of handwritten digits. It consists of 9,298 handwritten digit images (10 classes), each 16x16 grayscale image.</p>\n", "bases": "clarena.stl_datasets.base.STLDatasetFromRaw"}, {"fullname": "clarena.stl_datasets.usps.USPS.__init__", "modulename": "clarena.stl_datasets.usps", "qualname": "USPS.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>root</strong> (<code>str</code>): the root directory where the original USPS data 'USPS/' live.</li>\n<li><strong>validation_percentage</strong> (<code>float</code>): the percentage to randomly split some training data into validation data.</li>\n<li><strong>batch_size</strong> (<code>int</code>): The batch size in train, val, test dataloader.</li>\n<li><strong>num_workers</strong> (<code>int</code>): the number of workers for dataloaders.</li>\n<li><strong>custom_transforms</strong> (<code>transform</code> or <code>transforms.Compose</code> or <code>None</code>): the custom transforms to apply to ONLY TRAIN dataset. Can be a single transform, composed transforms or no transform. <code>ToTensor()</code>, normalize and so on are not included.</li>\n<li><strong>repeat_channels</strong> (<code>int</code> | <code>None</code>): the number of channels to repeat. Default is None, which means no repeat. If not None, it should be an integer.</li>\n<li><strong>to_tensor</strong> (<code>bool</code>): whether to include <code>ToTensor()</code> transform. Default is True.</li>\n<li><strong>resize</strong> (<code>tuple[int, int]</code> | <code>None</code> or list of them): the size to resize the images to. Default is None, which means no resize. If not None, it should be a tuple of two integers.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">root</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">validation_percentage</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_workers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">custom_transforms</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">,</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">repeat_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">to_tensor</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">resize</span><span class=\"p\">:</span> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.stl_datasets.usps.USPS.original_dataset_python_class", "modulename": "clarena.stl_datasets.usps", "qualname": "USPS.original_dataset_python_class", "kind": "variable", "doc": "<p>The original dataset class.</p>\n", "annotation": ": type[torch.utils.data.dataset.Dataset]", "default_value": "&lt;class &#x27;torchvision.datasets.usps.USPS&#x27;&gt;"}, {"fullname": "clarena.stl_datasets.usps.USPS.validation_percentage", "modulename": "clarena.stl_datasets.usps", "qualname": "USPS.validation_percentage", "kind": "variable", "doc": "<p>The percentage to randomly split some training data into validation data.</p>\n", "annotation": ": float"}, {"fullname": "clarena.stl_datasets.usps.USPS.prepare_data", "modulename": "clarena.stl_datasets.usps", "qualname": "USPS.prepare_data", "kind": "function", "doc": "<p>Download the original USPS dataset if haven't.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.usps.USPS.train_and_val_dataset", "modulename": "clarena.stl_datasets.usps", "qualname": "USPS.train_and_val_dataset", "kind": "function", "doc": "<p>Get the training and validation dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>train_and_val_dataset</strong> (<code>tuple[Dataset, Dataset]</code>): the train and validation dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.stl_datasets.usps.USPS.test_dataset", "modulename": "clarena.stl_datasets.usps", "qualname": "USPS.test_dataset", "kind": "function", "doc": "<p>Get the test dataset.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_dataset</strong> (<code>Dataset</code>): the test dataset.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils", "modulename": "clarena.utils", "kind": "module", "doc": "<h1 id=\"utilities\">Utilities</h1>\n\n<p>This submodule provides utilities that are used in CLArena, which includes:</p>\n\n<ul>\n<li><strong>Cfg</strong>: utilities for configuration files in CLArena.</li>\n<li><strong>Transforms</strong>: data transforms.</li>\n<li><strong>Metrics</strong>: custom torchmetrics.</li>\n<li><strong>Eval</strong>: evaluation modules and utilities.</li>\n<li><strong>Misc</strong>: miscellaneous utilities.</li>\n</ul>\n"}, {"fullname": "clarena.utils.cfg", "modulename": "clarena.utils.cfg", "kind": "module", "doc": "<p>The submodule in <code>utils</code> with tools related to configs.</p>\n"}, {"fullname": "clarena.utils.cfg.preprocess_config", "modulename": "clarena.utils.cfg", "qualname": "preprocess_config", "kind": "function", "doc": "<p>Preprocess the configuration before constructing experiment, which include:</p>\n\n<ol>\n<li>Construct the config for pipelines that borrow from other config.</li>\n<li>Convert the <code>DictConfig</code> to a Rich <code>Tree</code>, print the Rich <code>Tree</code> and save the Rich <code>Tree</code> to a file.</li>\n</ol>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the config dict to preprocess.</li>\n<li><strong>type</strong> (<code>str</code>): the type of the pipeline; one of:\n<ol>\n<li>'CL_MAIN_EXPR': continual learning main experiment.</li>\n<li>'CL_MAIN_EVAL': continual learning main evaluation.</li>\n<li>'CL_REF_JOINT_EXPR': reference joint learning experiment (continual learning).</li>\n<li>'CL_REF_INDEPENDENT_EXPR': reference independent learning experiment (continual learning).</li>\n<li>'CL_REF_RANDOM_EXPR': reference random learning experiment (continual learning).</li>\n<li>'CL_FULL_EVAL': continual learning full evaluation.</li>\n<li>'CL_FULL_EVAL_ATTACHED': continual unlearning full evaluation (attached to continual learning full experiment).</li>\n<li>'CUL_MAIN_EXPR': continual unlearning main experiment.</li>\n<li>'CUL_MAIN_EVAL': continual unlearning main evaluation.</li>\n<li>'CUL_REF_RETRAIN_EXPR': reference retrain learning experiment (continual unlearning).</li>\n<li>'CUL_REF_ORIGINAL_EXPR': reference original learning experiment (contin\n12, 'CUL_FULL_EVAL': continual unlearning full evaluation.</li>\n<li>'CUL_FULL_EVAL_ATTACHED': continual unlearning full evaluation (attached to continual unlearning full experiment).</li>\n<li>'MTL_EXPR': multi-task learning experiment.</li>\n<li>'MTL_EVAL': multi-task learning evaluation.</li>\n<li>'STL_EXPR': single-task learning experiment.</li>\n<li>'STL_EVAL': single-task learning evaluation.</li>\n</ol></li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the preprocessed config dict.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>, </span><span class=\"param\"><span class=\"nb\">type</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.cfg.cfg_to_tree", "modulename": "clarena.utils.cfg", "qualname": "cfg_to_tree", "kind": "function", "doc": "<p>Convert the configuration to a Rich <code>Tree</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cfg</strong> (<code>DictConfig</code>): the target config dict to be converted.</li>\n<li><strong>config_tree_cfg</strong> (<code>DictConfig</code>): the configuration for conversion of config tree.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>tree</strong> (<code>Tree</code>): the Rich <code>Tree</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span>,</span><span class=\"param\">\t<span class=\"n\">config_tree_cfg</span><span class=\"p\">:</span> <span class=\"n\">omegaconf</span><span class=\"o\">.</span><span class=\"n\">dictconfig</span><span class=\"o\">.</span><span class=\"n\">DictConfig</span></span><span class=\"return-annotation\">) -> <span class=\"n\">rich</span><span class=\"o\">.</span><span class=\"n\">tree</span><span class=\"o\">.</span><span class=\"n\">Tree</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.cfg.save_tree_to_file", "modulename": "clarena.utils.cfg", "qualname": "save_tree_to_file", "kind": "function", "doc": "<p>Save Rich <code>Tree</code> to a file.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>tree</strong> (<code>dict</code>): the Rich <code>Tree</code> to save.</li>\n<li><strong>save_path</strong> (<code>str</code>): the path to save the tree.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">tree</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>, </span><span class=\"param\"><span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.eval", "modulename": "clarena.utils.eval", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for evaluation utilities.</p>\n"}, {"fullname": "clarena.utils.eval.pylogger", "modulename": "clarena.utils.eval", "qualname": "pylogger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger clarena.utils.eval (WARNING)&gt;"}, {"fullname": "clarena.utils.eval.CULEvaluation", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation", "kind": "class", "doc": "<p>Full evaluation module for continual unlearning.</p>\n", "bases": "lightning.pytorch.core.module.LightningModule"}, {"fullname": "clarena.utils.eval.CULEvaluation.__init__", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>main_model</strong> (<code>CULAlgorithm</code>): the main model to evaluate.</li>\n<li><strong>refretrain_model</strong> (<code>CLAlgorithm</code>): the reference retrain model to evaluate against.</li>\n<li><strong>reforiginal_model</strong> (<code>CLAlgorithm</code>): the reference original model that has been trained on all tasks.</li>\n<li><strong>dd_eval_task_ids</strong> (<code>list[int]</code>): the list of task IDs to evaluate the DD on.</li>\n<li><strong>ad_eval_task_ids</strong> (<code>list[int]</code>): the list of task IDs to evaluate the accuracy difference on.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">main_model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cul_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CULAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">refretrain_model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">reforiginal_model</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">cl_algorithms</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">CLAlgorithm</span>,</span><span class=\"param\">\t<span class=\"n\">dd_eval_task_ids</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">ad_eval_task_ids</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "clarena.utils.eval.CULEvaluation.criterion", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.criterion", "kind": "variable", "doc": "<p>The loss function bewteen the output logits and the target labels. Default is cross-entropy loss.</p>\n"}, {"fullname": "clarena.utils.eval.CULEvaluation.main_model", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.main_model", "kind": "variable", "doc": "<p>The main model for evaluation.</p>\n"}, {"fullname": "clarena.utils.eval.CULEvaluation.refretrain_model", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.refretrain_model", "kind": "variable", "doc": "<p>The reference retrain model for evaluation.</p>\n"}, {"fullname": "clarena.utils.eval.CULEvaluation.reforiginal_model", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.reforiginal_model", "kind": "variable", "doc": "<p>The reference original model for evaluation.</p>\n"}, {"fullname": "clarena.utils.eval.CULEvaluation.dd_eval_task_ids", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.dd_eval_task_ids", "kind": "variable", "doc": "<p>The task IDs to evaluate the DD on.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.utils.eval.CULEvaluation.ad_eval_task_ids", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.ad_eval_task_ids", "kind": "variable", "doc": "<p>The task IDs to evaluate the AD on.</p>\n", "annotation": ": list[int]"}, {"fullname": "clarena.utils.eval.CULEvaluation.get_test_task_id_from_dataloader_idx", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.get_test_task_id_from_dataloader_idx", "kind": "function", "doc": "<p>Get the test task ID from the dataloader index.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the dataloader index.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>test_task_id</strong> (<code>int</code>): the test task ID.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.eval.CULEvaluation.test_step", "modulename": "clarena.utils.eval", "qualname": "CULEvaluation.test_step", "kind": "function", "doc": "<p>Test step for current task <code>self.task_id</code>, which tests for all seen tasks indexed by <code>dataloader_idx</code>.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>batch</strong> (<code>Any</code>): a batch of test data.</li>\n<li><strong>dataloader_idx</strong> (<code>int</code>): the task ID of seen tasks to be tested. A default value of 0 is given otherwise the LightningModule will raise a <code>RuntimeError</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>outputs</strong> (<code>dict[str, Tensor]</code>): a dictionary contains loss and other metrics from this test step. Keys (<code>str</code>) are the metrics names, and values (<code>Tensor</code>) are the metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">batch_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataloader_idx</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics", "modulename": "clarena.utils.metrics", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for custom torchmetrics.</p>\n"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch", "kind": "class", "doc": "<p>A TorchMetrics metric to calculate the mean of metrics across data batches.</p>\n\n<p>This is used for accumulated metrics in deep learning. See <a href=\"https://pengxiang-wang.com/posts/continual-learning-metrics.html#nte-accumulate\">here</a> for more details.</p>\n", "bases": "torchmetrics.aggregation.BaseAggregator"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.__init__", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.__init__", "kind": "function", "doc": "<p>Initialize the metric. Add state variables.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">nan_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;error&#39;</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.sum", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.sum", "kind": "variable", "doc": "<p>State variable created by <code>super().__init__()</code> to store the sum of the metric values till this batch.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.num", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.num", "kind": "variable", "doc": "<p>State variable created by <code>add_state()</code> to store the number of the data till this batch.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.update", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.update", "kind": "function", "doc": "<p>Update and accumulate the sum of metric value and num of the data till this batch from the batch.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>val</strong> (<code>torch.Tensor</code>): the metric value of the batch to update the sum.</li>\n<li><strong>batch_size</strong> (<code>int</code>): the value to update the num, which is the batch size.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics.MeanMetricBatch.compute", "modulename": "clarena.utils.metrics", "qualname": "MeanMetricBatch.compute", "kind": "function", "doc": "<p>Compute this mean metric value till this batch.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>mean</strong> (<code>Tensor</code>): the calculated mean result.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric", "kind": "class", "doc": "<p>A torchmetrics metric to calculate the network capacity of HAT (Hard Attention to the Task) algorithm.</p>\n\n<p>Network capacity is defined as the average adjustment rate over all parameters. See chapter 4.1 in the <a href=\"https://link.springer.com/chapter/10.1007/978-3-031-70352-2_9\">AdaHAT paper</a>.</p>\n", "bases": "torchmetrics.aggregation.BaseAggregator"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.__init__", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.__init__", "kind": "function", "doc": "<p>Initialise the HAT network capacity metric. Add state variables.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">nan_strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;error&#39;</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">:</span> <span class=\"n\">Any</span></span>)</span>"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.sum_adjustment_rate", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.sum_adjustment_rate", "kind": "variable", "doc": "<p>State variable created by <code>add_state()</code> to store the sum of the adjustment rate values till this layer.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.num_params", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.num_params", "kind": "variable", "doc": "<p>State variable created by <code>add_state()</code> to store the number of the parameters till this layer.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.update", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.update", "kind": "function", "doc": "<p>Update and accumulate the sum of adjustment rate values till this layer from the layer.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>adjustment_rate_weight_layer</strong> (<code>Tensor</code>): the adjustment rate values of the weight matrix of the layer.</li>\n<li><strong>adjustment_rate_bias_layer</strong> (<code>Tensor</code>): the adjustment rate values of the bias vector of the layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_rate_weight_layer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">adjustment_rate_bias_layer</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.metrics.HATNetworkCapacityMetric.compute", "modulename": "clarena.utils.metrics", "qualname": "HATNetworkCapacityMetric.compute", "kind": "function", "doc": "<p>Compute this HAT network capacity till this layer.</p>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>network_capacity</strong> (<code>Tensor</code>): the calculated network capacity result.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.misc", "modulename": "clarena.utils.misc", "kind": "module", "doc": "<p>The submodule in <code>utils</code> of miscellaneous utilities.</p>\n"}, {"fullname": "clarena.utils.misc.str_to_class", "modulename": "clarena.utils.misc", "qualname": "str_to_class", "kind": "function", "doc": "<p>Convert a string to a class.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>class_path</strong> (<code>str</code>): the string of the class path, e.g. <code>torchvision.datasets.MNIST</code>.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>cls</strong> (<code>type</code>): the class object.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">class_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">type</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.transforms", "modulename": "clarena.utils.transforms", "kind": "module", "doc": "<p>The submodule in <code>utils</code> for data transforms.</p>\n"}, {"fullname": "clarena.utils.transforms.ClassMapping", "modulename": "clarena.utils.transforms", "qualname": "ClassMapping", "kind": "class", "doc": "<p>Class mapping to dataset labels. Used as a PyTorch target Transform.</p>\n"}, {"fullname": "clarena.utils.transforms.ClassMapping.__init__", "modulename": "clarena.utils.transforms", "qualname": "ClassMapping.__init__", "kind": "function", "doc": "<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>cl_class_map</strong> (<code>dict[str | int, int]</code>): the class map.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">class_map</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span> <span class=\"o\">|</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "clarena.utils.transforms.ClassMapping.class_map", "modulename": "clarena.utils.transforms", "qualname": "ClassMapping.class_map", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "clarena.utils.transforms.Permute", "modulename": "clarena.utils.transforms", "qualname": "Permute", "kind": "class", "doc": "<p>Permutation operation to image. Used to construct permuted CL dataset.</p>\n\n<p>Used as a PyTorch Dataset Transform.</p>\n"}, {"fullname": "clarena.utils.transforms.Permute.__init__", "modulename": "clarena.utils.transforms", "qualname": "Permute.__init__", "kind": "function", "doc": "<p>Initialize the Permute transform object. The permutation order is constructed in the initialization to save runtime.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>num_channels</strong> (<code>int</code>): the number of channels in the image.</li>\n<li><strong>img_size</strong> (<code>torch.Size</code>): the size of the image to be permuted.</li>\n<li><strong>mode</strong> (<code>str</code>): the mode of permutation, shouble be one of the following:\n<ul>\n<li>'all': permute all pixels.</li>\n<li>'by_channel': permute channel by channel separately. All channels are applied the same permutation order.</li>\n<li>'first_channel_only': permute only the first channel.</li>\n</ul></li>\n<li><strong>seed</strong> (<code>int</code> or <code>None</code>): seed for permutation operation. If None, the permutation will use a default seed from PyTorch generator.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">num_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">img_size</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Size</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;first_channel_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "clarena.utils.transforms.Permute.mode", "modulename": "clarena.utils.transforms", "qualname": "Permute.mode", "kind": "variable", "doc": "<p>The mode of permutation.</p>\n"}, {"fullname": "clarena.utils.transforms.Permute.permute", "modulename": "clarena.utils.transforms", "qualname": "Permute.permute", "kind": "variable", "doc": "<p>The permutation order, a <code>Tensor</code> permuted from [1,2, ..., <code>num_pixels</code>] with the given seed. It is the core element of permutation operation.</p>\n", "annotation": ": torch.Tensor"}, {"fullname": "clarena.utils.transforms.insert_permute_in_compose", "modulename": "clarena.utils.transforms", "qualname": "insert_permute_in_compose", "kind": "function", "doc": "<p>Insert <code>permute_transform</code> in a <code>compose</code> (<code>transforms.Compose</code>).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">compose</span><span class=\"p\">:</span> <span class=\"n\">torchvision</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Compose</span>,</span><span class=\"param\">\t<span class=\"n\">permute_transform</span><span class=\"p\">:</span> <span class=\"n\">clarena</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">Permute</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.transforms.min_max_normalize", "modulename": "clarena.utils.transforms", "qualname": "min_max_normalize", "kind": "function", "doc": "<p>Normalize the tensor using min-max normalization.</p>\n\n<p><strong>Args:</strong></p>\n\n<ul>\n<li><strong>tensor</strong> (<code>Tensor</code>): the input tensor to normalize.</li>\n<li><strong>dim</strong> (<code>int</code> | <code>None</code>): the dimension to normalize along. If <code>None</code>, normalize the whole tensor.</li>\n<li><strong>epsilon</strong> (<code>float</code>): the epsilon value to avoid division by zero.</li>\n</ul>\n\n<p><strong>Returns:</strong></p>\n\n<ul>\n<li><strong>tensor</strong> (<code>Tensor</code>): the normalized tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tensor</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">epsilon</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-08</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "clarena.utils.transforms.js_div", "modulename": "clarena.utils.transforms", "qualname": "js_div", "kind": "function", "doc": "<p>Jensen-Shannon divergence between two probability distributions.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">target</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">size_average</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reduce</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">|</span> <span class=\"kc\">None</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();