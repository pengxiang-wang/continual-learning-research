1:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
2:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
3:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
4:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
5:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
6:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
7:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
8:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
9:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
10:
  _target_: torch.optim.Adam
  _partial_: True # partially instantiate optimizer without 'params' argument. Make sure this is included in any case!
  lr: 0.01
  weight_decay: 0.0
